import torch
from typing import Dict, Any, Optional, List, Tuple
from lm_saes import SparseAutoEncoder, LowRankSparseAttention
from transformer_lens import HookedTransformer
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from lm_saes.circuit.leela_board import LeelaBoard
    import chess
except ImportError:
    print("WARNING: leela_interp not found, chess functionality will be limited")
    LeelaBoard = None
    chess = None

BT4_MODEL_NAME = "lc0/BT4-1024x15x32h"
BT4_TC_BASE_PATH = "/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/tc"
BT4_LORSA_BASE_PATH = "/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/lorsa"


class PatchingAnalyzer:
    """æ¶ˆèåˆ†æå™¨ï¼Œç”¨äºåˆ†æç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“"""
    
    def __init__(self, model: HookedTransformer, 
                 transcoders: Dict[int, SparseAutoEncoder], 
                 lorsas: List[LowRankSparseAttention]):
        self.model = model
        self.transcoders = transcoders
        self.lorsas = lorsas
        
        # é¢„è®¡ç®—WDæƒé‡
        self.tc_WDs = {}
        self.lorsa_WDs = {}
        
        for layer in range(15):
            self.tc_WDs[layer] = transcoders[layer].W_D
            self.lorsa_WDs[layer] = lorsas[layer].W_O
    
    def get_activations(self, fen: str) -> Tuple[List, List]:
        """è·å–ç»™å®šFENçš„æ¿€æ´»å€¼"""
        output, cache = self.model.run_with_cache(fen, prepend_bos=False)
        
        lorsa_activations, tc_activations = [], []
        
        for layer in range(15):
            # LoRSAæ¿€æ´»
            lorsa_input = cache[f'blocks.{layer}.hook_attn_in']
            lorsa_dense_activation = self.lorsas[layer].encode(lorsa_input)
            lorsa_sparse_activation = lorsa_dense_activation.to_sparse_coo()
            lorsa_activations.append(lorsa_sparse_activation)
            
            # TCæ¿€æ´»
            tc_input = cache[f'blocks.{layer}.resid_mid_after_ln']
            tc_dense_activation = self.transcoders[layer].encode(tc_input)
            tc_sparse_activation = tc_dense_activation.to_sparse_coo()
            tc_activations.append(tc_sparse_activation)
        
        return lorsa_activations, tc_activations
    
    def steering_analysis(self, feature_type: str, layer: int, 
                                   pos: int, feature: int, steering_scale: int, 
                                   fen: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨hookè¿›è¡Œæ¶ˆèåˆ†æ"""
        
        # ç¡®ä¿æ— æ®‹ç•™hook
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        
        # è·å–æ¿€æ´»å€¼
        lorsa_activations, tc_activations = self.get_activations(fen)
        
        if feature_type == 'transcoder':
            activations = tc_activations[layer]
            WDs = self.tc_WDs[layer]
        elif feature_type == 'lorsa':
            activations = lorsa_activations[layer]
            WDs = self.lorsa_WDs[layer]
        else:
            raise ValueError("feature_typeå¿…é¡»æ˜¯'transcoder'æˆ–'lorsa'")
        
        # æŸ¥æ‰¾æ¿€æ´»å€¼
        target_indices = torch.tensor([0, pos, feature], 
                                    device=activations.indices().device)
        matches = (activations.indices() == 
                  target_indices.unsqueeze(1)).all(dim=0)
        
        if not matches.any():
            print('è¯¥ä½ç½®æ²¡æœ‰æ¿€æ´»å€¼ï¼Œæ— æ³•è¿›è¡Œæ¶ˆèåˆ†æ')
            return None
        
        activation_value = activations.values()[matches].item()
        
        # è®¡ç®—ç‰¹å¾è´¡çŒ®
        feature_contribution = activation_value * WDs[feature]  # [768]
        
        # ç¡®å®šè¦ä¿®æ”¹çš„hookä½ç½®
        if feature_type == 'transcoder':
            hook_name = f'blocks.{layer}.hook_mlp_out'
        else:  # lorsa
            hook_name = f'blocks.{layer}.hook_attn_out'
        
        # å†æ¬¡ç¡®ä¿æ— hookå¹¶è·å–åŸå§‹è¾“å‡ºï¼ˆæ— ä¿®æ”¹ï¼‰
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        original_output, cache = self.model.run_with_cache(fen, prepend_bos=False)
        
        # å®šä¹‰hookä¿®æ”¹å‡½æ•°
        def modify_hook(tensor, hook):
            modified_activation = tensor.clone()
            modified_activation[0, pos] = modified_activation[0, pos] + (steering_scale - 1) * feature_contribution
            return modified_activation
        
        # è¿è¡Œä¿®æ”¹åçš„æ¨¡å‹ï¼ˆä»…æœ¬æ¬¡ç”Ÿæ•ˆçš„hookï¼‰
        self.model.add_hook(hook_name, modify_hook)
        modified_output, _ = self.model.run_with_cache(
            fen, prepend_bos=False)
        # æ¸…ç†hookï¼Œé¿å…å½±å“åç»­è¯·æ±‚
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        
        # è®¡ç®—logitå·®å¼‚
        logit_diff = original_output[0] - modified_output[0]
        
        return {
            'feature_type': feature_type,
            'layer': layer,
            'pos': pos,
            'feature': feature,
            'activation_value': activation_value,
            'feature_contribution': feature_contribution.detach().cpu().numpy().tolist(),
            'original_output': original_output[0].detach().cpu().numpy().tolist(),
            'modified_output': modified_output[0].detach().cpu().numpy().tolist(),
            'logit_diff': logit_diff.detach().cpu().numpy().tolist(),
            'hook_name': hook_name
        }
    
    def analyze_steering_results(self, ablation_result: Dict[str, Any], 
                               fen: str) -> Dict[str, Any]:
        """åˆ†ææ¶ˆèç»“æœï¼Œè¿”å›å¯¹åˆæ³•ç§»åŠ¨çš„å½±å“"""
        if ablation_result is None:
            return None
        
        if LeelaBoard is None or chess is None:
            return {'error': 'Chess functionality not available'}
        
        logit_diff = torch.tensor(ablation_result['logit_diff'])
        original_output = torch.tensor(ablation_result['original_output'])
        modified_output = torch.tensor(ablation_result['modified_output'])
        
        # è·å–åˆæ³•ç§»åŠ¨
        lboard = LeelaBoard.from_fen(fen, history_synthesis=True)
        chess_board = chess.Board(fen)
        legal_uci_set = set(move.uci() for move in chess_board.legal_moves)
        
        # æ”¶é›†æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„ç´¢å¼•å’Œlogitå€¼
        legal_indices = []
        legal_original_logits = []
        legal_modified_logits = []
        
        for idx in range(1858):
            try:
                uci = lboard.idx2uci(idx)
                if uci in legal_uci_set:
                    legal_indices.append(idx)
                    legal_original_logits.append(original_output[0, idx].item())
                    legal_modified_logits.append(modified_output[0, idx].item())
            except Exception:
                continue
        
        # è®¡ç®—åŸå§‹å’Œä¿®æ”¹åçš„æ¦‚ç‡ï¼ˆå¯¹åˆæ³•ç§»åŠ¨çš„logitåšsoftmaxï¼‰
        if legal_indices and legal_original_logits and legal_modified_logits:
            # ç¡®ä¿æ˜¯float tensorå¹¶æ”¾åˆ°åŒä¸€device
            device = original_output.device if hasattr(original_output, 'device') else torch.device('cpu')
            
            # ç›´æ¥ä»original_outputå’Œmodified_outputä¸­æå–åˆæ³•ç§»åŠ¨çš„logit
            legal_indices_tensor = torch.tensor(legal_indices, device=device)
            orig_logits_legals = original_output[0, legal_indices_tensor].to(device).float()
            mod_logits_legals = modified_output[0, legal_indices_tensor].to(device).float()
            
            # Softmaxå½’ä¸€åŒ–ï¼ˆå¯¹æ‰€æœ‰åˆæ³•ç§»åŠ¨ï¼‰
            original_probs = torch.softmax(orig_logits_legals, dim=0)
            modified_probs = torch.softmax(mod_logits_legals, dim=0)
            
            # è®¡ç®—æ¦‚ç‡å·®å¼‚ï¼ˆä¿®æ”¹å - åŸå§‹ï¼‰
            prob_diff = modified_probs - original_probs
        else:
            original_probs = torch.tensor([])
            modified_probs = torch.tensor([])
            prob_diff = torch.tensor([])
        
        # è·å–æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„logitå·®å¼‚å’Œæ¦‚ç‡å·®å¼‚
        legal_moves_with_diff = []
        for i, idx in enumerate(legal_indices):
            try:
                uci = lboard.idx2uci(idx)
                logit_diff_value = logit_diff[0, idx].item()
                original_logit = legal_original_logits[i]
                modified_logit = legal_modified_logits[i]
                
                # è·å–æ¦‚ç‡å·®å¼‚
                prob_diff_value = prob_diff[i].item() if i < len(prob_diff) else 0.0
                original_prob = original_probs[i].item() if i < len(original_probs) else 0.0
                modified_prob = modified_probs[i].item() if i < len(modified_probs) else 0.0
                
                legal_moves_with_diff.append({
                    'uci': uci,
                    'diff': logit_diff_value,
                    'original_logit': original_logit,
                    'modified_logit': modified_logit,
                    'prob_diff': prob_diff_value,
                    'original_prob': original_prob,
                    'modified_prob': modified_prob,
                    'idx': idx
                })
            except Exception:
                continue
        
        # ç”ŸæˆæŒ‰æ¦‚ç‡å·®å¼‚æ’åºçš„åˆ—è¡¨ï¼ˆé™åºï¼‰
        sorted_by_prob = sorted(
            legal_moves_with_diff,
            key=lambda x: x['modified_prob'] - x['original_prob'],
            reverse=True
        )
        # ä»ä¿ç•™logitå·®å¼‚æ’åºä»¥å¤‡éœ€è¦
        sorted_by_logit = sorted(legal_moves_with_diff, key=lambda x: x['diff'], reverse=True)
        
        # åŸºäºæ¦‚ç‡å·®å¼‚çš„å‰å5ä¸ªï¼ˆæ­£å‘ä¿ƒè¿›=æ¦‚ç‡æå‡æœ€å¤šï¼›æŠ‘åˆ¶=æ¦‚ç‡ä¸‹é™æœ€å¤šï¼‰
        promoting_moves = sorted_by_prob[:5]
        inhibiting_moves = list(reversed(sorted_by_prob[-5:]))
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_legal_moves = len(legal_moves_with_diff)
        if total_legal_moves > 0:
            avg_logit_diff = (sum(x['diff'] for x in legal_moves_with_diff) / 
                            total_legal_moves)
            max_logit_diff = max(x['diff'] for x in legal_moves_with_diff)
            min_logit_diff = min(x['diff'] for x in legal_moves_with_diff)
            
            # æ¦‚ç‡å·®å¼‚ç»Ÿè®¡ï¼ˆä¿®æ”¹å - åŸå§‹ï¼‰
            avg_prob_diff = (sum((x['modified_prob'] - x['original_prob']) for x in legal_moves_with_diff) / 
                           total_legal_moves)
            max_prob_diff = max((x['modified_prob'] - x['original_prob']) for x in legal_moves_with_diff)
            min_prob_diff = min((x['modified_prob'] - x['original_prob']) for x in legal_moves_with_diff)
        else:
            avg_logit_diff = max_logit_diff = min_logit_diff = 0
            avg_prob_diff = max_prob_diff = min_prob_diff = 0
        
        return {
            # ç‰¹å¾ç¼ºå¤±ä¿ƒè¿›çš„ç§»åŠ¨ï¼ˆlogitä¸‹é™ï¼‰
            'promoting_moves': promoting_moves,
            # ç‰¹å¾ç¼ºå¤±æŠ‘åˆ¶çš„ç§»åŠ¨ï¼ˆlogitä¸Šå‡ï¼‰
            'inhibiting_moves': inhibiting_moves,
            'statistics': {
                'total_legal_moves': total_legal_moves,
                'avg_logit_diff': avg_logit_diff,
                'max_logit_diff': max_logit_diff,
                'min_logit_diff': min_logit_diff,
                'avg_prob_diff': avg_prob_diff,
                'max_prob_diff': max_prob_diff,
                'min_prob_diff': min_prob_diff
            },
            'ablation_info': {
                'feature_type': ablation_result['feature_type'],
                'layer': ablation_result['layer'],
                'pos': ablation_result['pos'],
                'feature': ablation_result['feature'],
                'activation_value': ablation_result['activation_value'],
                'hook_name': ablation_result['hook_name']
            },
            # è¿”å›æŒ‰æ¦‚ç‡å·®å¼‚æ’åºçš„å‰10ä¸ªç»“æœï¼Œä¾¿äºå‰ç«¯ç›´æ¥å±•ç¤º
            'top_moves_by_prob': sorted_by_prob[:10]
        }


# å…¨å±€åˆ†æå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œä»…æ”¯æŒBT4ï¼‰
_patching_analyzer = None

def get_patching_analyzer(metadata: Optional[Dict[str, Any]] = None) -> PatchingAnalyzer:
    """è·å–æˆ–åˆ›å»ºä»…æ”¯æŒBT4çš„patchingåˆ†æå™¨å®ä¾‹ã€‚metadataå‚æ•°ä¿ç•™ä»¥ä¿è¯å…¼å®¹æ€§ã€‚"""
    del metadata  # BT4æ¨¡å¼ä¸‹ä¸å†æ ¹æ®metadataåˆ‡æ¢æ¨¡å‹
    global _patching_analyzer
    
    if _patching_analyzer is not None:
        return _patching_analyzer
    
    try:
        from transformer_lens import HookedTransformer
        from lm_saes import SparseAutoEncoder, LowRankSparseAttention
        
        print("ğŸ” æ­£åœ¨åˆå§‹åŒ–BT4 Patchingåˆ†æå™¨...")
        print(f"ğŸ“ TCè·¯å¾„: {BT4_TC_BASE_PATH}")
        print(f"ğŸ“ LORSAè·¯å¾„: {BT4_LORSA_BASE_PATH}")
        print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {BT4_MODEL_NAME}")
        
        model = HookedTransformer.from_pretrained_no_processing(
            BT4_MODEL_NAME,
            dtype=torch.float32,
        ).eval()
        
        transcoders = {}
        for layer in range(15):
            tc_path = f"{BT4_TC_BASE_PATH}/L{layer}"
            print(f"ğŸ“ åŠ è½½TC L{layer}: {tc_path}")
            transcoders[layer] = SparseAutoEncoder.from_pretrained(
                tc_path,
                dtype=torch.float32,
                device='cuda',
            )
        
        lorsas = []
        for layer in range(15):
            lorsa_path = f"{BT4_LORSA_BASE_PATH}/L{layer}"
            print(f"ğŸ“ åŠ è½½LORSA L{layer}: {lorsa_path}")
            lorsas.append(LowRankSparseAttention.from_pretrained(
                lorsa_path, 
                device='cuda'
            ))
        
        _patching_analyzer = PatchingAnalyzer(model, transcoders, lorsas)
        print("âœ… BT4 Patchingåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Patchingåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    return _patching_analyzer


def run_feature_steering_analysis(fen: str, feature_type: str, layer: int, 
                         pos: int, feature: int, steering_scale: int, 
                         metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """è¿è¡Œpatchingåˆ†æçš„å…¬å…±æ¥å£"""
    analyzer = get_patching_analyzer(metadata)
    
    # è¿è¡Œæ¶ˆèåˆ†æ
    # ablation_result = analyzer.hook_based_ablation_analysis(
    ablation_result = analyzer.steering_analysis(
        feature_type=feature_type,
        layer=layer,
        pos=pos,
        feature=feature,
        steering_scale=steering_scale,
        fen=fen
    )
    
    if ablation_result is None:
        return {'error': 'è¯¥ä½ç½®æ²¡æœ‰æ¿€æ´»å€¼ï¼Œæ— æ³•è¿›è¡Œæ¶ˆèåˆ†æ'}
    
    # åˆ†æç»“æœ
    analysis_result = analyzer.analyze_steering_results(ablation_result, fen)
    
    return analysis_result