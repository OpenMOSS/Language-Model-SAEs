import torch
from typing import Dict, Any, Optional, List, Tuple
from lm_saes import SparseAutoEncoder, LowRankSparseAttention
from transformer_lens import HookedTransformer
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from lm_saes.circuit.leela_board import LeelaBoard
    import chess
except ImportError:
    print("WARNING: leela_interp not found, chess functionality will be limited")
    LeelaBoard = None
    chess = None

try:
    from src.chess_utils import get_move_from_policy_output_with_prob
    from src.chess_utils.move import get_value_from_output
except Exception:
    get_move_from_policy_output_with_prob = None
    get_value_from_output = None

# å…¨å±€ BT4 é…ç½®å¸¸é‡ï¼ˆå…¼å®¹è„šæœ¬è¿è¡Œå’Œ package å¯¼å…¥ï¼‰
try:
    from .constants import BT4_MODEL_NAME, BT4_TC_BASE_PATH, BT4_LORSA_BASE_PATH, get_bt4_sae_combo
except ImportError:
    from constants import BT4_MODEL_NAME, BT4_TC_BASE_PATH, BT4_LORSA_BASE_PATH, get_bt4_sae_combo


class PatchingAnalyzer:
    """æ¶ˆèåˆ†æå™¨ï¼Œç”¨äºåˆ†æç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“"""
    
    def __init__(self, model: HookedTransformer, 
                 transcoders: Dict[int, SparseAutoEncoder], 
                 lorsas: List[LowRankSparseAttention]):
        self.model = model
        self.transcoders = transcoders
        self.lorsas = lorsas
        
        # é¢„è®¡ç®—WDæƒé‡
        self.tc_WDs = {}
        self.lorsa_WDs = {}
        
        for layer in range(15):
            self.tc_WDs[layer] = transcoders[layer].W_D
            self.lorsa_WDs[layer] = lorsas[layer].W_O
    
    def _get_cache(self, fen: str):
        """è¿è¡Œæ¨¡å‹å¹¶è¿”å› cacheã€‚"""
        _, cache = self.model.run_with_cache(fen, prepend_bos=False)
        return cache

    def _get_lorsa_sparse_acts(self, cache: dict, layer: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå±‚çš„ LoRSA sparse activations: [batch,pos,feature] çš„ sparse_coo å½¢å¼ã€‚"""
        lorsa_hook = f"blocks.{layer}.hook_attn_in"
        if lorsa_hook not in cache:
            available_hooks = [k for k in cache.keys() if f"blocks.{layer}" in str(k)]
            raise KeyError(
                f"Missing hook '{lorsa_hook}' in cache. Available (sample): {available_hooks[:20]}"
            )
        lorsa_input = cache[lorsa_hook]
        lorsa_dense_activation = self.lorsas[layer].encode(lorsa_input)
        return lorsa_dense_activation.to_sparse_coo()

    def _get_tc_sparse_acts(self, cache: dict, layer: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå±‚çš„ Transcoder sparse activations: [batch,pos,feature] çš„ sparse_coo å½¢å¼ã€‚"""
        tc_hook = f"blocks.{layer}.resid_mid_after_ln"
        if tc_hook not in cache:
            available_hooks = [k for k in cache.keys() if f"blocks.{layer}" in str(k)]
            raise KeyError(
                f"Missing hook '{tc_hook}' in cache. Available (sample): {available_hooks[:20]}"
            )
        tc_input = cache[tc_hook]
        tc_dense_activation = self.transcoders[layer].encode(tc_input)
        return tc_dense_activation.to_sparse_coo()
    
    def steering_analysis(self, feature_type: str, layer: int, 
                                   pos: int, feature: int, steering_scale: int, 
                                   fen: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨hookè¿›è¡Œæ¶ˆèåˆ†æ"""
        
        # ç¡®ä¿æ— æ®‹ç•™hook
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        
        # è·å–æ¿€æ´»å€¼ï¼šåªè®¡ç®—å½“å‰ feature_type + å½“å‰ layerï¼Œé¿å…è®¿é—®æ— å…³ hook
        cache = self._get_cache(fen)
        if feature_type == 'transcoder':
            activations = self._get_tc_sparse_acts(cache, layer)
            WDs = self.tc_WDs[layer]
        elif feature_type == 'lorsa':
            activations = self._get_lorsa_sparse_acts(cache, layer)
            WDs = self.lorsa_WDs[layer]
        else:
            raise ValueError("feature_typeå¿…é¡»æ˜¯'transcoder'æˆ–'lorsa'")
        
        # æŸ¥æ‰¾æ¿€æ´»å€¼
        target_indices = torch.tensor([0, pos, feature], 
                                    device=activations.indices().device)
        matches = (activations.indices() == 
                  target_indices.unsqueeze(1)).all(dim=0)
        
        if not matches.any():
            print('è¯¥ä½ç½®æ²¡æœ‰æ¿€æ´»å€¼ï¼Œæ— æ³•è¿›è¡Œæ¶ˆèåˆ†æ')
            return None
        
        activation_value = activations.values()[matches].item()
        
        # è®¡ç®—ç‰¹å¾è´¡çŒ®
        feature_contribution = activation_value * WDs[feature]  # [768]
        
        # ç¡®å®šè¦ä¿®æ”¹çš„hookä½ç½®
        if feature_type == 'transcoder':
            hook_name = f'blocks.{layer}.hook_mlp_out'
        else:  # lorsa
            hook_name = f'blocks.{layer}.hook_attn_out'
        
        # å†æ¬¡ç¡®ä¿æ— hookå¹¶è·å–åŸå§‹è¾“å‡ºï¼ˆæ— ä¿®æ”¹ï¼‰
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        print(f"ğŸ” è°ƒç”¨ model.run_with_cacheï¼Œfen: {fen}")
        print(f"ğŸ” fen ç±»å‹: {type(fen)}, é•¿åº¦: {len(fen) if isinstance(fen, str) else 'N/A'}")

        original_output, cache = self.model.run_with_cache(fen, prepend_bos=False)

        print(f"ğŸ” model.run_with_cache è¿”å›:")
        print(f"ğŸ“Š original_output ç±»å‹: {type(original_output)}")
        print(f"ğŸ“Š original_output é•¿åº¦: {len(original_output) if hasattr(original_output, '__len__') else 'N/A'}")
        if hasattr(original_output, '__getitem__'):
            for i in range(min(3, len(original_output))):
                item = original_output[i]
                print(f"ğŸ“Š original_output[{i}] ç±»å‹: {type(item)}")
                if hasattr(item, 'shape'):
                    print(f"ğŸ“Š original_output[{i}] å½¢çŠ¶: {item.shape}")
                elif hasattr(item, '__len__'):
                    print(f"ğŸ“Š original_output[{i}] é•¿åº¦: {len(item)}")
                    if len(item) > 0 and isinstance(item, (list, tuple)):
                        print(f"ğŸ“Š original_output[{i}][0] ç±»å‹: {type(item[0])}")
                        if hasattr(item[0], 'shape'):
                            print(f"ğŸ“Š original_output[{i}][0] å½¢çŠ¶: {item[0].shape}")

        # æ£€æŸ¥policy logitsçš„å…·ä½“å½¢çŠ¶
        policy_logits = original_output[0]
        print(f"ğŸ“Š policy_logits å½¢çŠ¶: {policy_logits.shape}")
        print(f"ğŸ“Š policy_logits[:5]: {policy_logits[:5].tolist() if hasattr(policy_logits, 'tolist') else policy_logits[:5]}")
        
        # å®šä¹‰hookä¿®æ”¹å‡½æ•°
        def modify_hook(tensor, hook):
            modified_activation = tensor.clone()
            modified_activation[0, pos] = modified_activation[0, pos] + (steering_scale - 1) * feature_contribution
            return modified_activation
        
        # è¿è¡Œä¿®æ”¹åçš„æ¨¡å‹ï¼ˆä»…æœ¬æ¬¡ç”Ÿæ•ˆçš„hookï¼‰
        self.model.add_hook(hook_name, modify_hook)
        modified_output, _ = self.model.run_with_cache(
            fen, prepend_bos=False)
        # æ¸…ç†hookï¼Œé¿å…å½±å“åç»­è¯·æ±‚
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        
        # è®¡ç®—logitå·®å¼‚
        logit_diff = modified_output[0] - original_output[0]

        # è®¡ç®—valueå·®å¼‚ (Win - Loss)
        original_value = float(original_output[1][0][0] - original_output[1][0][2]) if get_value_from_output else 0.0
        modified_value = float(modified_output[1][0][0] - modified_output[1][0][2]) if get_value_from_output else 0.0
        value_diff = modified_value - original_value

        print(f"ğŸ” steering_analysis è¿”å›æ•°æ®:")
        print(f"ğŸ“Š original_output[0] å½¢çŠ¶: {original_output[0].shape}")
        print(f"ğŸ“Š modified_output[0] å½¢çŠ¶: {modified_output[0].shape}")
        print(f"ğŸ“Š logit_diff å½¢çŠ¶: {logit_diff.shape}")

        # ç¡®ä¿policy logitsæ˜¯æ­£ç¡®çš„å½¢çŠ¶
        policy_original = original_output[0]
        policy_modified = modified_output[0]

        # å¦‚æœæ˜¯ [1, 1858]ï¼Œå– [0] å¾—åˆ° [1858]
        if policy_original.ndim == 2:
            policy_original = policy_original[0]
        if policy_modified.ndim == 2:
            policy_modified = policy_modified[0]

        print(f"ğŸ“Š å¤„ç†åçš„policy_original å½¢çŠ¶: {policy_original.shape}")
        print(f"ğŸ“Š å¤„ç†åçš„policy_modified å½¢çŠ¶: {policy_modified.shape}")

        result = {
            'feature_type': feature_type,
            'layer': layer,
            'pos': pos,
            'feature': feature,
            'activation_value': activation_value,
            'feature_contribution': feature_contribution.detach().cpu().numpy().tolist(),
            'original_output': policy_original.detach().cpu().numpy().tolist(),
            'modified_output': policy_modified.detach().cpu().numpy().tolist(),
            'logit_diff': logit_diff.detach().cpu().numpy().tolist(),
            'original_value': float(original_value),
            'modified_value': float(modified_value),
            'value_diff': float(value_diff),
            'hook_name': hook_name
        }

        print(f"ğŸ“‹ è¿”å›çš„ original_output é•¿åº¦: {len(result['original_output'])}")
        print(f"ğŸ“‹ è¿”å›çš„ modified_output é•¿åº¦: {len(result['modified_output'])}")
        print(f"ğŸ“‹ è¿”å›çš„ logit_diff é•¿åº¦: {len(result['logit_diff'])}")

        return result

    def multi_steering_analysis(
        self,
        fen: str,
        feature_type: str,
        layer: int,
        nodes: List[Dict[str, Any]],
    ) -> Optional[Dict[str, Any]]:
        # ç¡®ä¿æ— æ®‹ç•™hook
        try:
            self.model.reset_hooks()
        except Exception:
            pass

        # å¯¹äºmulti steeringï¼Œæš‚æ—¶ä¸è®¡ç®—valueï¼ˆé¿å…ç´¢å¼•é”™è¯¯ï¼‰
        get_value_from_output = None

        # å‚æ•°æ ¡éªŒä¸è§„èŒƒåŒ–
        if not isinstance(nodes, list) or len(nodes) == 0:
            raise ValueError("nodes must be a non-empty list")
        normalized_nodes: List[Dict[str, Any]] = []
        for node in nodes:
            if not isinstance(node, dict):
                continue
            pos = node.get("pos")
            feature = node.get("feature")
            steering_scale = node.get("steering_scale", 1)
            if not isinstance(pos, int) or not isinstance(feature, int):
                continue
            if not isinstance(steering_scale, (int, float)):
                steering_scale = 1
            normalized_nodes.append(
                {"pos": pos, "feature": feature, "steering_scale": float(steering_scale)}
            )
        if len(normalized_nodes) == 0:
            raise ValueError("nodes is empty after validation")

        # è·å–æ¿€æ´»å€¼ï¼šåªè®¡ç®—å½“å‰ feature_type + å½“å‰ layerï¼Œé¿å…è®¿é—®æ— å…³ hook
        cache = self._get_cache(fen)
        if feature_type == "transcoder":
            activations = self._get_tc_sparse_acts(cache, layer)
            WDs = self.tc_WDs[layer]
            hook_name = f"blocks.{layer}.hook_mlp_out"
        elif feature_type == "lorsa":
            activations = self._get_lorsa_sparse_acts(cache, layer)
            WDs = self.lorsa_WDs[layer]
            hook_name = f"blocks.{layer}.hook_attn_out"
        else:
            raise ValueError("feature_typeå¿…é¡»æ˜¯'transcoder'æˆ–'lorsa'")

        # å°†éœ€è¦çš„ (pos, feature) åšæˆé›†åˆï¼Œç”¨ä¸€æ¬¡éå† sparse idx/value æŸ¥æ‰¾æ¿€æ´»å€¼
        targets = {(n["pos"], n["feature"]) for n in normalized_nodes}
        found_acts: Dict[Tuple[int, int], float] = {}
        try:
            idx = activations.indices()  # [3, nnz]
            val = activations.values()
            # idx[0] = batch index, idx[1] = pos, idx[2] = feature
            for j in range(idx.shape[1]):
                if int(idx[0, j].item()) != 0:
                    continue
                key = (int(idx[1, j].item()), int(idx[2, j].item()))
                if key in targets:
                    found_acts[key] = float(val[j].item())
        except Exception:
            # å¦‚æœ sparse ç»“æ„ä¸ç¬¦åˆé¢„æœŸï¼Œç›´æ¥å¤±è´¥
            raise ValueError("failed to parse sparse activations")

        # è¦æ±‚æ¯ä¸ª node éƒ½èƒ½åœ¨å¯¹åº” pos å–åˆ°æ¿€æ´»å€¼ï¼Œå¦åˆ™è¿”å› Noneï¼ˆä¸å• feature è¡Œä¸ºä¸€è‡´ï¼‰
        missing = [(p, f) for (p, f) in targets if (p, f) not in found_acts]
        if missing:
            print(f"è¯¥ä½ç½®æ²¡æœ‰æ¿€æ´»å€¼ï¼Œæ— æ³•è¿›è¡Œå¤š feature steering: missing={missing}")
            return None

        # è®¡ç®—æ¯ä¸ª pos çš„æ€» deltaï¼ˆå¤šä¸ª feature å¯èƒ½è½åœ¨åŒä¸€ä¸ª posï¼‰
        pos_to_delta: Dict[int, torch.Tensor] = {}
        node_details: List[Dict[str, Any]] = []
        for n in normalized_nodes:
            pos = n["pos"]
            feature = n["feature"]
            scale = n["steering_scale"]
            activation_value = found_acts[(pos, feature)]
            feature_contribution = activation_value * WDs[feature]  # [d_model]
            delta = (scale - 1.0) * feature_contribution
            if pos not in pos_to_delta:
                pos_to_delta[pos] = delta
            else:
                pos_to_delta[pos] = pos_to_delta[pos] + delta
            node_details.append(
                {
                    "pos": pos,
                    "feature": feature,
                    "steering_scale": scale,
                    "activation_value": activation_value,
                }
            )

        # åŸå§‹è¾“å‡ºï¼ˆæ— ä¿®æ”¹ï¼‰
        try:
            self.model.reset_hooks()
        except Exception:
            pass
        original_output, _ = self.model.run_with_cache(fen, prepend_bos=False)

        # Hookï¼šåœ¨æŒ‡å®š pos ä¸ŠåŠ ä¸Š delta
        def modify_hook(tensor, hook):
            modified_activation = tensor.clone()
            for pos, delta in pos_to_delta.items():
                modified_activation[0, pos] = modified_activation[0, pos] + delta
            return modified_activation

        self.model.add_hook(hook_name, modify_hook)
        modified_output, _ = self.model.run_with_cache(fen, prepend_bos=False)
        try:
            self.model.reset_hooks()
        except Exception:
            pass

        # è®¡ç®—logitå·®å¼‚ï¼Œå¤„ç†ä¸åŒæ ¼å¼çš„è¾“å‡º
        try:
            # å¤„ç†original_output - å¯èƒ½æ˜¯å¼ é‡æˆ–åˆ—è¡¨
            if isinstance(original_output, torch.Tensor):
                orig_logits = original_output
                if orig_logits.ndim == 2 and orig_logits.shape[0] == 1:
                    orig_logits = orig_logits[0]  # ä» [1, 1858] å˜ä¸º [1858]
            elif isinstance(original_output, (list, tuple)) and len(original_output) > 0:
                if isinstance(original_output[0], torch.Tensor):
                    orig_logits = original_output[0]
                    if orig_logits.ndim == 2 and orig_logits.shape[0] == 1:
                        orig_logits = orig_logits[0]
                else:
                    # å¤„ç†åµŒå¥—åˆ—è¡¨çš„æƒ…å†µ
                    orig_logits = torch.tensor(original_output[0])
                    if orig_logits.ndim == 2 and orig_logits.shape[0] == 1:
                        orig_logits = orig_logits[0]
            else:
                raise ValueError(f"Unexpected original_output format: {type(original_output)}")

            # å¤„ç†modified_output - å¯èƒ½æ˜¯å¼ é‡æˆ–åˆ—è¡¨
            if isinstance(modified_output, torch.Tensor):
                mod_logits = modified_output
                if mod_logits.ndim == 2 and mod_logits.shape[0] == 1:
                    mod_logits = mod_logits[0]  # ä» [1, 1858] å˜ä¸º [1858]
            elif isinstance(modified_output, (list, tuple)) and len(modified_output) > 0:
                if isinstance(modified_output[0], torch.Tensor):
                    mod_logits = modified_output[0]
                    if mod_logits.ndim == 2 and mod_logits.shape[0] == 1:
                        mod_logits = mod_logits[0]
                else:
                    # å¤„ç†åµŒå¥—åˆ—è¡¨çš„æƒ…å†µ
                    mod_logits = torch.tensor(modified_output[0])
                    if mod_logits.ndim == 2 and mod_logits.shape[0] == 1:
                        mod_logits = mod_logits[0]
            else:
                raise ValueError(f"Unexpected modified_output format: {type(modified_output)}")

            print(f"Original logits shape: {orig_logits.shape}")
            print(f"Modified logits shape: {mod_logits.shape}")

            logit_diff = mod_logits - orig_logits
            print(f"Logit diff shape: {logit_diff.shape}")
        except (RuntimeError, IndexError, TypeError) as e:
            print(f"Error computing logit difference: {e}")
            print(f"Original output type: {type(original_output)}, length: {len(original_output) if hasattr(original_output, '__len__') else 'N/A'}")
            print(f"Modified output type: {type(modified_output)}, length: {len(modified_output) if hasattr(modified_output, '__len__') else 'N/A'}")
            if len(original_output) > 0:
                print(f"original_output[0] type: {type(original_output[0])}, shape: {getattr(original_output[0], 'shape', 'no shape')}")
            if len(modified_output) > 0:
                print(f"modified_output[0] type: {type(modified_output[0])}, shape: {getattr(modified_output[0], 'shape', 'no shape')}")
            raise ValueError(f"Failed to compute logit difference: {e}")

        # è®¡ç®—valueå·®å¼‚ (Win - Loss)ï¼Œæ·»åŠ å®‰å…¨æ£€æŸ¥
        original_value = 0.0
        modified_value = 0.0
        value_diff = 0.0

        # å¯¹äºmulti steeringï¼Œæš‚æ—¶ä¸è®¡ç®—valueï¼ˆé¿å…ç´¢å¼•é”™è¯¯ï¼‰
        # if get_value_from_output and len(original_output) > 1 and len(modified_output) > 1:
        #     try:
        #         original_value = float(original_output[1][0][0] - original_output[1][0][2])
        #         modified_value = float(modified_output[1][0][0] - modified_output[1][0][2])
        #         value_diff = modified_value - original_value
        #     except (IndexError, TypeError):
        #         # å¦‚æœvalueè¾“å‡ºæ ¼å¼ä¸æ­£ç¡®ï¼Œä¿æŒé»˜è®¤å€¼0.0
        #         pass 

        # ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®
        def safe_to_numpy(tensor_or_list):
            if isinstance(tensor_or_list, torch.Tensor):
                # ç¡®ä¿æ˜¯ä¸€ç»´çš„ [1858] å½¢çŠ¶
                if tensor_or_list.ndim == 2 and tensor_or_list.shape[0] == 1:
                    tensor_or_list = tensor_or_list[0]
                return tensor_or_list.detach().cpu().numpy().tolist()
            else:
                # å¤„ç†åµŒå¥—åˆ—è¡¨çš„æƒ…å†µ
                tensor = torch.tensor(tensor_or_list)
                if tensor.ndim == 2 and tensor.shape[0] == 1:
                    tensor = tensor[0]
                return tensor.detach().cpu().numpy().tolist()

        return {
            "feature_type": feature_type,
            "layer": layer,
            "nodes": node_details,
            "original_output": safe_to_numpy(orig_logits),
            "modified_output": safe_to_numpy(mod_logits),
            "logit_diff": logit_diff.detach().cpu().numpy().tolist(),
            "original_value": float(original_value),
            "modified_value": float(modified_value),
            "value_diff": float(value_diff),
            "hook_name": hook_name,
        }
    
    def analyze_steering_results(self, ablation_result: Dict[str, Any],
                               fen: str) -> Dict[str, Any]:
        """åˆ†ææ¶ˆèç»“æœï¼Œè¿”å›å¯¹åˆæ³•ç§»åŠ¨çš„å½±å“"""
        if ablation_result is None:
            return None

        if LeelaBoard is None or chess is None:
            return {'error': 'Chess functionality not available'}

        print(f"ğŸ” analyze_steering_results å¼€å§‹")
        orig_out = ablation_result.get('original_output', [])

        logit_diff = torch.tensor(ablation_result['logit_diff'])
        original_output = torch.tensor(ablation_result['original_output'])
        modified_output = torch.tensor(ablation_result['modified_output'])

        print(f"ğŸ”§ åˆ›å»ºtensorå - original_output å½¢çŠ¶: {original_output.shape}")

        # ç¡®ä¿è¾“å‡ºæ˜¯æ­£ç¡®çš„å½¢çŠ¶ [1858] è€Œä¸æ˜¯ [1, 1858]
        if original_output.ndim == 2 and original_output.shape[0] == 1:
            original_output = original_output[0]
        if modified_output.ndim == 2 and modified_output.shape[0] == 1:
            modified_output = modified_output[0]

        # ç¡®ä¿ logit_diff ä¸ original_output å½¢çŠ¶ä¸€è‡´
        if logit_diff.ndim == 2 and logit_diff.shape[0] == 1:
            logit_diff = logit_diff[0]

        lboard = LeelaBoard.from_fen(fen, history_synthesis=True)
        chess_board = chess.Board(fen)
        legal_uci_set = set(move.uci() for move in chess_board.legal_moves)
        
        # æ”¶é›†æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„ idx / uci / logit
        legal_moves: list[dict[str, Any]] = []
        for idx in range(1858):
            try:
                uci = lboard.idx2uci(idx)
            except Exception:
                continue
            if uci not in legal_uci_set:
                continue
            legal_moves.append(
                {
                    "idx": idx,
                    "uci": uci,
                    "original_logit": float(original_output[idx].item()),
                    "modified_logit": float(modified_output[idx].item()),
                }
            )

        # ç»Ÿä¸€æ¦‚ç‡å£å¾„ï¼šåœ¨æ‰€æœ‰åˆæ³•ç§»åŠ¨ä¸­è®¡ç®—softmaxæ¦‚ç‡
        original_prob_by_uci: dict[str, float] = {}
        modified_prob_by_uci: dict[str, float] = {}

        # ä»policy logitsä¸­æå–åˆæ³•ç§»åŠ¨çš„æ¦‚ç‡
        def get_legal_move_probs(policy_logits: torch.Tensor, fen: str) -> dict[str, float]:
            """ä»policy logitsä¸­è®¡ç®—æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„softmaxæ¦‚ç‡"""
            print(f"ğŸ” å¼€å§‹è®¡ç®—æ¦‚ç‡, policy_logitså½¢çŠ¶: {policy_logits.shape}, ç±»å‹: {type(policy_logits)}")

            # ç¡®ä¿policy_logitsæ˜¯æ­£ç¡®çš„å½¢çŠ¶
            if policy_logits.ndim == 2:
                policy_logits = policy_logits[0]  # ç§»é™¤batchç»´åº¦
                print(f"ğŸ“Š ç§»é™¤batchç»´åº¦åå½¢çŠ¶: {policy_logits.shape}")

            # è·å–åˆæ³•ç§»åŠ¨
            chess_board = chess.Board(fen)
            legal_uci_set = set(move.uci() for move in chess_board.legal_moves)
            print(f"ğŸ“‹ åˆæ³•ç§»åŠ¨æ•°é‡: {len(legal_uci_set)}")

            # æå–åˆæ³•ç§»åŠ¨çš„logits
            legal_logits = []
            legal_ucis = []
            for idx in range(1858):
                try:
                    uci = lboard.idx2uci(idx)
                    if uci in legal_uci_set:
                        logit_value = float(policy_logits[idx].item())
                        legal_logits.append(logit_value)
                        legal_ucis.append(uci)
                except Exception:
                    continue

            print(f"ğŸ“Š æå–åˆ°çš„åˆæ³•ç§»åŠ¨æ•°é‡: {len(legal_logits)}")
            if not legal_logits:
                return {}

            # è®¡ç®—softmaxæ¦‚ç‡
            legal_logits_tensor = torch.tensor(legal_logits)
            print(f"ğŸ”¢ logitsèŒƒå›´: {min(legal_logits):.3f} - {max(legal_logits):.3f}")

            probs = torch.softmax(legal_logits_tensor, dim=0)
            prob_by_uci = {uci: float(prob.item()) for uci, prob in zip(legal_ucis, probs)}

            print(f"ğŸ“ˆ æ¦‚ç‡èŒƒå›´: {min(prob_by_uci.values()):.6f} - {max(prob_by_uci.values()):.6f}")
            return prob_by_uci

        try:
            original_prob_by_uci = get_legal_move_probs(original_output, fen)
            modified_prob_by_uci = get_legal_move_probs(modified_output, fen)
            print(f"âœ… æ¦‚ç‡è®¡ç®—æˆåŠŸ: original_prob_by_uci æœ‰ {len(original_prob_by_uci)} ä¸ªç§»åŠ¨, modified_prob_by_uci æœ‰ {len(modified_prob_by_uci)} ä¸ªç§»åŠ¨")
            if original_prob_by_uci:
                sample_uci = list(original_prob_by_uci.keys())[0]
                print(f"ç¤ºä¾‹æ¦‚ç‡ - {sample_uci}: åŸå§‹={original_prob_by_uci[sample_uci]:.6f}, ä¿®æ”¹å={modified_prob_by_uci.get(sample_uci, 0):.6f}")
        except Exception as e:
            print(f"âŒ è®¡ç®—æ¦‚ç‡å¤±è´¥: {e}")
            import traceback
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            original_prob_by_uci = {}
            modified_prob_by_uci = {}
        
        # è·å–æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„logitå·®å¼‚å’Œæ¦‚ç‡å·®å¼‚
        # å–å‰kä¸ªæœ€é«˜æ¦‚ç‡çš„ç§»åŠ¨ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹æ¦‚ç‡ï¼Œä¸é‡æ–°å½’ä¸€åŒ–ï¼‰
        topk = 5
        def _get_topk_probs(prob_by_uci: dict[str, float], k: int) -> dict[str, float]:
            if not prob_by_uci:
                return {}
            items = sorted(prob_by_uci.items(), key=lambda x: x[1], reverse=True)[: max(1, int(k))]
            return {uci: prob for uci, prob in items}

        original_prob_topk_by_uci = _get_topk_probs(original_prob_by_uci, topk)
        modified_prob_topk_by_uci = _get_topk_probs(modified_prob_by_uci, topk)

        legal_moves_with_diff: list[dict[str, Any]] = []
        for m in legal_moves:
            uci = m["uci"]
            idx = m["idx"]
            original_prob = float(original_prob_by_uci.get(uci, 0.0))
            modified_prob = float(modified_prob_by_uci.get(uci, 0.0))
            original_prob_topk = float(original_prob_topk_by_uci.get(uci, 0.0))
            modified_prob_topk = float(modified_prob_topk_by_uci.get(uci, 0.0))
            # æ ¹æ® logit_diff çš„å½¢çŠ¶é€‰æ‹©æ­£ç¡®çš„ç´¢å¼•æ–¹å¼
            if logit_diff.ndim == 2:
                diff_value = float(logit_diff[0, idx].item())
            else:
                diff_value = float(logit_diff[idx].item())

            legal_moves_with_diff.append(
                {
                    "uci": uci,
                    "diff": diff_value,
                    "original_logit": m["original_logit"],
                    "modified_logit": m["modified_logit"],
                    "prob_diff": float(modified_prob - original_prob),
                    "original_prob": original_prob,
                    "modified_prob": modified_prob,
                    "prob_diff_topk": float(modified_prob_topk - original_prob_topk),
                    "original_prob_topk": original_prob_topk,
                    "modified_prob_topk": modified_prob_topk,
                    "idx": idx,
                }
            )
        
        # æ³¨æ„ï¼šè¿™é‡ŒåŒºåˆ†ä¸‰ç§æ’åºå£å¾„ï¼Œé¿å…â€œTop Moves by Probâ€è¯­ä¹‰æ··ä¹±
        # 1) prob_diff æ’åºï¼šæ‰¾â€œæ¦‚ç‡æå‡/ä¸‹é™æœ€å¤šâ€çš„èµ°æ³•ï¼ˆæ›´é€‚åˆ promoting/inhibitingï¼‰
        sorted_by_prob_diff = sorted(
            legal_moves_with_diff,
            key=lambda x: x.get("prob_diff", 0.0),
            reverse=True,
        )
        # 2) prob æ’åºï¼šæ‰¾â€œä¿®æ”¹åæ¦‚ç‡æœ€é«˜â€çš„èµ°æ³•ï¼ˆæ›´é€‚åˆ top moves by probï¼‰
        sorted_by_modified_prob = sorted(
            legal_moves_with_diff,
            key=lambda x: x.get("modified_prob", 0.0),
            reverse=True,
        )
        # 3) top-k prob æ’åºï¼šåŒ¹é… logit-lens çš„å±•ç¤ºå£å¾„
        sorted_by_modified_prob_topk = sorted(
            legal_moves_with_diff,
            key=lambda x: x.get("modified_prob_topk", 0.0),
            reverse=True,
        )
        # ä»ä¿ç•™logitå·®å¼‚æ’åºä»¥å¤‡éœ€è¦
        sorted_by_logit = sorted(legal_moves_with_diff, key=lambda x: x['diff'], reverse=True)
        
        # åŸºäºæ¦‚ç‡å·®å¼‚çš„å‰å5ä¸ªï¼ˆæ­£å‘ä¿ƒè¿›=æ¦‚ç‡æå‡æœ€å¤šï¼›æŠ‘åˆ¶=æ¦‚ç‡ä¸‹é™æœ€å¤šï¼‰
        promoting_moves = sorted_by_prob_diff[:5]
        inhibiting_moves = list(reversed(sorted_by_prob_diff[-5:]))
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_legal_moves = len(legal_moves_with_diff)
        if total_legal_moves > 0:
            avg_logit_diff = (sum(x['diff'] for x in legal_moves_with_diff) / 
                            total_legal_moves)
            max_logit_diff = max(x['diff'] for x in legal_moves_with_diff)
            min_logit_diff = min(x['diff'] for x in legal_moves_with_diff)
            
            # æ¦‚ç‡å·®å¼‚ç»Ÿè®¡ï¼ˆä¿®æ”¹å - åŸå§‹ï¼‰
            avg_prob_diff = (sum((x['modified_prob'] - x['original_prob']) for x in legal_moves_with_diff) / 
                           total_legal_moves)
            max_prob_diff = max((x['modified_prob'] - x['original_prob']) for x in legal_moves_with_diff)
            min_prob_diff = min((x['modified_prob'] - x['original_prob']) for x in legal_moves_with_diff)
        else:
            avg_logit_diff = max_logit_diff = min_logit_diff = 0
            avg_prob_diff = max_prob_diff = min_prob_diff = 0
        
        # è®¡ç®—valueç»Ÿè®¡ä¿¡æ¯
        original_value = ablation_result.get('original_value', 0.0)
        modified_value = ablation_result.get('modified_value', 0.0)
        value_diff = ablation_result.get('value_diff', 0.0)

        return {
            # ç‰¹å¾ç¼ºå¤±ä¿ƒè¿›çš„ç§»åŠ¨ï¼ˆlogitä¸‹é™ï¼‰
            'promoting_moves': promoting_moves,
            # ç‰¹å¾ç¼ºå¤±æŠ‘åˆ¶çš„ç§»åŠ¨ï¼ˆlogitä¸Šå‡ï¼‰
            'inhibiting_moves': inhibiting_moves,
            'statistics': {
                'total_legal_moves': total_legal_moves,
                'avg_logit_diff': avg_logit_diff,
                'max_logit_diff': max_logit_diff,
                'min_logit_diff': min_logit_diff,
                'avg_prob_diff': avg_prob_diff,
                'max_prob_diff': max_prob_diff,
                'min_prob_diff': min_prob_diff,
                'original_value': original_value,
                'modified_value': modified_value,
                'value_diff': value_diff
            },
            'ablation_info': {
                'feature_type': ablation_result.get('feature_type'),
                'layer': ablation_result.get('layer'),
                # å• feature æ—¶è¿™äº›å­—æ®µå­˜åœ¨ï¼›å¤š feature æ—¶ç”¨ nodes æ›¿ä»£
                'pos': ablation_result.get('pos'),
                'feature': ablation_result.get('feature'),
                'activation_value': ablation_result.get('activation_value'),
                'nodes': ablation_result.get('nodes'),
                'hook_name': ablation_result.get('hook_name')
            },
            # è¿”å›ä¸¤å¥—"Top moves"ï¼š
            # - top_moves_by_prob: æŒ‰ä¿®æ”¹åæ¦‚ç‡ï¼ˆall-legal softmaxï¼‰æ’åº
            # - top_moves_by_prob_topk: æŒ‰ä¿®æ”¹åæ¦‚ç‡ï¼ˆtop-k legal softmaxï¼ŒåŒ¹é… logit-lensï¼‰æ’åº
            'top_moves_by_prob': sorted_by_modified_prob[:10],
            'top_moves_by_prob_topk': sorted_by_modified_prob_topk[:10],
            # ä¿ç•™ï¼šæŒ‰ prob_diff æ’åºçš„å‰10ä¸ªï¼ˆç”¨äºè¯Šæ–­/å¯¹é½ï¼‰
            'top_moves_by_prob_diff': sorted_by_prob_diff[:10],
        }


# å…¨å±€åˆ†æå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œä»…æ”¯æŒBT4ï¼‰
# ä½¿ç”¨å­—å…¸å­˜å‚¨ä¸åŒç»„åˆçš„åˆ†æå™¨ï¼Œkeyä¸ºcombo_id
_patching_analyzers: Dict[str, PatchingAnalyzer] = {}
_current_combo_id: Optional[str] = None

def clear_patching_analyzer(combo_id: Optional[str] = None):
    """æ¸…ç†æŒ‡å®šç»„åˆçš„patchingåˆ†æå™¨ï¼Œå¦‚æœcombo_idä¸ºNoneåˆ™æ¸…ç†æ‰€æœ‰"""
    global _patching_analyzers, _current_combo_id
    if combo_id is None:
        _patching_analyzers.clear()
        _current_combo_id = None
        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰patchingåˆ†æå™¨")
    elif combo_id in _patching_analyzers:
        del _patching_analyzers[combo_id]
        if _current_combo_id == combo_id:
            _current_combo_id = None
        print(f"ğŸ§¹ å·²æ¸…ç†ç»„åˆ {combo_id} çš„patchingåˆ†æå™¨")

def get_patching_analyzer(metadata: Optional[Dict[str, Any]] = None, combo_id: Optional[str] = None) -> PatchingAnalyzer:
    """
    è·å–æˆ–åˆ›å»ºä»…æ”¯æŒBT4çš„patchingåˆ†æå™¨å®ä¾‹ã€‚
    
    Args:
        metadata: ä¿ç•™å‚æ•°ä»¥ä¿è¯å…¼å®¹æ€§ï¼Œå·²å¼ƒç”¨
        combo_id: SAEç»„åˆIDï¼ˆä¾‹å¦‚ "k_128_e_128"ï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»app.pyè·å–å½“å‰ç»„åˆ
    
    Returns:
        PatchingAnalyzer: åˆ†æå™¨å®ä¾‹
    """
    global _patching_analyzers, _current_combo_id
    
    # ä¼˜å…ˆä» metadata ä¸­è·å–ç»„åˆIDï¼ˆå‰ç«¯ä¼šä¼  sae_combo_idï¼‰
    if combo_id is None and isinstance(metadata, dict):
        meta_combo_id = metadata.get("sae_combo_id")
        if isinstance(meta_combo_id, str) and meta_combo_id.strip():
            combo_id = meta_combo_id.strip()

    # è·å–å½“å‰ç»„åˆID
    if combo_id is None:
        try:
            # å°è¯•ä»app.pyè·å–å½“å‰ç»„åˆ
            import sys
            if 'app' in sys.modules:
                from app import CURRENT_BT4_SAE_COMBO_ID
                combo_id = CURRENT_BT4_SAE_COMBO_ID
            else:
                # å¦‚æœappæ¨¡å—æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤ç»„åˆ
                combo_id = "k_30_e_16"
        except (ImportError, AttributeError):
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨é»˜è®¤ç»„åˆ
            combo_id = "k_30_e_16"
    
    # å¦‚æœå·²ç»æœ‰è¯¥ç»„åˆçš„åˆ†æå™¨ï¼Œç›´æ¥è¿”å›
    if combo_id in _patching_analyzers:
        return _patching_analyzers[combo_id]
    
    try:
        from transformer_lens import HookedTransformer
        from lm_saes import SparseAutoEncoder, LowRankSparseAttention
        
        # è·å–å½“å‰ç»„åˆçš„é…ç½®
        combo_cfg = get_bt4_sae_combo(combo_id)
        tc_base_path = combo_cfg["tc_base_path"]
        lorsa_base_path = combo_cfg["lorsa_base_path"]
        
        print(f"ğŸ” æ­£åœ¨åˆå§‹åŒ–BT4 Patchingåˆ†æå™¨ï¼ˆç»„åˆ: {combo_id}ï¼‰...")
        print(f"ğŸ“ TCè·¯å¾„: {tc_base_path}")
        print(f"ğŸ“ LORSAè·¯å¾„: {lorsa_base_path}")
        print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {BT4_MODEL_NAME}")
        
        # æ„å»ºcache_keyï¼ˆä¸preload_circuit_modelsä¿æŒä¸€è‡´ï¼‰
        cache_key = f"{BT4_MODEL_NAME}::{combo_id}"
        
        # å°è¯•ä»circuits_serviceè·å–ç¼“å­˜çš„æ¨¡å‹ï¼ˆä½¿ç”¨cache_keyï¼‰
        try:
            from circuits_service import get_cached_models
            cached_hooked_model, cached_transcoders, cached_lorsas, _ = get_cached_models(cache_key)
            
            if cached_hooked_model is not None and cached_transcoders is not None and cached_lorsas is not None:
                if len(cached_transcoders) == 15 and len(cached_lorsas) == 15:
                    print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsasï¼ˆç»„åˆ: {combo_id}ï¼‰")
                    model = cached_hooked_model
                    transcoders = cached_transcoders
                    lorsas = cached_lorsas
                else:
                    raise ValueError(f"ç¼“å­˜ä¸å®Œæ•´: transcoders={len(cached_transcoders)}, lorsas={len(cached_lorsas)}")
            else:
                raise ValueError(f"ç¼“å­˜ä¸å­˜åœ¨: cache_key={cache_key}")
        except (ImportError, ValueError) as e:
            print(f"âš ï¸ æ— æ³•ä½¿ç”¨ç¼“å­˜ï¼Œéœ€è¦ç­‰å¾…é¢„åŠ è½½å®Œæˆ: {e}")
            print(f"ğŸ’¡ æç¤º: è¯·å…ˆè°ƒç”¨ /circuit/preload_models é¢„åŠ è½½ç»„åˆ {combo_id} çš„æ¨¡å‹")
            raise RuntimeError(
                f"ç»„åˆ {combo_id} çš„æ¨¡å‹å°šæœªé¢„åŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ /circuit/preload_models æ¥å£é¢„åŠ è½½æ¨¡å‹ï¼Œ"
                f"æˆ–ç­‰å¾…é¢„åŠ è½½å®Œæˆåå†ä½¿ç”¨patchingåˆ†æåŠŸèƒ½ã€‚"
            )
        
        # åˆ›å»ºåˆ†æå™¨å¹¶ç¼“å­˜
        analyzer = PatchingAnalyzer(model, transcoders, lorsas)
        _patching_analyzers[combo_id] = analyzer
        _current_combo_id = combo_id
        print(f"âœ… BT4 Patchingåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸï¼ˆç»„åˆ: {combo_id}ï¼‰")
        return analyzer
    except Exception as e:
        print(f"âŒ Patchingåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


def run_feature_steering_analysis(fen: str, feature_type: str, layer: int, 
                         pos: int, feature: int, steering_scale: int, 
                         metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """è¿è¡Œpatchingåˆ†æçš„å…¬å…±æ¥å£"""
    analyzer = get_patching_analyzer(metadata)
    
    # è¿è¡Œæ¶ˆèåˆ†æ
    # ablation_result = analyzer.hook_based_ablation_analysis(
    ablation_result = analyzer.steering_analysis(
        feature_type=feature_type,
        layer=layer,
        pos=pos,
        feature=feature,
        steering_scale=steering_scale,
        fen=fen
    )
    
    if ablation_result is None:
        return {'error': 'è¯¥ä½ç½®æ²¡æœ‰æ¿€æ´»å€¼ï¼Œæ— æ³•è¿›è¡Œæ¶ˆèåˆ†æ'}
    
    # åˆ†æç»“æœ
    analysis_result = analyzer.analyze_steering_results(ablation_result, fen)
    
    return analysis_result


def run_multi_feature_steering_analysis(
    fen: str,
    feature_type: str,
    layer: int,
    nodes: List[Dict[str, Any]],
    metadata: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    è¿è¡Œå¤š feature steering åˆ†æï¼ˆæ¯ä¸ª feature å¯¹åº”ä¸€ä¸ª positionï¼‰å¹¶è¿”å›ç»“æœã€‚

    Args:
        fen: FEN å­—ç¬¦ä¸²
        feature_type: 'transcoder' æˆ– 'lorsa'
        layer: å±‚å·
        nodes: node åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ è‡³å°‘åŒ…å« pos/feature/steering_scale
        metadata: ä¿ç•™å‚æ•°ä»¥ä¿è¯å…¼å®¹æ€§ï¼ˆç›®å‰ä¸ä½¿ç”¨ï¼‰

    Returns:
        ä¸ run_feature_steering_analysis åŒç»“æ„çš„åˆ†æç»“æœã€‚
    """
    analyzer = get_patching_analyzer(metadata)
    ablation_result = analyzer.multi_steering_analysis(
        fen=fen,
        feature_type=feature_type,
        layer=layer,
        nodes=nodes,
    )
    if ablation_result is None:
        return {"error": "è‡³å°‘æœ‰ä¸€ä¸ª node åœ¨å¯¹åº” pos ä¸Šæ²¡æœ‰æ¿€æ´»å€¼ï¼Œæ— æ³•è¿›è¡Œ steering"}
    return analyzer.analyze_steering_results(ablation_result, fen)