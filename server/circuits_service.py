#!/usr/bin/env python3
"""
Fast tracing test script for chess SAE attribution.
This script can be run with torchrun for distributed execution.
"""

import argparse
import json
import logging
import sys
import time
from pathlib import Path
from typing import Optional, Dict, Any, Tuple, List

import torch
import chess
from transformer_lens import HookedTransformer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from lm_saes import ReplacementModel, LowRankSparseAttention, SparseAutoEncoder
from lm_saes.circuit.attribution_qk import attribute
from lm_saes.circuit.graph_lc0 import Graph
from lm_saes.circuit.utils.create_graph_files import create_graph_files, build_model, create_nodes, create_used_nodes_and_edges, prune_graph
from lm_saes.circuit.leela_board import LeelaBoard
from src.lm_saes.config import MongoDBConfig
from src.lm_saes.database import (
    MongoClient,
    SAERecord,
    DatasetRecord,
    ModelRecord,
)


def setup_logging(log_level: str = "INFO") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


# å…¨å±€ç¼“å­˜ï¼ˆä¸Žapp.pyå…±äº«ï¼‰
_global_hooked_models: Dict[str, HookedTransformer] = {}
_global_transcoders_cache: Dict[str, Dict[int, SparseAutoEncoder]] = {}
_global_lorsas_cache: Dict[str, List[LowRankSparseAttention]] = {}
_global_replacement_models_cache: Dict[str, ReplacementModel] = {}


def get_cached_models(model_name: str) -> Tuple[Optional[HookedTransformer], Optional[Dict[int, SparseAutoEncoder]], Optional[List[LowRankSparseAttention]], Optional[ReplacementModel]]:
    """èŽ·å–ç¼“å­˜çš„æ¨¡åž‹ã€transcoderså’Œlorsas"""
    global _global_hooked_models, _global_transcoders_cache, _global_lorsas_cache, _global_replacement_models_cache
    
    hooked_model = _global_hooked_models.get(model_name)
    transcoders = _global_transcoders_cache.get(model_name)
    lorsas = _global_lorsas_cache.get(model_name)
    replacement_model = _global_replacement_models_cache.get(model_name)
    
    return hooked_model, transcoders, lorsas, replacement_model


def set_cached_models(
    model_name: str,
    hooked_model: HookedTransformer,
    transcoders: Dict[int, SparseAutoEncoder],
    lorsas: List[LowRankSparseAttention],
    replacement_model: ReplacementModel
):
    """è®¾ç½®ç¼“å­˜çš„æ¨¡åž‹ã€transcoderså’Œlorsas"""
    global _global_hooked_models, _global_transcoders_cache, _global_lorsas_cache, _global_replacement_models_cache
    
    _global_hooked_models[model_name] = hooked_model
    _global_transcoders_cache[model_name] = transcoders
    _global_lorsas_cache[model_name] = lorsas
    _global_replacement_models_cache[model_name] = replacement_model


def load_model_and_transcoders(
    model_name: str,
    device: str,
    tc_base_path: str,
    lorsa_base_path: str,
    n_layers: int = 15,
    hooked_model: Optional[HookedTransformer] = None,  # æ–°å¢žå‚æ•°
    loading_logs: Optional[list] = None  # æ–°å¢žå‚æ•°ï¼šç”¨äºŽæ”¶é›†åŠ è½½æ—¥å¿—
) -> Tuple[ReplacementModel, Dict[int, SparseAutoEncoder], List[LowRankSparseAttention]]:
    """åŠ è½½æ¨¡åž‹å’Œtranscodersï¼ˆå¸¦å…¨å±€ç¼“å­˜ï¼‰"""
    logger = logging.getLogger(__name__)
    
    # è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ æ—¥å¿—ï¼ˆåŒæ—¶æ‰“å°åˆ°æŽ§åˆ¶å°å’Œæ”¶é›†åˆ°æ—¥å¿—åˆ—è¡¨ï¼‰
    def add_log(message: str):
        print(message)
        logger.info(message)
        if loading_logs is not None:
            loading_logs.append({
                "timestamp": time.time(),
                "message": message
            })
    
    # å…ˆæ£€æŸ¥å…¨å±€ç¼“å­˜
    cached_hooked_model, cached_transcoders, cached_lorsas, cached_replacement_model = get_cached_models(model_name)
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å®Œæ•´ï¼ˆæœ‰transcoderså’Œlorsasï¼Œä¸”å±‚æ•°æ­£ç¡®ï¼‰
    if cached_transcoders is not None and cached_lorsas is not None:
        if len(cached_transcoders) == n_layers and len(cached_lorsas) == n_layers:
            if cached_replacement_model is not None:
                add_log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡åž‹ã€transcoderså’Œlorsas: {model_name}")
                logger.info(f"âœ… ä»Žç¼“å­˜åŠ è½½: {model_name} (transcoders={len(cached_transcoders)}å±‚, lorsas={len(cached_lorsas)}å±‚)")
                return cached_replacement_model, cached_transcoders, cached_lorsas
    
    # å¦‚æžœç¼“å­˜ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œåˆ™åŠ è½½
    add_log(f"ðŸ” å¼€å§‹åŠ è½½æ¨¡åž‹å’Œtranscoders: {model_name}")
    
    # ä½¿ç”¨ä¼ å…¥çš„æ¨¡åž‹æˆ–ä»Žç¼“å­˜èŽ·å–æˆ–åŠ è½½æ–°æ¨¡åž‹
    if hooked_model is not None:
        logger.info("ä½¿ç”¨ä¼ å…¥çš„HookedTransformeræ¨¡åž‹")
        model = hooked_model
    elif cached_hooked_model is not None:
        logger.info("ä½¿ç”¨ç¼“å­˜çš„HookedTransformeræ¨¡åž‹")
        model = cached_hooked_model
    else:
        logger.info("åŠ è½½æ–°çš„HookedTransformeræ¨¡åž‹")
        model = HookedTransformer.from_pretrained_no_processing(
            model_name,
            dtype=torch.float32,
        ).eval()
        # ç¼“å­˜æ¨¡åž‹
        _global_hooked_models[model_name] = model
    
    # åŠ è½½transcoders
    add_log(f"ðŸ” å¼€å§‹åŠ è½½Transcodersï¼Œå…±{n_layers}å±‚...")
    transcoders = {}
    for layer in range(n_layers):
        # æ ¹æ®æ¨¡åž‹åç§°é€‰æ‹©ä¸åŒçš„è·¯å¾„æ ¼å¼
        # if 'BT4' in model_name:
        #     # BT4è·¯å¾„æ ¼å¼: L{layer}
        #     tc_path = f"{tc_base_path}/L{layer}"
        # else:
        #     # é»˜è®¤T82è·¯å¾„æ ¼å¼
        #     tc_path = f"{tc_base_path}/lc0_L{layer}M_16x_k30_lr2e-03_auxk_sparseadam"
        tc_path = f"{tc_base_path}/L{layer}"
        add_log(f"  [TC Layer {layer}/{n_layers-1}] å¼€å§‹åŠ è½½: {tc_path}")
        logger.info(f"ðŸ“ åŠ è½½TC L{layer}: {tc_path}")
        start_time = time.time()
        transcoders[layer] = SparseAutoEncoder.from_pretrained(
            tc_path,
            dtype=torch.float32,
            device=device,
        )
        load_time = time.time() - start_time
        add_log(f"  [TC Layer {layer}/{n_layers-1}] âœ… åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
    
    add_log(f"âœ… æ‰€æœ‰TranscodersåŠ è½½å®Œæˆï¼Œå…±{len(transcoders)}å±‚")
    
    # åŠ è½½LORSA
    add_log(f"ðŸ” å¼€å§‹åŠ è½½LoRSAsï¼Œå…±{n_layers}å±‚...")
    lorsas = []
    for layer in range(n_layers):
        # æ ¹æ®æ¨¡åž‹åç§°é€‰æ‹©ä¸åŒçš„è·¯å¾„æ ¼å¼
        # if 'BT4' in model_name:
        #     # BT4è·¯å¾„æ ¼å¼: L{layer}
        #     lorsa_path = f"{lorsa_base_path}/lc0_L{layer}_bidirectional_lr0.0002_k_aux4096_coefficient0.125_dead_threshold1000000"
        # else:
        #     # é»˜è®¤T82è·¯å¾„æ ¼å¼
        #     lorsa_path = f"{lorsa_base_path}/lc0_L{layer}_bidirectional_lr8e-05_k_aux4096_coefficient0.0625_dead_threshold1000000"
        lorsa_path = f"{lorsa_base_path}/L{layer}"
        add_log(f"  [LoRSA Layer {layer}/{n_layers-1}] å¼€å§‹åŠ è½½: {lorsa_path}")
        logger.info(f"ðŸ“ åŠ è½½LORSA L{layer}: {lorsa_path}")
        start_time = time.time()
        lorsas.append(LowRankSparseAttention.from_pretrained(
            lorsa_path,
            device=device
        ))
        load_time = time.time() - start_time
        add_log(f"  [LoRSA Layer {layer}/{n_layers-1}] âœ… åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
    
    add_log(f"âœ… æ‰€æœ‰LoRSAsåŠ è½½å®Œæˆï¼Œå…±{len(lorsas)}å±‚")
    
    # åˆ›å»ºæ›¿æ¢æ¨¡åž‹
    replacement_model = ReplacementModel.from_pretrained_model(
        model, transcoders, lorsas
    )
    
    # ç¼“å­˜æ‰€æœ‰åŠ è½½çš„æ¨¡åž‹
    set_cached_models(model_name, model, transcoders, lorsas, replacement_model)
    add_log(f"âœ… æ¨¡åž‹ã€transcoderså’Œlorsaså·²ç¼“å­˜: {model_name}")
    
    return replacement_model, transcoders, lorsas


def setup_mongodb(mongo_uri: str, mongo_db: str) -> Optional[MongoClient]:
    """è®¾ç½®MongoDBè¿žæŽ¥"""
    logger = logging.getLogger(__name__)
    
    try:
        mongo_config = MongoDBConfig(
            mongo_uri=mongo_uri,
            mongo_db=mongo_db
        )
        mongo_client = MongoClient(mongo_config)
        logger.info(f"MongoDBè¿žæŽ¥æˆåŠŸ: {mongo_config.mongo_db}")
        return mongo_client
    except Exception as e:
        logger.warning(f"MongoDBè¿žæŽ¥å¤±è´¥: {e}")
        return None


def run_attribution(
    model: ReplacementModel,
    prompt: str,
    fen: str,
    move_uci: str,
    side: str,
    max_n_logits: int,
    desired_logit_prob: float,
    max_feature_nodes: int,
    batch_size: int,
    order_mode: str,
    mongo_client: Optional[MongoClient],
    sae_series: str,
    act_times_max: Optional[int] = None,
    encoder_demean: bool = False,
    save_activation_info: bool = False,
    negative_move_uci: Optional[str] = None  # æ–°å¢žnegative_move_uciå‚æ•°
) -> Dict[str, Any]:
    """è¿è¡Œattributionåˆ†æž"""
    logger = logging.getLogger(__name__)
    
    # è®¾ç½®æ£‹ç›˜
    lboard = LeelaBoard.from_fen(fen, history_synthesis=True)
    is_castle = False  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    
    # å¤„ç†move_idxï¼šæ ¹æ®order_modeå’Œnegative_move_uciå†³å®š
    if order_mode == 'move_pair':
        # move_pairæ¨¡å¼ï¼šéœ€è¦positiveå’Œnegative move
        if not negative_move_uci:
            raise ValueError("negative_move_uci is required for move_pair mode")
        positive_move_idx = lboard.uci2idx(move_uci)
        negative_move_idx = lboard.uci2idx(negative_move_uci)
        move_idx = (positive_move_idx, negative_move_idx)
        logger.info(f"Move pair mode: positive_move_idx={positive_move_idx}, negative_move_idx={negative_move_idx}")
    else:
        # positiveæˆ–negativeæ¨¡å¼ï¼šåªæœ‰ä¸€ä¸ªmove
        move_idx = lboard.uci2idx(move_uci)
    
    # è®¾ç½®æ¢¯åº¦
    torch.set_grad_enabled(True)
    model.reset_hooks()
    model.zero_grad(set_to_none=True)
    
    # è¿è¡Œattribution
    logger.info(f"å¼€å§‹attributionåˆ†æž: {prompt}")
    start_time = time.time()
    
    attribution_result = attribute(
        prompt=prompt,
        model=model,
        is_castle=is_castle,
        side=side,
        max_n_logits=max_n_logits,
        desired_logit_prob=desired_logit_prob,
        batch_size=batch_size,
        max_feature_nodes=max_feature_nodes,
        offload=None,
        update_interval=4,
        use_legal_moves_only=False,
        fen=fen,
        lboard=lboard,
        move_idx=move_idx,
        encoder_demean=encoder_demean,
        act_times_max=act_times_max,
        mongo_client=mongo_client,
        sae_series=sae_series,
        analysis_name='default',
        order_mode=order_mode,
        save_activation_info=save_activation_info,
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"Attributionåˆ†æžå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")
    
    return attribution_result


def create_graph_from_attribution(
    model,
    attribution_result: Dict[str, Any],
    prompt: str,
    side: str,
    slug: str,  # å°† slug ç§»åˆ°å‰é¢
    sae_series: Optional[str] = None,
) -> Graph:
    """
    ä»Žattributionç»“æžœåˆ›å»ºGraphå¯¹è±¡
    
    Args:
        model: æ›¿æ¢æ¨¡åž‹å®žä¾‹
        attribution_result: Attributionç»“æžœå­—å…¸
        prompt: è¾“å…¥æç¤º
        side: åˆ†æžä¾§ ('q', 'k', æˆ– 'both')
        slug: å›¾çš„æ ‡è¯†ç¬¦
        sae_series: SAEç³»åˆ—åç§°
    
    Returns:
        Graph: åˆ›å»ºçš„å›¾å¯¹è±¡
    """
    logger = logging.getLogger(__name__)
    logger.info(f"æ­£åœ¨ä¸ºä¾§'{side}'åˆ›å»ºå›¾å¯¹è±¡...")
    try:
        # æå–å…¬å…±æ•°æ®
        lorsa_activation_matrix = attribution_result['lorsa_activations']['lorsa_activation_matrix']
        tc_activation_matrix = attribution_result['tc_activations']['tc_activation_matrix']
        input_embedding = attribution_result['input']['input_embedding']
        logit_idx = attribution_result['logits']['indices']
        logit_p = attribution_result['logits']['probabilities']
        lorsa_active_features = attribution_result['lorsa_activations']['indices']
        lorsa_activation_values = attribution_result['lorsa_activations']['values']
        tc_active_features = attribution_result['tc_activations']['indices']
        tc_activation_values = attribution_result['tc_activations']['values']
        
        # æ ¹æ®sideé€‰æ‹©å¯¹åº”çš„æ•°æ®
        if side == 'q':
            q_data = attribution_result.get('q')
            if q_data is None:
                raise ValueError("Attributionç»“æžœä¸­æ²¡æœ‰æ‰¾åˆ°'q'ä¾§æ•°æ®")
            full_edge_matrix = q_data['full_edge_matrix']
            selected_features = q_data['selected_features']
            side_logit_position = q_data.get('move_positions')
            activation_info = attribution_result.get('activation_info', {}).get('q')
            
        elif side == 'k':
            k_data = attribution_result.get('k')
            if k_data is None:
                raise ValueError("Attributionç»“æžœä¸­æ²¡æœ‰æ‰¾åˆ°'k'ä¾§æ•°æ®")
            full_edge_matrix = k_data['full_edge_matrix']
            selected_features = k_data['selected_features']
            side_logit_position = k_data.get('move_positions')
            activation_info = attribution_result.get('activation_info', {}).get('k')
            
        elif side == 'both':
            # å¤„ç†bothæƒ…å†µï¼Œéœ€è¦åˆå¹¶qå’Œkä¾§çš„æ•°æ®
            q_data = attribution_result.get('q')
            k_data = attribution_result.get('k')
            if q_data is None or k_data is None:
                raise ValueError("Attributionç»“æžœä¸­æ²¡æœ‰æ‰¾åˆ°'q'æˆ–'k'ä¾§æ•°æ®ï¼Œæ— æ³•è¿›è¡Œbothæ¨¡å¼åˆå¹¶")
            
            # å¯¼å…¥merge_qk_graphå‡½æ•°
            from lm_saes.circuit.attribution_qk import merge_qk_graph
            
            logger.info("å¼€å§‹åˆå¹¶qå’Œkä¾§æ•°æ®...")
            merged = merge_qk_graph(attribution_result)
            
            full_edge_matrix = merged["adjacency_matrix"]
            selected_features = merged["selected_features"]
            side_logit_position = merged["logit_position"]
            
            # ä½¿ç”¨merge_qk_graphè¿”å›žçš„åˆå¹¶æ¿€æ´»ä¿¡æ¯
            activation_info = merged.get("activation_info")
            logger.info(f"åˆå¹¶å®Œæˆï¼ŒåŒ…å« {len(selected_features)} ä¸ªé€‰ä¸­ç‰¹å¾")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¾§: {side}")
        
        # åˆ›å»ºGraphå¯¹è±¡
        graph = Graph(
            input_string=prompt,
            input_tokens=input_embedding,
            logit_tokens=logit_idx,
            logit_probabilities=logit_p,
            logit_position=side_logit_position,
            lorsa_active_features=lorsa_active_features,
            lorsa_activation_values=lorsa_activation_values,
            tc_active_features=tc_active_features,
            tc_activation_values=tc_activation_values,
            selected_features=selected_features,
            adjacency_matrix=full_edge_matrix,
            cfg=model.cfg,
            sae_series=sae_series,
            slug=slug,
            activation_info=activation_info,
        )
        
        logger.info(f"æˆåŠŸåˆ›å»ºå›¾å¯¹è±¡ï¼ŒåŒ…å« {len(selected_features)} ä¸ªé€‰ä¸­ç‰¹å¾")
        return graph
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå›¾å¯¹è±¡æ—¶å‡ºé”™: {e}")
        raise


def create_graph_json_data(
    graph: Graph,
    slug: str,
    node_threshold: float = 0.8,
    edge_threshold: float = 0.98,
    sae_series: Optional[str] = None,
    lorsa_analysis_name: str = "",
    tc_analysis_name: str = "",
) -> Dict[str, Any]:
    """åˆ›å»ºgraphçš„JSONæ•°æ®ï¼Œä¸ä¿å­˜åˆ°æ–‡ä»¶"""
    logger = logging.getLogger(__name__)
    
    logger.info(f"å¼€å§‹åˆ›å»ºgraph JSONæ•°æ®: {slug}")
    start_time = time.time()
    
    if sae_series is None:
        if graph.sae_series is None:
            raise ValueError(
                "Neither sae_series nor graph.sae_series was set. One must be set to identify "
                "which transcoders were used when creating the graph."
            )
        sae_series = graph.sae_series

    device = "cuda" if torch.cuda.is_available() else "cpu"
    graph.to(device)
    
    fen = graph.input_string
    lboard = None
    if fen:
        print(f'in graph input_string {fen = }')
        lboard = LeelaBoard.from_fen(fen)
    else:
        print('[Warning] fen is none')
        
    to_uci = lboard.idx2uci if lboard is not None else None 
    
    if isinstance(graph.logit_tokens, torch.Tensor):
        _logit_idxs = graph.logit_tokens.view(-1).tolist()
    else:
        _logit_idxs = list(graph.logit_tokens)
    
    
    logit_moves = [
        (to_uci(int(i)) if to_uci is not None else f"idx:{int(i)}")
        for i in _logit_idxs
    ]
    target_move = logit_moves[0] if logit_moves else None
    
    print(f'{target_move = }') 
    print(f'{graph.adjacency_matrix.shape = }')
    
    node_mask, edge_mask, cumulative_scores = (
        el.to(device) for el in prune_graph(graph, node_threshold, edge_threshold)
    )

    nodes = create_nodes(graph, node_mask, cumulative_scores, to_uci = to_uci)
    used_nodes, used_edges = create_used_nodes_and_edges(graph, nodes, edge_mask)
    model = build_model(
        graph=graph,
        used_nodes=used_nodes,
        used_edges=used_edges,
        slug=slug,
        sae_series=sae_series,
        node_threshold=node_threshold,
        lorsa_analysis_name=lorsa_analysis_name,
        tc_analysis_name=tc_analysis_name,
        logit_moves = logit_moves,
        target_move = target_move,
    )

    elapsed_time = time.time() - start_time
    logger.info(f"Graph JSONæ•°æ®åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")
    
    return model.model_dump()


def run_circuit_trace(
    prompt: str,
    move_uci: str,
    negative_move_uci: Optional[str] = None,  # æ–°å¢žnegative_move_uciå‚æ•°
    model_name: str = "lc0/BT4-1024x15x32h",
    device: str = "cuda",
    tc_base_path: str = "/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/tc",
    lorsa_base_path: str = "/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/lorsa",
    n_layers: int = 15,
    side: str = "both",
    max_n_logits: int = 1,
    desired_logit_prob: float = 0.95,
    max_feature_nodes: int = 4096,
    batch_size: int = 1,
    order_mode: str = "positive",
    mongo_uri: str = "mongodb://10.244.94.234:27017",
    mongo_db: str = "mechinterp",
    sae_series: str = "BT4-exp128",
    act_times_max: Optional[int] = None,
    encoder_demean: bool = False,
    save_activation_info: bool = False,
    node_threshold: float = 0.73,
    edge_threshold: float = 0.57,
    log_level: str = "INFO",
    hooked_model: Optional[HookedTransformer] = None,  # æ–°å¢žå‚æ•°
    cached_transcoders: Optional[Dict[int, SparseAutoEncoder]] = None,  # æ–°å¢žï¼šç¼“å­˜çš„transcoders
    cached_lorsas: Optional[List[LowRankSparseAttention]] = None,  # æ–°å¢žï¼šç¼“å­˜çš„lorsas
    cached_replacement_model: Optional[ReplacementModel] = None  # æ–°å¢žï¼šç¼“å­˜çš„replacement_model
) -> Dict[str, Any]:
    """è¿è¡Œcircuit traceå¹¶è¿”å›žgraphæ•°æ®"""
    logger = setup_logging(log_level)
    
    # è®¾ç½®è®¾å¤‡
    if device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        device = "cpu"
    
    try:
        # åˆæ³•æ€§æ£€æµ‹ï¼šéªŒè¯move_uciåœ¨prompt fenä¸‹æ˜¯å¦åˆæ³•
        board = chess.Board(prompt)
        legal_uci_moves = [move.uci() for move in board.legal_moves]
        if move_uci not in legal_uci_moves:
            logger.error(f"âŒ ç§»åŠ¨ {move_uci} åœ¨fen {prompt} ä¸‹ä¸åˆæ³•ï¼")
            raise Exception(f"ä¸åˆæ³•çš„UCIç§»åŠ¨: {move_uci} ä¸åœ¨fen {prompt}çš„åˆæ³•èµ°æ³•ä¸­ã€‚\nåˆæ³•èµ°æ³•åˆ—è¡¨: {legal_uci_moves}")

        # åŠ è½½æ¨¡åž‹ï¼ˆå¦‚æžœå·²æœ‰ç¼“å­˜åˆ™ä½¿ç”¨ç¼“å­˜ï¼‰
        if cached_replacement_model is not None and cached_transcoders is not None and cached_lorsas is not None:
            print("âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡åž‹ã€transcoderså’Œlorsas...")
            logger.info("ä½¿ç”¨ç¼“å­˜çš„æ¨¡åž‹ã€transcoderså’Œlorsas...")
            model = cached_replacement_model
            transcoders = cached_transcoders
            lorsas = cached_lorsas
        else:
            print("åŠ è½½æ¨¡åž‹å’Œtranscoders...")
            print(f'{lorsa_base_path = }')
            print(f'{tc_base_path = }')
            
            logger.info("åŠ è½½æ¨¡åž‹å’Œtranscoders...")
            model, transcoders, lorsas = load_model_and_transcoders(
                model_name, device, tc_base_path, 
                lorsa_base_path, n_layers, hooked_model  # ä¼ é€’hooked_model
            )
        
        # è®¾ç½®MongoDB
        mongo_client = setup_mongodb(mongo_uri, mongo_db)
        print(f'DEBUG: mongo_client = {mongo_client}')
        # ç”Ÿæˆslug
        slug = f'circuit_trace_{order_mode}_{side}_{max_feature_nodes}'
        
        # è¿è¡Œattribution
        attribution_result = run_attribution(
            model=model,
            prompt=prompt,
            fen=prompt,
            move_uci=move_uci,
            side=side,
            max_n_logits=max_n_logits,
            desired_logit_prob=desired_logit_prob,
            max_feature_nodes=max_feature_nodes,
            batch_size=batch_size,
            order_mode=order_mode,
            mongo_client=mongo_client,
            sae_series=sae_series,
            act_times_max=act_times_max,
            encoder_demean=encoder_demean,
            save_activation_info=True,  # å¼ºåˆ¶è®¾ç½®ä¸ºTrueä»¥èŽ·å–æ¿€æ´»ä¿¡æ¯
            negative_move_uci=negative_move_uci  # ä¼ é€’negative_move_uci
        )
        
        # åˆ›å»ºGraph
        logger.info("åˆ›å»ºGraphå¯¹è±¡...")
        graph = create_graph_from_attribution(
            model=model,
            attribution_result=attribution_result,
            prompt=prompt,
            side=side,
            slug=slug,
            sae_series=sae_series
        )
        
        # åˆ›å»ºJSONæ•°æ®
        graph_data = create_graph_json_data(
            graph, slug, node_threshold, edge_threshold, 
            sae_series, "", ""
        )
        
        logger.info("Circuit traceåˆ†æžå®Œæˆ!")
        return graph_data
        
    except Exception as e:
        logger.error(f"æœ‰ç‚¹é—®é¢˜: {e}")
        # logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


def save_graph_files(
    graph: Graph,
    slug: str,
    output_path: str,
    node_threshold: float = 0.9,
    edge_threshold: float = 0.69
) -> None:
    """ä¿å­˜graphæ–‡ä»¶"""
    logger = logging.getLogger(__name__)
    
    logger.info(f"å¼€å§‹ä¿å­˜graphæ–‡ä»¶åˆ°: {output_path}")
    start_time = time.time()
    
    create_graph_files(
        graph=graph,
        slug=slug,
        output_path=output_path,
        node_threshold=node_threshold,
        edge_threshold=edge_threshold,
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"Graphæ–‡ä»¶ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Fast tracing test for chess SAE attribution")
    
    # æ¨¡åž‹å‚æ•°
    parser.add_argument("--model_name", type=str, default="lc0/BT4-1024x15x32h",
                       help="æ¨¡åž‹åç§°")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--n_layers", type=int, default=15,
                       help="æ¨¡åž‹å±‚æ•°")
    
    # è·¯å¾„å‚æ•°
    parser.add_argument("--tc_base_path", type=str, 
                       default="/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/tc",
                       help="TCæ¨¡åž‹åŸºç¡€è·¯å¾„")
    parser.add_argument("--lorsa_base_path", type=str,
                       default="/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/lorsa",
                       help="LORSAæ¨¡åž‹åŸºç¡€è·¯å¾„")
    parser.add_argument("--output_path", type=str,
                       default="/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/graphs/fast_tracing",
                       help="è¾“å‡ºè·¯å¾„")
    
    # åˆ†æžå‚æ•°
    parser.add_argument("--prompt", type=str, default="2k5/4Q3/3P4/8/6p1/4p3/q1pbK3/1R6 b - - 0 32",
                       help="FENå­—ç¬¦ä¸²")
    parser.add_argument("--move_uci", type=str, default="a2c4",
                       help="è¦åˆ†æžçš„UCIç§»åŠ¨")
    parser.add_argument("--side", type=str, default="k", choices=["q", "k", "both"],
                       help="åˆ†æžä¾§ (q/k/both)")
    parser.add_argument("--max_n_logits", type=int, default=1,
                       help="æœ€å¤§logitæ•°é‡")
    parser.add_argument("--desired_logit_prob", type=float, default=0.95,
                       help="æœŸæœ›logitæ¦‚çŽ‡")
    parser.add_argument("--max_feature_nodes", type=int, default=1024,
                       help="æœ€å¤§ç‰¹å¾èŠ‚ç‚¹æ•°")
    parser.add_argument("--batch_size", type=int, default=1,
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--order_mode", type=str, default="positive",
                       choices=["positive", "negative", "move_pair", "group"],
                       help="æŽ’åºæ¨¡å¼")
    
    # MongoDBå‚æ•°
    parser.add_argument("--mongo_uri", type=str, default="mongodb://10.244.94.234:27017",
                       help="MongoDB URI")
    parser.add_argument("--mongo_db", type=str, default="mechinterp",
                       help="MongoDBæ•°æ®åº“å")
    parser.add_argument("--sae_series", type=str, default="BT4",
                       help="SAEç³»åˆ—å")
    parser.add_argument("--act_times_max", type=lambda x: int(x) if x.lower() != "none" else None, default=None, help="æœ€å¤§æ¿€æ´»æ¬¡æ•° (å¯é€‰)")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--encoder_demean", action="store_true",
                       help="æ˜¯å¦å¯¹encoderè¿›è¡Œdemean")
    parser.add_argument("--save_activation_info", action="store_true",
                       help="æ˜¯å¦ä¿å­˜æ¿€æ´»ä¿¡æ¯")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--node_threshold", type=float, default=0.73,
                       help="èŠ‚ç‚¹é˜ˆå€¼")
    parser.add_argument("--edge_threshold", type=float, default=0.57,
                       help="è¾¹é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.log_level)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = "cpu"
    
    try:
        # åŠ è½½æ¨¡åž‹
        logger.info("åŠ è½½æ¨¡åž‹å’Œtranscoders...")
        model, transcoders, lorsas = load_model_and_transcoders(
            args.model_name, args.device, args.tc_base_path, 
            args.lorsa_base_path, args.n_layers
        )
        
        # è®¾ç½®MongoDB
        mongo_client = setup_mongodb(args.mongo_uri, args.mongo_db)
        
        # ç”Ÿæˆslug
        slug = f'fast_tracing_{args.side}_{args.max_feature_nodes}'
        
        # è¿è¡Œattribution
        attribution_result = run_attribution(
            model=model,
            prompt=args.prompt,
            fen=args.prompt,
            move_uci=args.move_uci,
            side=args.side,
            max_n_logits=args.max_n_logits,
            desired_logit_prob=args.desired_logit_prob,
            max_feature_nodes=args.max_feature_nodes,
            batch_size=args.batch_size,
            order_mode=args.order_mode,
            mongo_client=mongo_client,
            sae_series=args.sae_series,
            act_times_max=args.act_times_max,
            encoder_demean=args.encoder_demean,
            save_activation_info=args.save_activation_info
        )
        
        # åˆ›å»ºGraph
        logger.info("åˆ›å»ºGraphå¯¹è±¡...")
        graph = create_graph_from_attribution(
            model=model,
            attribution_result=attribution_result,
            prompt=args.prompt,
            side=args.side,
            slug=slug,
            sae_series=args.sae_series
        )
        
        # ä¿å­˜æ–‡ä»¶
        save_graph_files(
            graph, slug, args.output_path, 
            args.node_threshold, args.edge_threshold
        )
        
        logger.info("åˆ†æžå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


def check_dense_features(
    nodes: List[Dict[str, Any]],
    threshold: Optional[int],
    mongo_client: Optional[MongoClient],
    sae_series: str = "BT4-exp128",
    lorsa_analysis_name: Optional[str] = None,
    tc_analysis_name: Optional[str] = None
) -> List[str]:
    """
    æ£€æŸ¥å“ªäº›èŠ‚ç‚¹æ˜¯dense featureï¼ˆæ¿€æ´»æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼‰
    
    Args:
        nodes: èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«node_id, feature, layer, feature_typeç­‰ä¿¡æ¯
        threshold: æ¿€æ´»æ¬¡æ•°é˜ˆå€¼ï¼ŒNoneè¡¨ç¤ºæ— é™å¤§ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½ä¸æ˜¯denseï¼‰
        mongo_client: MongoDBå®¢æˆ·ç«¯
        sae_series: SAEç³»åˆ—åç§°
        lorsa_analysis_name: LoRSAåˆ†æžåç§°æ¨¡æ¿ï¼ˆå¦‚ "BT4_lorsa_L{}A"ï¼‰
        tc_analysis_name: TCåˆ†æžåç§°æ¨¡æ¿ï¼ˆå¦‚ "BT4_tc_L{}M"ï¼‰
    
    Returns:
        denseèŠ‚ç‚¹çš„node_idåˆ—è¡¨
    """
    logger = logging.getLogger(__name__)
    
    if threshold is None:
        # é˜ˆå€¼ä¸ºNoneï¼Œæ‰€æœ‰èŠ‚ç‚¹éƒ½ä¸æ˜¯dense
        return []
    
    if mongo_client is None:
        logger.warning("MongoDBå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•æ£€æŸ¥dense features")
        return []
    
    # æ‰“å°ä¼ å…¥çš„æ¨¡æ¿å‚æ•°
    logger.info(f"ðŸ” Denseæ£€æŸ¥å‚æ•°: lorsa_analysis_name={lorsa_analysis_name}, tc_analysis_name={tc_analysis_name}, threshold={threshold}")
    
    dense_node_ids = []
    not_dense_nodes = []  # è®°å½•éždenseèŠ‚ç‚¹ç”¨äºŽè°ƒè¯•
    
    for node in nodes:
        try:
            node_id = node.get('node_id')
            feature_idx = node.get('feature')
            layer = node.get('layer')
            feature_type = node.get('feature_type', '').lower()
            
            if node_id is None or feature_idx is None or layer is None:
                logger.debug(f"è·³è¿‡èŠ‚ç‚¹ {node_id}: ç¼ºå°‘å¿…è¦ä¿¡æ¯")
                continue
            
            # æž„å»ºSAEåç§°
            sae_name = None
            if 'lorsa' in feature_type:
                if lorsa_analysis_name:
                    # ä½¿ç”¨æä¾›çš„æ¨¡æ¿
                    sae_name = lorsa_analysis_name.replace("{}", str(layer))
                else:
                    # é»˜è®¤æ ¼å¼
                    sae_name = f"lc0-lorsa-L{layer}"
            elif 'transcoder' in feature_type or 'cross layer transcoder' in feature_type:
                if tc_analysis_name:
                    # ä½¿ç”¨æä¾›çš„æ¨¡æ¿
                    sae_name = tc_analysis_name.replace("{}", str(layer))
                else:
                    # é»˜è®¤æ ¼å¼
                    sae_name = f"lc0_L{layer}M_16x_k30_lr2e-03_auxk_sparseadam"
            else:
                logger.debug(f"è·³è¿‡èŠ‚ç‚¹ {node_id}: æœªçŸ¥ç‰¹å¾ç±»åž‹ {feature_type}")
                continue
            
            # è¯¦ç»†æ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„analysis_name
            logger.info(f"ðŸ“‹ èŠ‚ç‚¹ {node_id}: feature_type={feature_type}, layer={layer}, feature={feature_idx}, sae_name={sae_name}")
            
            # ä»ŽMongoDBèŽ·å–è¯¥ç‰¹å¾çš„æ¿€æ´»æ¬¡æ•°
            feature_data = mongo_client.get_feature(
                sae_name=sae_name,
                sae_series=sae_series,
                index=feature_idx
            )
            
            if feature_data is None:
                logger.warning(f"âŒ èŠ‚ç‚¹ {node_id}: åœ¨MongoDBä¸­æœªæ‰¾åˆ°ç‰¹å¾æ•°æ® (sae={sae_name}, sae_series={sae_series}, idx={feature_idx})")
                not_dense_nodes.append({
                    'node_id': node_id,
                    'reason': 'MongoDBä¸­æœªæ‰¾åˆ°',
                    'sae_name': sae_name,
                    'sae_series': sae_series,
                    'feature_idx': feature_idx
                })
                continue
            
            # èŽ·å–è¯¥ç‰¹å¾çš„æ¿€æ´»æ¬¡æ•°
            if feature_data.analyses:
                analysis = feature_data.analyses[0]
                act_times = getattr(analysis, 'act_times', 0)
                
                logger.info(f"ðŸ“Š èŠ‚ç‚¹ {node_id}: act_times={act_times}, threshold={threshold}, sae_name={sae_name}")
                
                if act_times > threshold:
                    dense_node_ids.append(node_id)
                    logger.info(f"âœ… DenseèŠ‚ç‚¹: {node_id} (act_times={act_times} > threshold={threshold})")
                else:
                    not_dense_nodes.append({
                        'node_id': node_id,
                        'reason': f'act_times={act_times} <= threshold={threshold}',
                        'sae_name': sae_name,
                        'act_times': act_times
                    })
                    logger.info(f"âšª éžDenseèŠ‚ç‚¹: {node_id} (act_times={act_times} <= threshold={threshold})")
            else:
                logger.warning(f"âŒ èŠ‚ç‚¹ {node_id}: æ²¡æœ‰åˆ†æžæ•°æ®")
                not_dense_nodes.append({
                    'node_id': node_id,
                    'reason': 'æ²¡æœ‰åˆ†æžæ•°æ®',
                    'sae_name': sae_name
                })
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥èŠ‚ç‚¹ {node.get('node_id')} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    logger.info(f"ðŸ“ˆ ç»Ÿè®¡: æ€»èŠ‚ç‚¹={len(nodes)}, DenseèŠ‚ç‚¹={len(dense_node_ids)}, éžDenseèŠ‚ç‚¹={len(not_dense_nodes)}")
    if not_dense_nodes:
        logger.info(f"ðŸ” éžDenseèŠ‚ç‚¹è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰:")
        for node_info in not_dense_nodes[:10]:
            logger.info(f"  - {node_info}")
    
    return dense_node_ids


if __name__ == "__main__":
    main()
