#!/usr/bin/env python3
"""
Fast tracing test script for chess SAE attribution.
This script can be run with torchrun for distributed execution.
"""

import argparse
import json
import logging
import sys
import time
import io
import contextlib
from pathlib import Path
from typing import Optional, Dict, Any, Tuple, List

import torch
import chess
from transformer_lens import HookedTransformer
from tqdm import tqdm

# å…¨å±€ BT4 å¸¸é‡ï¼ˆæ¨¡å‹ä¸ SAE è·¯å¾„ï¼‰
# å…¼å®¹ç›´æ¥è¿è¡Œ server ç›®å½•å’Œä½œä¸º package å¯¼å…¥ä¸¤ç§æ–¹å¼
try:
    from .constants import BT4_TC_BASE_PATH, BT4_LORSA_BASE_PATH
except ImportError:
    from constants import BT4_TC_BASE_PATH, BT4_LORSA_BASE_PATH

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from lm_saes import ReplacementModel, LowRankSparseAttention, SparseAutoEncoder
from lm_saes.circuit.attribution_qk_for_feature_attribution import attribute
from lm_saes.circuit.graph_lc0 import Graph
from lm_saes.circuit.utils.create_graph_files import create_graph_files, build_model, create_nodes, create_used_nodes_and_edges, prune_graph
from lm_saes.circuit.leela_board import LeelaBoard
from src.lm_saes.config import MongoDBConfig
from src.lm_saes.database import (
    MongoClient,
    SAERecord,
    DatasetRecord,
    ModelRecord,
)


def setup_logging(log_level: str = "INFO") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


class TeeWriter:
    """ä¸€ä¸ªåŒæ—¶å†™å…¥å¤šä¸ªç›®æ ‡çš„writerç±»"""
    def __init__(self, *targets):
        self.targets = targets
    
    def write(self, text):
        for target in self.targets:
            target.write(text)
    
    def flush(self):
        for target in self.targets:
            if hasattr(target, 'flush'):
                target.flush()


class LogCapture:
    """æ•è·printã€loggerå’Œtqdmè¾“å‡ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, log_list: list):
        """
        Args:
            log_list: ç”¨äºå­˜å‚¨æ—¥å¿—çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {"timestamp": float, "message": str}
        """
        self.log_list = log_list
        self.original_stdout = sys.stdout
        self.original_stderr = sys.stderr
        self.log_buffer = io.StringIO()
        self.log_handlers = []
        
    def _log_message(self, message: str):
        """å°†æ¶ˆæ¯æ·»åŠ åˆ°æ—¥å¿—åˆ—è¡¨"""
        if message.strip():  # åªæ·»åŠ éç©ºæ¶ˆæ¯
            self.log_list.append({
                "timestamp": time.time(),
                "message": message.strip()
            })
    
    def _write_and_log(self, text: str, original_stream):
        """å†™å…¥åŸå§‹æµå¹¶è®°å½•æ—¥å¿—"""
        original_stream.write(text)  # å…ˆå†™å…¥åŸå§‹æµ
        # æŒ‰è¡Œåˆ†å‰²å¹¶è®°å½•
        if text:
            for line in text.rstrip('\n').split('\n'):
                if line.strip():
                    self._log_message(line)
    
    def _setup_logger_handler(self):
        """è®¾ç½®loggerå¤„ç†å™¨"""
        class LogListHandler(logging.Handler):
            def __init__(self, log_list):
                super().__init__()
                self.log_list = log_list
                
            def emit(self, record):
                log_entry = self.format(record)
                self.log_list.append({
                    "timestamp": time.time(),
                    "message": log_entry
                })
        
        # ä¸ºattribution loggeræ·»åŠ handler
        attribution_logger = logging.getLogger("attribution")
        handler = LogListHandler(self.log_list)
        handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))
        attribution_logger.addHandler(handler)
        self.log_handlers.append((attribution_logger, handler))
        
        # ä¹Ÿä¸ºroot loggeræ·»åŠ handlerï¼ˆæ•è·æ‰€æœ‰æ—¥å¿—ï¼‰
        root_logger = logging.getLogger()
        root_handler = LogListHandler(self.log_list)
        root_handler.setFormatter(logging.Formatter('%(name)s - %(levelname)s - %(message)s'))
        root_logger.addHandler(root_handler)
        self.log_handlers.append((root_logger, root_handler))
    
    def _setup_tqdm_handler(self):
        """è®¾ç½®tqdmçš„å†™å…¥å‡½æ•°"""
        # ä¿å­˜åŸå§‹çš„tqdm.write
        self.original_tqdm_write = tqdm.write
        
        def custom_tqdm_write(s, file=None, end="\n", nolock=False):
            # è°ƒç”¨åŸå§‹å‡½æ•°
            self.original_tqdm_write(s, file=file, end=end, nolock=nolock)
            # è®°å½•æ—¥å¿—
            if s.strip():
                self._log_message(s.strip())
        
        tqdm.write = custom_tqdm_write
    
    def __enter__(self):
        # åˆ›å»ºä¸€ä¸ªåŒ…è£…çš„stdoutï¼ŒåŒæ—¶å†™å…¥åŸå§‹stdoutå’Œè®°å½•æ—¥å¿—
        class LoggingStdout:
            def __init__(self, original, log_capture):
                self.original = original
                self.log_capture = log_capture
            
            def write(self, text):
                self.log_capture._write_and_log(text, self.original)
            
            def flush(self):
                self.original.flush()
            
            def __getattr__(self, name):
                return getattr(self.original, name)
        
        class LoggingStderr:
            def __init__(self, original, log_capture):
                self.original = original
                self.log_capture = log_capture
            
            def write(self, text):
                self.log_capture._write_and_log(text, self.original)
            
            def flush(self):
                self.original.flush()
            
            def __getattr__(self, name):
                return getattr(self.original, name)
        
        # æ›¿æ¢stdoutå’Œstderr
        sys.stdout = LoggingStdout(self.original_stdout, self)
        sys.stderr = LoggingStderr(self.original_stderr, self)
        
        # è®¾ç½®logger handler
        self._setup_logger_handler()
        
        # è®¾ç½®tqdm handler
        self._setup_tqdm_handler()
        
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ¢å¤stdoutå’Œstderr
        sys.stdout = self.original_stdout
        sys.stderr = self.original_stderr
        
        # ç§»é™¤logger handlers
        for logger, handler in self.log_handlers:
            logger.removeHandler(handler)
        
        # æ¢å¤tqdm.write
        if hasattr(self, 'original_tqdm_write'):
            tqdm.write = self.original_tqdm_write
        
        return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸


# å…¨å±€ç¼“å­˜ï¼ˆä¸app.pyå…±äº«ï¼‰
_global_hooked_models: Dict[str, HookedTransformer] = {}
_global_transcoders_cache: Dict[str, Dict[int, SparseAutoEncoder]] = {}
_global_lorsas_cache: Dict[str, List[LowRankSparseAttention]] = {}
_global_replacement_models_cache: Dict[str, ReplacementModel] = {}

# åŠ è½½é”ï¼Œé˜²æ­¢å¹¶å‘åŠ è½½å¯¼è‡´é‡å¤åŠ è½½
import threading
_loading_lock = threading.Lock()
_is_loading: Dict[str, bool] = {}  # model_name -> is_loading


def get_cached_models(cache_key: str) -> Tuple[Optional[HookedTransformer], Optional[Dict[int, SparseAutoEncoder]], Optional[List[LowRankSparseAttention]], Optional[ReplacementModel]]:
    """
    è·å–ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsas
    
    Args:
        cache_key: ç¼“å­˜é”®ï¼Œæ ¼å¼ä¸º "model_name" æˆ– "model_name::combo_id"
    """
    global _global_hooked_models, _global_transcoders_cache, _global_lorsas_cache, _global_replacement_models_cache
    
    # HookedTransformer æ¨¡å‹ä¸ä¾èµ– combo_idï¼Œåªä½¿ç”¨ model_name
    model_name = cache_key.split("::")[0] if "::" in cache_key else cache_key
    hooked_model = _global_hooked_models.get(model_name)
    
    # transcoders, lorsas, replacement_model ä½¿ç”¨å®Œæ•´çš„ cache_keyï¼ˆåŒ…å« combo_idï¼‰
    transcoders = _global_transcoders_cache.get(cache_key)
    lorsas = _global_lorsas_cache.get(cache_key)
    replacement_model = _global_replacement_models_cache.get(cache_key)
    
    return hooked_model, transcoders, lorsas, replacement_model


def set_cached_models(
    cache_key: str,
    hooked_model: HookedTransformer,
    transcoders: Dict[int, SparseAutoEncoder],
    lorsas: List[LowRankSparseAttention],
    replacement_model: ReplacementModel
):
    """
    è®¾ç½®ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsas
    
    Args:
        cache_key: ç¼“å­˜é”®ï¼Œæ ¼å¼ä¸º "model_name" æˆ– "model_name::combo_id"
    """
    global _global_hooked_models, _global_transcoders_cache, _global_lorsas_cache, _global_replacement_models_cache
    
    # HookedTransformer æ¨¡å‹ä¸ä¾èµ– combo_idï¼Œåªä½¿ç”¨ model_name
    model_name = cache_key.split("::")[0] if "::" in cache_key else cache_key
    _global_hooked_models[model_name] = hooked_model
    
    # transcoders, lorsas, replacement_model ä½¿ç”¨å®Œæ•´çš„ cache_keyï¼ˆåŒ…å« combo_idï¼‰
    _global_transcoders_cache[cache_key] = transcoders
    _global_lorsas_cache[cache_key] = lorsas
    _global_replacement_models_cache[cache_key] = replacement_model


def load_model_and_transcoders(
    model_name: str,
    device: str,
    tc_base_path: str,
    lorsa_base_path: str,
    n_layers: int = 15,
    hooked_model: Optional[HookedTransformer] = None,  # æ–°å¢å‚æ•°
    loading_logs: Optional[list] = None,  # æ–°å¢å‚æ•°ï¼šç”¨äºæ”¶é›†åŠ è½½æ—¥å¿—
    cancel_flag: Optional[dict] = None,  # æ–°å¢å‚æ•°ï¼šç”¨äºæ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸­æ–­åŠ è½½ {"combo_key": should_cancel}
    cache_key: Optional[str] = None  # æ–°å¢å‚æ•°ï¼šç¼“å­˜é”®ï¼Œæ ¼å¼ä¸º "model_name::combo_id"ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ model_name
) -> Tuple[ReplacementModel, Dict[int, SparseAutoEncoder], List[LowRankSparseAttention]]:
    """
    åŠ è½½æ¨¡å‹å’Œtranscodersï¼ˆå¸¦å…¨å±€ç¼“å­˜å’ŒåŠ è½½é”ï¼Œé˜²æ­¢é‡å¤åŠ è½½ï¼‰
    
    Args:
        cache_key: ç¼“å­˜é”®ï¼Œæ ¼å¼ä¸º "model_name::combo_id"ã€‚å¦‚æœä¸æä¾›ï¼Œåˆ™ä½¿ç”¨ model_nameï¼ˆå‘åå…¼å®¹ï¼‰
    """
    global _global_hooked_models, _global_transcoders_cache, _global_lorsas_cache, _global_replacement_models_cache
    global _loading_lock, _is_loading
    
    logger = logging.getLogger(__name__)
    
    # ç¡®å®šç¼“å­˜é”®
    if cache_key is None:
        cache_key = model_name
    
    # è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ æ—¥å¿—ï¼ˆåŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œæ”¶é›†åˆ°æ—¥å¿—åˆ—è¡¨ï¼‰
    def add_log(message: str):
        print(message)
        logger.info(message)
        if loading_logs is not None:
            log_entry = {
                "timestamp": time.time(),
                "message": message
            }
            loading_logs.append(log_entry)
            # è°ƒè¯•ï¼šæ‰“å°æ—¥å¿—åˆ—è¡¨çš„é•¿åº¦
            if len(loading_logs) % 5 == 0:  # æ¯5æ¡æ—¥å¿—æ‰“å°ä¸€æ¬¡
                print(f"ğŸ“ å½“å‰æ—¥å¿—åˆ—è¡¨é•¿åº¦: {len(loading_logs)}")
    
    # å…ˆæ£€æŸ¥å…¨å±€ç¼“å­˜ï¼ˆæ— é”å¿«é€Ÿæ£€æŸ¥ï¼‰
    cached_hooked_model, cached_transcoders, cached_lorsas, cached_replacement_model = get_cached_models(cache_key)
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å®Œæ•´ï¼ˆæœ‰transcoderså’Œlorsasï¼Œä¸”å±‚æ•°æ­£ç¡®ï¼‰
    if cached_transcoders is not None and cached_lorsas is not None:
        if len(cached_transcoders) == n_layers and len(cached_lorsas) == n_layers:
            if cached_replacement_model is not None:
                add_log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsas: {model_name}")
                logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½: {model_name} (transcoders={len(cached_transcoders)}å±‚, lorsas={len(cached_lorsas)}å±‚)")
                return cached_replacement_model, cached_transcoders, cached_lorsas
    
    # è·å–åŠ è½½é”ï¼Œé˜²æ­¢å¹¶å‘åŠ è½½
    with _loading_lock:
        # å†æ¬¡æ£€æŸ¥ç¼“å­˜ï¼ˆåŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ï¼‰
        cached_hooked_model, cached_transcoders, cached_lorsas, cached_replacement_model = get_cached_models(cache_key)
        if cached_transcoders is not None and cached_lorsas is not None:
            if len(cached_transcoders) == n_layers and len(cached_lorsas) == n_layers:
                if cached_replacement_model is not None:
                    add_log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsasï¼ˆåŒé‡æ£€æŸ¥ï¼‰: {cache_key}")
                    return cached_replacement_model, cached_transcoders, cached_lorsas
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åŠ è½½ï¼ˆä½¿ç”¨ cache_key ä½œä¸ºåŠ è½½çŠ¶æ€é”®ï¼‰
        if _is_loading.get(cache_key, False):
            add_log(f"â³ æ¨¡å‹ {cache_key} æ­£åœ¨è¢«å…¶ä»–çº¿ç¨‹åŠ è½½ï¼Œç­‰å¾…...")
            # é‡Šæ”¾é”å¹¶ç­‰å¾…
    
    # å¦‚æœæ­£åœ¨åŠ è½½ï¼Œç­‰å¾…åŠ è½½å®Œæˆ
    wait_count = 0
    max_wait = 600  # æœ€å¤šç­‰å¾…600ç§’
    while _is_loading.get(cache_key, False) and wait_count < max_wait:
        time.sleep(1)
        wait_count += 1
        if wait_count % 10 == 0:
            add_log(f"â³ ç­‰å¾…æ¨¡å‹åŠ è½½ä¸­... ({wait_count}ç§’)")
    
    # å†æ¬¡æ£€æŸ¥ç¼“å­˜
    cached_hooked_model, cached_transcoders, cached_lorsas, cached_replacement_model = get_cached_models(cache_key)
    if cached_transcoders is not None and cached_lorsas is not None:
        if len(cached_transcoders) == n_layers and len(cached_lorsas) == n_layers:
            if cached_replacement_model is not None:
                add_log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsasï¼ˆç­‰å¾…åï¼‰: {cache_key}")
                return cached_replacement_model, cached_transcoders, cached_lorsas
    
    # è·å–åŠ è½½é”å¹¶æ ‡è®°ä¸ºæ­£åœ¨åŠ è½½
    with _loading_lock:
        # æœ€ç»ˆæ£€æŸ¥
        cached_hooked_model, cached_transcoders, cached_lorsas, cached_replacement_model = get_cached_models(cache_key)
        if cached_transcoders is not None and cached_lorsas is not None:
            if len(cached_transcoders) == n_layers and len(cached_lorsas) == n_layers:
                if cached_replacement_model is not None:
                    add_log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsasï¼ˆæœ€ç»ˆæ£€æŸ¥ï¼‰: {cache_key}")
                    return cached_replacement_model, cached_transcoders, cached_lorsas
        
        # æ ‡è®°ä¸ºæ­£åœ¨åŠ è½½ï¼ˆä½¿ç”¨ cache_keyï¼‰
        _is_loading[cache_key] = True
        add_log(f"ğŸ”’ è·å–åŠ è½½é”ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹: {cache_key}")
    
    try:
        # å¦‚æœç¼“å­˜ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œåˆ™åŠ è½½
        add_log(f"ğŸ” å¼€å§‹åŠ è½½æ¨¡å‹å’Œtranscoders: {model_name}")
        
        # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹æˆ–ä»ç¼“å­˜è·å–æˆ–åŠ è½½æ–°æ¨¡å‹
        if hooked_model is not None:
            add_log("ä½¿ç”¨ä¼ å…¥çš„HookedTransformeræ¨¡å‹")
            model = hooked_model
        elif cached_hooked_model is not None:
            add_log("ä½¿ç”¨ç¼“å­˜çš„HookedTransformeræ¨¡å‹")
            model = cached_hooked_model
        else:
            add_log("åŠ è½½æ–°çš„HookedTransformeræ¨¡å‹...")
            model = HookedTransformer.from_pretrained_no_processing(
                model_name,
                dtype=torch.float32,
            ).eval()
            # ç¼“å­˜æ¨¡å‹
            _global_hooked_models[model_name] = model
            add_log("âœ… HookedTransformeræ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–æˆ–è·å–å·²æœ‰çš„transcodersç¼“å­˜ï¼ˆä½¿ç”¨ cache_keyï¼‰
        if cache_key not in _global_transcoders_cache:
            _global_transcoders_cache[cache_key] = {}
        transcoders = _global_transcoders_cache[cache_key]
        
        # åŠ è½½transcodersï¼ˆé€å±‚æ£€æŸ¥ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        add_log(f"ğŸ” å¼€å§‹åŠ è½½Transcodersï¼Œå…±{n_layers}å±‚...")
        for layer in range(n_layers):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸­æ–­åŠ è½½
            if cancel_flag is not None:
                # å¦‚æœæœ‰æ£€æŸ¥å‡½æ•°ï¼Œè°ƒç”¨å®ƒï¼›å¦åˆ™ç›´æ¥æ£€æŸ¥ should_cancel
                if "check_fn" in cancel_flag and callable(cancel_flag["check_fn"]):
                    should_cancel = cancel_flag["check_fn"]()
                else:
                    should_cancel = cancel_flag.get("should_cancel", False)
                if should_cancel:
                    add_log(f"ğŸ›‘ åŠ è½½è¢«ä¸­æ–­ï¼ˆTC Layer {layer}/{n_layers-1}ï¼‰")
                    raise InterruptedError("åŠ è½½è¢«ç”¨æˆ·ä¸­æ–­")
            
            # æ£€æŸ¥è¯¥å±‚æ˜¯å¦å·²ç»åŠ è½½
            if layer in transcoders:
                add_log(f"  [TC Layer {layer}/{n_layers-1}] âœ… å·²ç¼“å­˜ï¼Œè·³è¿‡åŠ è½½")
                continue
            
            tc_path = f"{tc_base_path}/L{layer}"
            add_log(f"  [TC Layer {layer}/{n_layers-1}] å¼€å§‹åŠ è½½: {tc_path}")
            logger.info(f"ğŸ“ åŠ è½½TC L{layer}: {tc_path}")
            start_time = time.time()
            transcoders[layer] = SparseAutoEncoder.from_pretrained(
                tc_path,
                dtype=torch.float32,
                device=device,
            )
            load_time = time.time() - start_time
            add_log(f"  [TC Layer {layer}/{n_layers-1}] âœ… åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        
        add_log(f"âœ… æ‰€æœ‰TranscodersåŠ è½½å®Œæˆï¼Œå…±{len(transcoders)}å±‚")
        
        # åˆå§‹åŒ–æˆ–è·å–å·²æœ‰çš„lorsasç¼“å­˜ï¼ˆä½¿ç”¨ cache_keyï¼‰
        if cache_key not in _global_lorsas_cache:
            _global_lorsas_cache[cache_key] = []
        lorsas = _global_lorsas_cache[cache_key]
        
        # åŠ è½½LORSAï¼ˆé€å±‚æ£€æŸ¥ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        add_log(f"ğŸ” å¼€å§‹åŠ è½½LoRSAsï¼Œå…±{n_layers}å±‚...")
        for layer in range(n_layers):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸­æ–­åŠ è½½
            if cancel_flag is not None:
                # å¦‚æœæœ‰æ£€æŸ¥å‡½æ•°ï¼Œè°ƒç”¨å®ƒï¼›å¦åˆ™ç›´æ¥æ£€æŸ¥ should_cancel
                if "check_fn" in cancel_flag and callable(cancel_flag["check_fn"]):
                    should_cancel = cancel_flag["check_fn"]()
                else:
                    should_cancel = cancel_flag.get("should_cancel", False)
                if should_cancel:
                    add_log(f"ğŸ›‘ åŠ è½½è¢«ä¸­æ–­ï¼ˆLoRSA Layer {layer}/{n_layers-1}ï¼‰")
                    raise InterruptedError("åŠ è½½è¢«ç”¨æˆ·ä¸­æ–­")
            
            # æ£€æŸ¥è¯¥å±‚æ˜¯å¦å·²ç»åŠ è½½
            if layer < len(lorsas):
                add_log(f"  [LoRSA Layer {layer}/{n_layers-1}] âœ… å·²ç¼“å­˜ï¼Œè·³è¿‡åŠ è½½")
                continue
            
            lorsa_path = f"{lorsa_base_path}/L{layer}"
            add_log(f"  [LoRSA Layer {layer}/{n_layers-1}] å¼€å§‹åŠ è½½: {lorsa_path}")
            logger.info(f"ğŸ“ åŠ è½½LORSA L{layer}: {lorsa_path}")
            start_time = time.time()
            lorsas.append(LowRankSparseAttention.from_pretrained(
                lorsa_path,
                device=device
            ))
            load_time = time.time() - start_time
            add_log(f"  [LoRSA Layer {layer}/{n_layers-1}] âœ… åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        
        add_log(f"âœ… æ‰€æœ‰LoRSAsåŠ è½½å®Œæˆï¼Œå…±{len(lorsas)}å±‚")
        
        # åˆ›å»ºæ›¿æ¢æ¨¡å‹
        add_log("ğŸ” åˆ›å»ºReplacementModel...")
        replacement_model = ReplacementModel.from_pretrained_model(
            model, transcoders, lorsas
        )
        add_log("âœ… ReplacementModelåˆ›å»ºå®Œæˆ")
        
        # ç¼“å­˜æ‰€æœ‰åŠ è½½çš„æ¨¡å‹ï¼ˆä½¿ç”¨ cache_keyï¼‰
        set_cached_models(cache_key, model, transcoders, lorsas, replacement_model)
        add_log(f"âœ… æ¨¡å‹ã€transcoderså’Œlorsaså·²ç¼“å­˜: {cache_key}")
        
        return replacement_model, transcoders, lorsas
    except Exception as e:
        # ä»»ä½•å¼‚å¸¸ï¼ˆåŒ…æ‹¬ OOMï¼‰æ—¶ï¼Œæ¸…ç†å½“å‰ cache_key ä¸‹å·²åŠ è½½çš„ SAEï¼Œé¿å…å ç”¨æ˜¾å­˜
        add_log(f"âŒ åŠ è½½è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œå°†æ¸…ç©ºç¼“å­˜ {cache_key}: {e}")
        try:
            # å°†å·²åŠ è½½çš„ SAE æŒªåˆ° CPU å†åˆ é™¤å¼•ç”¨
            if cache_key in _global_transcoders_cache:
                for sae in _global_transcoders_cache[cache_key].values():
                    try:
                        if hasattr(sae, "to"):
                            sae.to("cpu")
                    except Exception:
                        continue
                del _global_transcoders_cache[cache_key]
            if cache_key in _global_lorsas_cache:
                for sae in _global_lorsas_cache[cache_key]:
                    try:
                        if hasattr(sae, "to"):
                            sae.to("cpu")
                    except Exception:
                        continue
                del _global_lorsas_cache[cache_key]
            if cache_key in _global_replacement_models_cache:
                del _global_replacement_models_cache[cache_key]
        finally:
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    add_log("ğŸ§¹ å·²åœ¨å¼‚å¸¸åè°ƒç”¨ torch.cuda.empty_cache() é‡Šæ”¾æ˜¾å­˜")
            except Exception:
                pass
        # å°†å¼‚å¸¸ç»§ç»­æŠ›å‡ºï¼Œè®©ä¸Šå±‚å¤„ç† HTTP é”™è¯¯ç ç­‰
        raise
    
    finally:
        # é‡Šæ”¾åŠ è½½é”ï¼ˆä½¿ç”¨ cache_keyï¼‰
        with _loading_lock:
            _is_loading[cache_key] = False
            add_log(f"ğŸ”“ é‡Šæ”¾åŠ è½½é”: {cache_key}")


def setup_mongodb(mongo_uri: str, mongo_db: str) -> Optional[MongoClient]:
    """è®¾ç½®MongoDBè¿æ¥"""
    logger = logging.getLogger(__name__)
    
    try:
        mongo_config = MongoDBConfig(
            mongo_uri=mongo_uri,
            mongo_db=mongo_db
        )
        mongo_client = MongoClient(mongo_config)
        logger.info(f"MongoDBè¿æ¥æˆåŠŸ: {mongo_config.mongo_db}")
        return mongo_client
    except Exception as e:
        logger.warning(f"MongoDBè¿æ¥å¤±è´¥: {e}")
        return None


def run_attribution(
    model: ReplacementModel,
    prompt: str,
    fen: str,
    move_uci: str,
    side: str,
    max_n_logits: int,
    desired_logit_prob: float,
    max_feature_nodes: int,
    batch_size: int,
    order_mode: str,
    mongo_client: Optional[MongoClient],
    sae_series: str,
    act_times_max: Optional[int] = None,
    encoder_demean: bool = False,
    save_activation_info: bool = False,
    negative_move_uci: Optional[str] = None  # æ–°å¢negative_move_uciå‚æ•°
) -> Dict[str, Any]:
    """è¿è¡Œattributionåˆ†æ"""
    logger = logging.getLogger(__name__)
    
    # è®¾ç½®æ£‹ç›˜
    lboard = LeelaBoard.from_fen(fen, history_synthesis=True)
    is_castle = False  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    
    # å¤„ç†move_idxï¼šæ ¹æ®order_modeå’Œnegative_move_uciå†³å®š
    if order_mode == 'move_pair':
        # move_pairæ¨¡å¼ï¼šéœ€è¦positiveå’Œnegative move
        if not negative_move_uci:
            raise ValueError("negative_move_uci is required for move_pair mode")
        positive_move_idx = lboard.uci2idx(move_uci)
        negative_move_idx = lboard.uci2idx(negative_move_uci)
        move_idx = (positive_move_idx, negative_move_idx)
        logger.info(f"Move pair mode: positive_move_idx={positive_move_idx}, negative_move_idx={negative_move_idx}")
    else:
        # positiveæˆ–negativeæ¨¡å¼ï¼šåªæœ‰ä¸€ä¸ªmove
        move_idx = lboard.uci2idx(move_uci)
    
    # è®¾ç½®æ¢¯åº¦
    torch.set_grad_enabled(True)
    model.reset_hooks()
    model.zero_grad(set_to_none=True)
    
    # è¿è¡Œattribution
    logger.info(f"å¼€å§‹attributionåˆ†æ: {prompt}")
    start_time = time.time()
    
    attribution_result = attribute(
        prompt=prompt,
        model=model,
        is_castle=is_castle,
        side=side,
        max_n_logits=max_n_logits,
        desired_logit_prob=desired_logit_prob,
        batch_size=batch_size,
        max_feature_nodes=max_feature_nodes,
        offload=None,
        update_interval=4,
        use_legal_moves_only=False,
        fen=fen,
        lboard=lboard,
        move_idx=move_idx,
        encoder_demean=encoder_demean,
        act_times_max=act_times_max,
        mongo_client=mongo_client,
        sae_series=sae_series,
        analysis_name='default',
        order_mode=order_mode,
        save_activation_info=save_activation_info,
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"Attributionåˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")
    
    return attribution_result


def create_graph_from_attribution(
    model,
    attribution_result: Dict[str, Any],
    prompt: str,
    side: str,
    slug: str,  # å°† slug ç§»åˆ°å‰é¢
    sae_series: Optional[str] = None,
) -> Graph:
    """
    ä»attributionç»“æœåˆ›å»ºGraphå¯¹è±¡
    
    Args:
        model: æ›¿æ¢æ¨¡å‹å®ä¾‹
        attribution_result: Attributionç»“æœå­—å…¸
        prompt: è¾“å…¥æç¤º
        side: åˆ†æä¾§ ('q', 'k', æˆ– 'both')
        slug: å›¾çš„æ ‡è¯†ç¬¦
        sae_series: SAEç³»åˆ—åç§°
    
    Returns:
        Graph: åˆ›å»ºçš„å›¾å¯¹è±¡
    """
    logger = logging.getLogger(__name__)
    logger.info(f"æ­£åœ¨ä¸ºä¾§'{side}'åˆ›å»ºå›¾å¯¹è±¡...")
    try:
        # æå–å…¬å…±æ•°æ®
        lorsa_activation_matrix = attribution_result['lorsa_activations']['lorsa_activation_matrix']
        tc_activation_matrix = attribution_result['tc_activations']['tc_activation_matrix']
        input_embedding = attribution_result['input']['input_embedding']
        logit_idx = attribution_result['logits']['indices']
        logit_p = attribution_result['logits']['probabilities']
        lorsa_active_features = attribution_result['lorsa_activations']['indices']
        lorsa_activation_values = attribution_result['lorsa_activations']['values']
        tc_active_features = attribution_result['tc_activations']['indices']
        tc_activation_values = attribution_result['tc_activations']['values']
        
        # æ ¹æ®sideé€‰æ‹©å¯¹åº”çš„æ•°æ®
        if side == 'q':
            q_data = attribution_result.get('q')
            if q_data is None:
                raise ValueError("Attributionç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°'q'ä¾§æ•°æ®")
            full_edge_matrix = q_data['full_edge_matrix']
            selected_features = q_data['selected_features']
            side_logit_position = q_data.get('move_positions')
            activation_info = attribution_result.get('activation_info', {}).get('q')
            
        elif side == 'k':
            k_data = attribution_result.get('k')
            if k_data is None:
                raise ValueError("Attributionç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°'k'ä¾§æ•°æ®")
            full_edge_matrix = k_data['full_edge_matrix']
            selected_features = k_data['selected_features']
            side_logit_position = k_data.get('move_positions')
            activation_info = attribution_result.get('activation_info', {}).get('k')
            
        elif side == 'both':
            # å¤„ç†bothæƒ…å†µï¼Œéœ€è¦åˆå¹¶qå’Œkä¾§çš„æ•°æ®
            q_data = attribution_result.get('q')
            k_data = attribution_result.get('k')
            if q_data is None or k_data is None:
                raise ValueError("Attributionç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°'q'æˆ–'k'ä¾§æ•°æ®ï¼Œæ— æ³•è¿›è¡Œbothæ¨¡å¼åˆå¹¶")
            
            # å¯¼å…¥merge_qk_graphå‡½æ•°
            from lm_saes.circuit.attribution_qk_for_feature_attribution import merge_qk_graph
            
            logger.info("å¼€å§‹åˆå¹¶qå’Œkä¾§æ•°æ®...")
            merged = merge_qk_graph(attribution_result)
            
            full_edge_matrix = merged["adjacency_matrix"]
            selected_features = merged["selected_features"]
            side_logit_position = merged["logit_position"]
            
            # ä½¿ç”¨merge_qk_graphè¿”å›çš„åˆå¹¶æ¿€æ´»ä¿¡æ¯
            activation_info = merged.get("activation_info")
            logger.info(f"åˆå¹¶å®Œæˆï¼ŒåŒ…å« {len(selected_features)} ä¸ªé€‰ä¸­ç‰¹å¾")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¾§: {side}")
        
        # åˆ›å»ºGraphå¯¹è±¡
        graph = Graph(
            input_string=prompt,
            input_tokens=input_embedding,
            logit_tokens=logit_idx,
            logit_probabilities=logit_p,
            logit_position=side_logit_position,
            lorsa_active_features=lorsa_active_features,
            lorsa_activation_values=lorsa_activation_values,
            tc_active_features=tc_active_features,
            tc_activation_values=tc_activation_values,
            selected_features=selected_features,
            adjacency_matrix=full_edge_matrix,
            cfg=model.cfg,
            sae_series=sae_series,
            slug=slug,
            activation_info=activation_info,
        )
        
        logger.info(f"æˆåŠŸåˆ›å»ºå›¾å¯¹è±¡ï¼ŒåŒ…å« {len(selected_features)} ä¸ªé€‰ä¸­ç‰¹å¾")
        return graph
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå›¾å¯¹è±¡æ—¶å‡ºé”™: {e}")
        raise


def create_graph_json_data(
    graph: Graph,
    slug: str,
    node_threshold: float = 0.8,
    edge_threshold: float = 0.98,
    sae_series: Optional[str] = None,
    lorsa_analysis_name: str = "",
    tc_analysis_name: str = "",
) -> Dict[str, Any]:
    """åˆ›å»ºgraphçš„JSONæ•°æ®ï¼Œä¸ä¿å­˜åˆ°æ–‡ä»¶"""
    logger = logging.getLogger(__name__)
    
    logger.info(f"å¼€å§‹åˆ›å»ºgraph JSONæ•°æ®: {slug}")
    start_time = time.time()
    
    if sae_series is None:
        if graph.sae_series is None:
            raise ValueError(
                "Neither sae_series nor graph.sae_series was set. One must be set to identify "
                "which transcoders were used when creating the graph."
            )
        sae_series = graph.sae_series

    device = "cuda" if torch.cuda.is_available() else "cpu"
    graph.to(device)
    
    fen = graph.input_string
    lboard = None
    if fen:
        print(f'in graph input_string {fen = }')
        lboard = LeelaBoard.from_fen(fen)
    else:
        print('[Warning] fen is none')
        
    to_uci = lboard.idx2uci if lboard is not None else None 
    
    if isinstance(graph.logit_tokens, torch.Tensor):
        _logit_idxs = graph.logit_tokens.view(-1).tolist()
    else:
        _logit_idxs = list(graph.logit_tokens)
    
    
    logit_moves = [
        (to_uci(int(i)) if to_uci is not None else f"idx:{int(i)}")
        for i in _logit_idxs
    ]
    target_move = logit_moves[0] if logit_moves else None
    
    print(f'{target_move = }') 
    print(f'{graph.adjacency_matrix.shape = }')
    
    node_mask, edge_mask, cumulative_scores = (
        el.to(device) for el in prune_graph(graph, node_threshold, edge_threshold)
    )

    nodes = create_nodes(graph, node_mask, cumulative_scores, to_uci = to_uci)
    used_nodes, used_edges = create_used_nodes_and_edges(graph, nodes, edge_mask)
    model = build_model(
        graph=graph,
        used_nodes=used_nodes,
        used_edges=used_edges,
        slug=slug,
        sae_series=sae_series,
        node_threshold=node_threshold,
        lorsa_analysis_name=lorsa_analysis_name,
        tc_analysis_name=tc_analysis_name,
        logit_moves = logit_moves,
        target_move = target_move,
    )

    elapsed_time = time.time() - start_time
    logger.info(f"Graph JSONæ•°æ®åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")
    
    return model.model_dump()


def run_circuit_trace(
    prompt: str,
    move_uci: str,
    negative_move_uci: Optional[str] = None,  # æ–°å¢negative_move_uciå‚æ•°
    model_name: str = "lc0/BT4-1024x15x32h",
    device: str = "cuda",
    tc_base_path: str = BT4_TC_BASE_PATH,
    lorsa_base_path: str = BT4_LORSA_BASE_PATH,
    n_layers: int = 15,
    side: str = "both",
    max_n_logits: int = 1,
    desired_logit_prob: float = 0.95,
    max_feature_nodes: int = 4096,
    batch_size: int = 1,
    order_mode: str = "positive",
    mongo_uri: str = "mongodb://10.245.40.143:27017",
    mongo_db: str = "mechinterp",
    sae_series: str = "BT4-exp128",
    act_times_max: Optional[int] = None,
    encoder_demean: bool = False,
    save_activation_info: bool = False,
    node_threshold: float = 0.73,
    edge_threshold: float = 0.57,
    log_level: str = "INFO",
    hooked_model: Optional[HookedTransformer] = None,  # æ–°å¢å‚æ•°
    cached_transcoders: Optional[Dict[int, SparseAutoEncoder]] = None,  # æ–°å¢ï¼šç¼“å­˜çš„transcoders
    cached_lorsas: Optional[List[LowRankSparseAttention]] = None,  # æ–°å¢ï¼šç¼“å­˜çš„lorsas
    cached_replacement_model: Optional[ReplacementModel] = None,  # æ–°å¢ï¼šç¼“å­˜çš„replacement_model
    sae_combo_id: Optional[str] = None,  # æ–°å¢ï¼šSAEç»„åˆIDï¼Œç”¨äºç”Ÿæˆæ­£ç¡®çš„analysis_nameæ¨¡æ¿
    trace_logs: Optional[list] = None  # æ–°å¢ï¼šç”¨äºå­˜å‚¨æ—¥å¿—çš„åˆ—è¡¨
) -> Dict[str, Any]:
    """è¿è¡Œcircuit traceå¹¶è¿”å›graphæ•°æ®"""
    logger = setup_logging(log_level)
    
    # å¦‚æœæä¾›äº†trace_logsï¼Œä½¿ç”¨æ—¥å¿—æ•è·
    if trace_logs is not None:
        log_capture = LogCapture(trace_logs)
        log_capture.__enter__()
    else:
        log_capture = None
    
    # è®¾ç½®è®¾å¤‡
    if device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        device = "cpu"
    
    try:
        # åˆæ³•æ€§æ£€æµ‹ï¼šéªŒè¯move_uciåœ¨prompt fenä¸‹æ˜¯å¦åˆæ³•
        board = chess.Board(prompt)
        legal_uci_moves = [move.uci() for move in board.legal_moves]
        if move_uci not in legal_uci_moves:
            logger.error(f"âŒ ç§»åŠ¨ {move_uci} åœ¨fen {prompt} ä¸‹ä¸åˆæ³•ï¼")
            raise Exception(f"ä¸åˆæ³•çš„UCIç§»åŠ¨: {move_uci} ä¸åœ¨fen {prompt}çš„åˆæ³•èµ°æ³•ä¸­ã€‚\nåˆæ³•èµ°æ³•åˆ—è¡¨: {legal_uci_moves}")

        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå·²æœ‰ç¼“å­˜åˆ™ä½¿ç”¨ç¼“å­˜ï¼‰
        if cached_replacement_model is not None and cached_transcoders is not None and cached_lorsas is not None:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ã€transcoderså’Œlorsas...")
            model = cached_replacement_model
            transcoders = cached_transcoders
            lorsas = cached_lorsas
        else:
            print("åŠ è½½æ¨¡å‹å’Œtranscoders...")
            print(f'{lorsa_base_path = }')
            print(f'{tc_base_path = }')
            
            logger.info("åŠ è½½æ¨¡å‹å’Œtranscoders...")
            model, transcoders, lorsas = load_model_and_transcoders(
                model_name, device, tc_base_path, 
                lorsa_base_path, n_layers, hooked_model  # ä¼ é€’hooked_model
            )
        
        # è®¾ç½®MongoDB
        mongo_client = setup_mongodb(mongo_uri, mongo_db)
        print(f'DEBUG: mongo_client = {mongo_client}')
        # ç”Ÿæˆslug
        slug = f'circuit_trace_{order_mode}_{side}_{max_feature_nodes}'
        
        # è¿è¡Œattribution
        attribution_result = run_attribution(
            model=model,
            prompt=prompt,
            fen=prompt,
            move_uci=move_uci,
            side=side,
            max_n_logits=max_n_logits,
            desired_logit_prob=desired_logit_prob,
            max_feature_nodes=max_feature_nodes,
            batch_size=batch_size,
            order_mode=order_mode,
            mongo_client=mongo_client,
            sae_series=sae_series,
            act_times_max=act_times_max,
            encoder_demean=encoder_demean,
            save_activation_info=True,  # å¼ºåˆ¶è®¾ç½®ä¸ºTrueä»¥è·å–æ¿€æ´»ä¿¡æ¯
            negative_move_uci=negative_move_uci  # ä¼ é€’negative_move_uci
        )
        
        # åˆ›å»ºGraph
        logger.info("åˆ›å»ºGraphå¯¹è±¡...")
        graph = create_graph_from_attribution(
            model=model,
            attribution_result=attribution_result,
            prompt=prompt,
            side=side,
            slug=slug,
            sae_series=sae_series
        )
        
        # æ ¹æ®sae_combo_idè·å–å¯¹åº”çš„analysis_nameï¼ˆä»BT4_SAE_COMBOSé…ç½®ä¸­è¯»å–ï¼‰
        # å¦‚æœsae_combo_idä¸ºNoneï¼Œå°è¯•ä»tc_base_pathå’Œlorsa_base_pathæ¨æ–­
        try:
            try:
                from .constants import get_bt4_sae_combo, BT4_SAE_COMBOS
            except ImportError:
                from constants import get_bt4_sae_combo, BT4_SAE_COMBOS
            
            # å¦‚æœsae_combo_idä¸ºNoneï¼Œå°è¯•ä»è·¯å¾„æ¨æ–­
            if sae_combo_id is None:
                # ä»è·¯å¾„ä¸­æå–ç»„åˆIDï¼ˆä¾‹å¦‚ï¼š/path/to/tc/k_30_e_16 -> k_30_e_16ï¼‰
                import os
                tc_path_parts = os.path.normpath(tc_base_path).split(os.sep)
                lorsa_path_parts = os.path.normpath(lorsa_base_path).split(os.sep)
                
                # æŸ¥æ‰¾è·¯å¾„ä¸­çš„ç»„åˆIDï¼ˆé€šå¸¸åœ¨è·¯å¾„çš„æœ€åå‡ éƒ¨åˆ†ï¼‰
                inferred_combo_id = None
                for combo_id in BT4_SAE_COMBOS.keys():
                    if combo_id in tc_path_parts or combo_id in lorsa_path_parts:
                        inferred_combo_id = combo_id
                        break
                
                if inferred_combo_id:
                    sae_combo_id = inferred_combo_id
                    logger.info(f"ä»è·¯å¾„æ¨æ–­SAEç»„åˆID: {sae_combo_id}")
                else:
                    logger.warning(f"æ— æ³•ä»è·¯å¾„æ¨æ–­SAEç»„åˆIDï¼Œä½¿ç”¨é»˜è®¤ç»„åˆ")
            
            combo_cfg = get_bt4_sae_combo(sae_combo_id)
            # ç›´æ¥ä»é…ç½®ä¸­è¯»å–analysis_nameå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™å›é€€åˆ°æ¨¡æ¿å­—æ®µ
            lorsa_analysis_name = combo_cfg.get("lorsa_analysis_name", combo_cfg.get("lorsa_sae_name_template", ""))
            tc_analysis_name = combo_cfg.get("tc_analysis_name", combo_cfg.get("tc_sae_name_template", ""))
            logger.info(f"ä½¿ç”¨SAEç»„åˆ {combo_cfg['id']} çš„analysis_name: LoRSA={lorsa_analysis_name}, TC={tc_analysis_name}")
        except Exception as e:
            logger.warning(f"æ— æ³•è·å–SAEç»„åˆé…ç½®ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²: {e}")
            import traceback
            traceback.print_exc()
            lorsa_analysis_name = ""
            tc_analysis_name = ""
        
        # åˆ›å»ºJSONæ•°æ®ï¼Œä¼ é€’analysis_nameï¼ˆä»BT4_SAE_COMBOSé…ç½®ä¸­è¯»å–ï¼‰
        graph_data = create_graph_json_data(
            graph, slug, node_threshold, edge_threshold, 
            sae_series, lorsa_analysis_name, tc_analysis_name
        )
        
        logger.info("Circuit traceåˆ†æå®Œæˆ!")
        
        # é€€å‡ºæ—¥å¿—æ•è·
        if log_capture is not None:
            log_capture.__exit__(None, None, None)
        
        return graph_data
        
    except Exception as e:
        logger.error(f"æœ‰ç‚¹é—®é¢˜: {e}")
        # logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
        # ç¡®ä¿é€€å‡ºæ—¥å¿—æ•è·
        if log_capture is not None:
            log_capture.__exit__(None, None, None)
        
        raise


def save_graph_files(
    graph: Graph,
    slug: str,
    output_path: str,
    node_threshold: float = 0.9,
    edge_threshold: float = 0.69
) -> None:
    """ä¿å­˜graphæ–‡ä»¶"""
    logger = logging.getLogger(__name__)
    
    logger.info(f"å¼€å§‹ä¿å­˜graphæ–‡ä»¶åˆ°: {output_path}")
    start_time = time.time()
    
    create_graph_files(
        graph=graph,
        slug=slug,
        output_path=output_path,
        node_threshold=node_threshold,
        edge_threshold=edge_threshold,
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"Graphæ–‡ä»¶ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Fast tracing test for chess SAE attribution")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_name", type=str, default="lc0/BT4-1024x15x32h",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--n_layers", type=int, default=15,
                       help="æ¨¡å‹å±‚æ•°")
    
    # è·¯å¾„å‚æ•°
    parser.add_argument("--tc_base_path", type=str, 
                       default="/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/tc/k_128_e_128",
                       help="TCæ¨¡å‹åŸºç¡€è·¯å¾„")
    parser.add_argument("--lorsa_base_path", type=str,
                       default="/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/result_BT4/lorsa/k_128_e_128",
                       help="LORSAæ¨¡å‹åŸºç¡€è·¯å¾„")
    parser.add_argument("--output_path", type=str,
                       default="/inspire/hdd/global_user/hezhengfu-240208120186/rlin_projects/rlin_projects/chess-SAEs-N/graphs/fast_tracing",
                       help="è¾“å‡ºè·¯å¾„")
    
    # åˆ†æå‚æ•°
    parser.add_argument("--prompt", type=str, default="2k5/4Q3/3P4/8/6p1/4p3/q1pbK3/1R6 b - - 0 32",
                       help="FENå­—ç¬¦ä¸²")
    parser.add_argument("--move_uci", type=str, default="a2c4",
                       help="è¦åˆ†æçš„UCIç§»åŠ¨")
    parser.add_argument("--side", type=str, default="k", choices=["q", "k", "both"],
                       help="åˆ†æä¾§ (q/k/both)")
    parser.add_argument("--max_n_logits", type=int, default=1,
                       help="æœ€å¤§logitæ•°é‡")
    parser.add_argument("--desired_logit_prob", type=float, default=0.95,
                       help="æœŸæœ›logitæ¦‚ç‡")
    parser.add_argument("--max_feature_nodes", type=int, default=1024,
                       help="æœ€å¤§ç‰¹å¾èŠ‚ç‚¹æ•°")
    parser.add_argument("--batch_size", type=int, default=1,
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--order_mode", type=str, default="positive",
                       choices=["positive", "negative", "move_pair", "group"],
                       help="æ’åºæ¨¡å¼")
    
    # MongoDBå‚æ•°
    parser.add_argument("--mongo_uri", type=str, default="mongodb://10.245.40.143:27017",
                       help="MongoDB URI")
    parser.add_argument("--mongo_db", type=str, default="mechinterp",
                       help="MongoDBæ•°æ®åº“å")
    parser.add_argument("--sae_series", type=str, default="BT4",
                       help="SAEç³»åˆ—å")
    parser.add_argument("--act_times_max", type=lambda x: int(x) if x.lower() != "none" else None, default=None, help="æœ€å¤§æ¿€æ´»æ¬¡æ•° (å¯é€‰)")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--encoder_demean", action="store_true",
                       help="æ˜¯å¦å¯¹encoderè¿›è¡Œdemean")
    parser.add_argument("--save_activation_info", action="store_true",
                       help="æ˜¯å¦ä¿å­˜æ¿€æ´»ä¿¡æ¯")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--node_threshold", type=float, default=0.73,
                       help="èŠ‚ç‚¹é˜ˆå€¼")
    parser.add_argument("--edge_threshold", type=float, default=0.57,
                       help="è¾¹é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.log_level)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = "cpu"
    
    try:
        # åŠ è½½æ¨¡å‹
        logger.info("åŠ è½½æ¨¡å‹å’Œtranscoders...")
        model, transcoders, lorsas = load_model_and_transcoders(
            args.model_name, args.device, args.tc_base_path, 
            args.lorsa_base_path, args.n_layers
        )
        
        # è®¾ç½®MongoDB
        mongo_client = setup_mongodb(args.mongo_uri, args.mongo_db)
        
        # ç”Ÿæˆslug
        slug = f'fast_tracing_{args.side}_{args.max_feature_nodes}'
        
        # è¿è¡Œattribution
        attribution_result = run_attribution(
            model=model,
            prompt=args.prompt,
            fen=args.prompt,
            move_uci=args.move_uci,
            side=args.side,
            max_n_logits=args.max_n_logits,
            desired_logit_prob=args.desired_logit_prob,
            max_feature_nodes=args.max_feature_nodes,
            batch_size=args.batch_size,
            order_mode=args.order_mode,
            mongo_client=mongo_client,
            sae_series=args.sae_series,
            act_times_max=args.act_times_max,
            encoder_demean=args.encoder_demean,
            save_activation_info=args.save_activation_info
        )
        
        # åˆ›å»ºGraph
        logger.info("åˆ›å»ºGraphå¯¹è±¡...")
        graph = create_graph_from_attribution(
            model=model,
            attribution_result=attribution_result,
            prompt=args.prompt,
            side=args.side,
            slug=slug,
            sae_series=args.sae_series
        )
        
        # ä¿å­˜æ–‡ä»¶
        save_graph_files(
            graph, slug, args.output_path, 
            args.node_threshold, args.edge_threshold
        )
        
        logger.info("åˆ†æå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


def check_dense_features(
    nodes: List[Dict[str, Any]],
    threshold: Optional[int],
    mongo_client: Optional[MongoClient],
    sae_series: str = "BT4-exp128",
    lorsa_analysis_name: Optional[str] = None,
    tc_analysis_name: Optional[str] = None
) -> List[str]:
    """
    æ£€æŸ¥å“ªäº›èŠ‚ç‚¹æ˜¯dense featureï¼ˆæ¿€æ´»æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼‰
    
    Args:
        nodes: èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«node_id, feature, layer, feature_typeç­‰ä¿¡æ¯
        threshold: æ¿€æ´»æ¬¡æ•°é˜ˆå€¼ï¼ŒNoneè¡¨ç¤ºæ— é™å¤§ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½ä¸æ˜¯denseï¼‰
        mongo_client: MongoDBå®¢æˆ·ç«¯
        sae_series: SAEç³»åˆ—åç§°
        lorsa_analysis_name: LoRSAåˆ†æåç§°æ¨¡æ¿ï¼ˆå¦‚ ""ï¼‰
        tc_analysis_name: TCåˆ†æåç§°æ¨¡æ¿ï¼ˆå¦‚ "BT4_tc_L{}M"ï¼‰BT4_lorsa_L{}A
    
    Returns:
        denseèŠ‚ç‚¹çš„node_idåˆ—è¡¨
    """
    logger = logging.getLogger(__name__)
    
    if threshold is None:
        # é˜ˆå€¼ä¸ºNoneï¼Œæ‰€æœ‰èŠ‚ç‚¹éƒ½ä¸æ˜¯dense
        return []
    
    if mongo_client is None:
        logger.warning("MongoDBå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•æ£€æŸ¥dense features")
        return []
    
    # æ‰“å°ä¼ å…¥çš„æ¨¡æ¿å‚æ•°
    logger.info(f"ğŸ” Denseæ£€æŸ¥å‚æ•°: lorsa_analysis_name={lorsa_analysis_name}, tc_analysis_name={tc_analysis_name}, threshold={threshold}")
    
    dense_node_ids = []
    not_dense_nodes = []  # è®°å½•édenseèŠ‚ç‚¹ç”¨äºè°ƒè¯•
    
    for node in nodes:
        try:
            node_id = node.get('node_id')
            feature_idx = node.get('feature')
            layer = node.get('layer')
            feature_type = node.get('feature_type', '').lower()
            
            if node_id is None or feature_idx is None or layer is None:
                logger.debug(f"è·³è¿‡èŠ‚ç‚¹ {node_id}: ç¼ºå°‘å¿…è¦ä¿¡æ¯")
                continue
            
            # æ„å»ºSAEåç§°
            sae_name = None
            if 'lorsa' in feature_type:
                if lorsa_analysis_name:
                    # ä½¿ç”¨æä¾›çš„æ¨¡æ¿
                    sae_name = lorsa_analysis_name.replace("{}", str(layer))
                else:
                    # é»˜è®¤æ ¼å¼
                    sae_name = f"lc0-lorsa-L{layer}"
            elif 'transcoder' in feature_type or 'cross layer transcoder' in feature_type:
                if tc_analysis_name:
                    # ä½¿ç”¨æä¾›çš„æ¨¡æ¿
                    sae_name = tc_analysis_name.replace("{}", str(layer))
                else:
                    # é»˜è®¤æ ¼å¼
                    sae_name = f"lc0_L{layer}M_16x_k30_lr2e-03_auxk_sparseadam"
            else:
                logger.debug(f"è·³è¿‡èŠ‚ç‚¹ {node_id}: æœªçŸ¥ç‰¹å¾ç±»å‹ {feature_type}")
                continue
            
            # è¯¦ç»†æ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„analysis_name
            logger.info(f"ğŸ“‹ èŠ‚ç‚¹ {node_id}: feature_type={feature_type}, layer={layer}, feature={feature_idx}, sae_name={sae_name}")
            
            # ä»MongoDBè·å–è¯¥ç‰¹å¾çš„æ¿€æ´»æ¬¡æ•°
            feature_data = mongo_client.get_feature(
                sae_name=sae_name,
                sae_series=sae_series,
                index=feature_idx
            )
            
            if feature_data is None:
                logger.warning(f"âŒ èŠ‚ç‚¹ {node_id}: åœ¨MongoDBä¸­æœªæ‰¾åˆ°ç‰¹å¾æ•°æ® (sae={sae_name}, sae_series={sae_series}, idx={feature_idx})")
                not_dense_nodes.append({
                    'node_id': node_id,
                    'reason': 'MongoDBä¸­æœªæ‰¾åˆ°',
                    'sae_name': sae_name,
                    'sae_series': sae_series,
                    'feature_idx': feature_idx
                })
                continue
            
            # è·å–è¯¥ç‰¹å¾çš„æ¿€æ´»æ¬¡æ•°
            if feature_data.analyses:
                analysis = feature_data.analyses[0]
                act_times = getattr(analysis, 'act_times', 0)
                
                logger.info(f"ğŸ“Š èŠ‚ç‚¹ {node_id}: act_times={act_times}, threshold={threshold}, sae_name={sae_name}")
                
                if act_times > threshold:
                    dense_node_ids.append(node_id)
                    logger.info(f"âœ… DenseèŠ‚ç‚¹: {node_id} (act_times={act_times} > threshold={threshold})")
                else:
                    not_dense_nodes.append({
                        'node_id': node_id,
                        'reason': f'act_times={act_times} <= threshold={threshold}',
                        'sae_name': sae_name,
                        'act_times': act_times
                    })
                    logger.info(f"âšª éDenseèŠ‚ç‚¹: {node_id} (act_times={act_times} <= threshold={threshold})")
            else:
                logger.warning(f"âŒ èŠ‚ç‚¹ {node_id}: æ²¡æœ‰åˆ†ææ•°æ®")
                not_dense_nodes.append({
                    'node_id': node_id,
                    'reason': 'æ²¡æœ‰åˆ†ææ•°æ®',
                    'sae_name': sae_name
                })
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥èŠ‚ç‚¹ {node.get('node_id')} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    logger.info(f"ğŸ“ˆ ç»Ÿè®¡: æ€»èŠ‚ç‚¹={len(nodes)}, DenseèŠ‚ç‚¹={len(dense_node_ids)}, éDenseèŠ‚ç‚¹={len(not_dense_nodes)}")
    if not_dense_nodes:
        logger.info(f"ğŸ” éDenseèŠ‚ç‚¹è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰:")
        for node_info in not_dense_nodes[:10]:
            logger.info(f"  - {node_info}")
    
    return dense_node_ids


if __name__ == "__main__":
    main()
