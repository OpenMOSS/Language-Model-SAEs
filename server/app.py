# NEW HEADER
import os
from pathlib import Path
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
import io
from functools import lru_cache
from typing import Any, Optional, Tuple, List, Dict
from pathlib import Path

import msgpack
import numpy as np
import plotly.graph_objects as go
import torch
from datasets import Dataset
from fastapi import FastAPI, Response, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware

try:
    from .constants import (
        BT4_MODEL_NAME,
        BT4_TC_BASE_PATH,
        BT4_LORSA_BASE_PATH,
        BT4_SAE_COMBOS,
        BT4_DEFAULT_SAE_COMBO,
        get_bt4_sae_combo,
    )
except ImportError:
    from constants import (
        BT4_MODEL_NAME,
        BT4_TC_BASE_PATH,
        BT4_LORSA_BASE_PATH,
        BT4_SAE_COMBOS,
        BT4_DEFAULT_SAE_COMBO,
        get_bt4_sae_combo,
    )
try:
    from torchvision import transforms
except ImportError:
    transforms = None
    print("WARNING: torchvision not found, image processing will be disabled")

from lm_saes.backend import LanguageModel
from lm_saes.config import MongoDBConfig, SAEConfig
from lm_saes.database import MongoClient
from lm_saes.resource_loaders import load_dataset_shard, load_model
from lm_saes.sae import SparseAutoEncoder
import subprocess
import json
import tempfile
import os
import time

import random
import chess

try:
    from transformer_lens import HookedTransformer
    HOOKED_TRANSFORMER_AVAILABLE = True
except ImportError:
    HookedTransformer = None
    HOOKED_TRANSFORMER_AVAILABLE = False
    print("WARNING: transformer_lens not found, HookedTransformer will not be available")

from lm_saes.lc0_mapping.lc0_mapping import (
    idx_to_uci_mappings,
    get_mapping_index,
)
from lm_saes.circuit.leela_board import LeelaBoard
from move_evaluation import evaluate_move_quality

try:
    from .circuit_interpretation import (
        create_circuit_annotation as create_circuit_annotation_service,
        get_circuits_by_feature as get_circuits_by_feature_service,
        get_circuit_annotation as get_circuit_annotation_service,
        list_circuit_annotations as list_circuit_annotations_service,
        update_circuit_interpretation as update_circuit_interpretation_service,
        add_feature_to_circuit as add_feature_to_circuit_service,
        remove_feature_from_circuit as remove_feature_from_circuit_service,
        update_feature_interpretation_in_circuit as update_feature_interpretation_in_circuit_service,
        delete_circuit_annotation as delete_circuit_annotation_service,
        add_edge_to_circuit as add_edge_to_circuit_service,
        remove_edge_from_circuit as remove_edge_from_circuit_service,
        update_edge_weight as update_edge_weight_service,
        set_feature_level as set_feature_level_service,
    )
except ImportError:
    from circuit_interpretation import (
        create_circuit_annotation as create_circuit_annotation_service,
        get_circuits_by_feature as get_circuits_by_feature_service,
        get_circuit_annotation as get_circuit_annotation_service,
        list_circuit_annotations as list_circuit_annotations_service,
        update_circuit_interpretation as update_circuit_interpretation_service,
        add_feature_to_circuit as add_feature_to_circuit_service,
        remove_feature_from_circuit as remove_feature_from_circuit_service,
        update_feature_interpretation_in_circuit as update_feature_interpretation_in_circuit_service,
        delete_circuit_annotation as delete_circuit_annotation_service,
        add_edge_to_circuit as add_edge_to_circuit_service,
        remove_edge_from_circuit as remove_edge_from_circuit_service,
        update_edge_weight as update_edge_weight_service,
        set_feature_level as set_feature_level_service,
    )

# Interaction functions are now implemented directly in this file

try:
    from tactic_features import analyze_tactic_features, validate_fens
    from lm_saes import LowRankSparseAttention
    TACTIC_FEATURES_AVAILABLE = True
except ImportError:
    analyze_tactic_features = None
    validate_fens = None
    LowRankSparseAttention = None
    TACTIC_FEATURES_AVAILABLE = False
    print("WARNING: tactic_features not found, tactic analysis will not be available")

try:
    from activation import get_activated_features_at_position
    ACTIVATION_MODULE_AVAILABLE = True
except ImportError:
    get_activated_features_at_position = None
    ACTIVATION_MODULE_AVAILABLE = False
    print("WARNING: activation module not found, get_features_at_position endpoint will not be available")

device = "cuda" if torch.cuda.is_available() else "cpu"

app = FastAPI()

app.add_middleware(GZipMiddleware, minimum_size=1000)

SEARCH_TRACE_OUTPUT_DIR = Path("search_trace_outputs")

client = MongoClient(MongoDBConfig())
sae_series = os.environ.get("SAE_SERIES", "default")
tokenizer_only = os.environ.get("TOKENIZER_ONLY", "false").lower() == "true"
if tokenizer_only:
    print("WARNING: Tokenizer only mode is enabled, some features may not be available")

# Remove global caches in favor of LRU cache
# sae_cache: dict[str, SparseAutoEncoder] = {}
# lm_cache: dict[str, LanguageModel] = {}
# dataset_cache: dict[tuple[str, int, int], Dataset] = {}


@lru_cache(maxsize=8)
def get_model(name: str) -> LanguageModel:
    """Load and cache a language model.

    Args:
        name: Name of the model to load

    Returns:
        LanguageModel: The loaded model

    Raises:
        ValueError: If the model is not found
    """
    cfg = client.get_model_cfg(name)
    if cfg is None:
        raise ValueError(f"Model {name} not found")
    cfg.tokenizer_only = tokenizer_only
    return load_model(cfg)


@lru_cache(maxsize=16)
def get_dataset(name: str, shard_idx: int = 0, n_shards: int = 1) -> Dataset:
    """Load and cache a dataset shard.

    Args:
        name: Name of the dataset
        shard_idx: Index of the shard to load
        n_shards: Total number of shards

    Returns:
        Dataset: The loaded dataset shard

    Raises:
        AssertionError: If the dataset is not found
    """
    cfg = client.get_dataset_cfg(name)
    assert cfg is not None, f"Dataset {name} not found"
    return load_dataset_shard(cfg, shard_idx, n_shards)


@lru_cache(maxsize=8)
def get_sae(name: str) -> SparseAutoEncoder:
    """Load and cache a sparse autoencoder.

    Args:
        name: Name of the SAE to load

    Returns:
        SparseAutoEncoder: The loaded SAE

    Raises:
        AssertionError: If the SAE is not found
    """
    path = client.get_sae_path(name, sae_series)
    assert path is not None, f"SAE {name} not found"
    cfg = SAEConfig.from_pretrained(path)
    sae = SparseAutoEncoder.from_config(cfg)
    sae.eval()
    return sae


###############################################################################
###############################################################################

CURRENT_BT4_SAE_COMBO_ID: str = BT4_DEFAULT_SAE_COMBO


def _make_combo_cache_key(model_name: str, combo_id: str | None) -> str:
    """Generate cache/log key: different keys for different combos of the same model."""

    if not combo_id:
        return model_name
    return f"{model_name}::{combo_id}"


_hooked_models: Dict[str, Any] = {}
_transcoders_cache: Dict[str, Dict[int, SparseAutoEncoder]] = {}
_lorsas_cache: Dict[str, Any] = {}  # combo_key -> List[LowRankSparseAttention]
_replacement_models_cache: Dict[str, Any] = {}  # combo_key -> ReplacementModel
_single_sae_cache: Dict[str, Any] = {}  # cache_key -> SAE (Lorsa or Transcoder)

_loading_logs: Dict[str, list] = {}  # combo_key -> [log1, log2, ...]

import threading

_global_loading_lock = threading.Lock()
_hooked_model_loading_lock = threading.Lock()  # ä¸“é—¨ç”¨äº HookedTransformer æ¨¡å‹åŠ è½½çš„é”
_hooked_model_loading_status: Dict[str, bool] = {}  # model_name -> is_loading
_hooked_model_loading_condition = threading.Condition(_hooked_model_loading_lock)  # æ¡ä»¶å˜é‡ï¼Œç”¨äºç­‰å¾…åŠ è½½å®Œæˆ

_loading_locks: Dict[str, threading.Lock] = {}  # combo_key -> Lock
_loading_status: Dict[str, dict] = {}  # combo_key -> {"is_loading": bool}
_cancel_loading: Dict[str, bool] = {}

_circuit_trace_logs: Dict[str, list] = {}  # trace_key -> [log1, log2, ...]
_circuit_trace_status: Dict[str, dict] = {}  # trace_key -> {"is_tracing": bool}
_circuit_trace_results: Dict[str, dict] = {}  # trace_key -> {"graph_data": ..., "finished_at": ts}

TRACE_RESULTS_DIR = Path(__file__).parent.parent / "circuit_trace_results"
TRACE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)

def _save_trace_result_to_disk(trace_key: str, result: dict) -> None:
    """Save trace result to disk"""
    try:
        # Use safe filename (replace special characters)
        safe_key = trace_key.replace("::", "_").replace("/", "_").replace(" ", "_")
        file_path = TRACE_RESULTS_DIR / f"{safe_key}.json"
        
        # Save result (include trace_key for recovery)
        save_data = {
            "trace_key": trace_key,
            "result": result,
            "saved_at": time.time()
        }
        
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Traceç»“æœå·²ä¿å­˜åˆ°ç£ç›˜: {file_path}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜traceç»“æœåˆ°ç£ç›˜å¤±è´¥: {e}")

def _load_trace_results_from_disk() -> None:
    """ä»ç£ç›˜åŠ è½½å·²ä¿å­˜çš„traceç»“æœ"""
    global _circuit_trace_results
    
    try:
        if not TRACE_RESULTS_DIR.exists():
            return
        
        loaded_count = 0
        for file_path in TRACE_RESULTS_DIR.glob("*.json"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    save_data = json.load(f)
                
                trace_key = save_data.get("trace_key")
                result = save_data.get("result")
                
                if trace_key and result:
                    # åªåŠ è½½æœ€è¿‘30å¤©çš„ç»“æœï¼ˆé¿å…åŠ è½½è¿‡å¤šæ—§æ•°æ®ï¼‰
                    saved_at = save_data.get("saved_at", 0)
                    if time.time() - saved_at < 30 * 24 * 3600:
                        _circuit_trace_results[trace_key] = result
                        loaded_count += 1
                    else:
                        # åˆ é™¤è¿‡æœŸæ–‡ä»¶
                        file_path.unlink()
                        print(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸçš„traceç»“æœ: {file_path}")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½traceç»“æœæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        if loaded_count > 0:
            print(f"âœ… ä»ç£ç›˜åŠ è½½äº† {loaded_count} ä¸ªtraceç»“æœ")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½traceç»“æœå¤±è´¥: {e}")

# æœåŠ¡å™¨å¯åŠ¨æ—¶åŠ è½½å·²ä¿å­˜çš„ç»“æœ
_load_trace_results_from_disk()

# ä½¿ç”¨ç»Ÿä¸€çš„æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨ä¸Šæ–¹å®šä¹‰ï¼‰
def _load_trace_result_from_disk(trace_key: str) -> dict | None:
    """ä»ç£ç›˜åŠ è½½traceç»“æœï¼ˆä½¿ç”¨ç»Ÿä¸€çš„å­˜å‚¨æ ¼å¼ï¼‰"""
    import urllib.parse
    try:
        # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å
        safe_key = trace_key.replace("::", "_").replace("/", "_").replace(" ", "_")
        file_path = TRACE_RESULTS_DIR / f"{safe_key}.json"
        
        if file_path.exists():
            with open(file_path, "r", encoding="utf-8") as f:
                save_data = json.load(f)
            
            # æ£€æŸ¥trace_keyæ˜¯å¦åŒ¹é…
            saved_trace_key = save_data.get("trace_key")
            if saved_trace_key == trace_key:
                result = save_data.get("result")
                if result:
                    print(f"âœ… ä»ç£ç›˜åŠ è½½traceç»“æœ: {file_path}")
                    return result
        
        # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•éå†æ‰€æœ‰æ–‡ä»¶æŸ¥æ‰¾åŒ¹é…çš„trace_keyï¼ˆå¤„ç†ç¼–ç å·®å¼‚ï¼‰
        if TRACE_RESULTS_DIR.exists():
            for storage_file in TRACE_RESULTS_DIR.glob("*.json"):
                try:
                    with open(storage_file, "r", encoding="utf-8") as f:
                        save_data = json.load(f)
                    
                    saved_trace_key = save_data.get("trace_key")
                    # å°è¯•è§£ç æ¯”è¾ƒï¼ˆå¤„ç†å¯èƒ½çš„ç¼–ç å·®å¼‚ï¼‰
                    if saved_trace_key == trace_key:
                        result = save_data.get("result")
                        if result:
                            print(f"âœ… ä»ç£ç›˜åŠ è½½traceç»“æœï¼ˆé€šè¿‡éå†æŸ¥æ‰¾ï¼‰: {storage_file}")
                            return result
                except Exception as e:
                    continue
        
        return None
    except FileNotFoundError:
        return None
    except Exception as e:
        print(f"âš ï¸ ä»ç£ç›˜åŠ è½½traceç»“æœå¤±è´¥ ({trace_key}): {e}")
        return None

def get_hooked_model(model_name: str = 'lc0/BT4-1024x15x32h'):
    """è·å–æˆ–åŠ è½½HookedTransformeræ¨¡å‹ - ä»…æ”¯æŒBT4ï¼ˆå¸¦å…¨å±€ç¼“å­˜å’ŒåŠ è½½é”ï¼‰"""
    global _hooked_models, _hooked_model_loading_lock, _hooked_model_loading_status, _hooked_model_loading_condition
    
    # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
    model_name = 'lc0/BT4-1024x15x32h'
    
    # å…ˆæ£€æŸ¥circuits_serviceçš„ç¼“å­˜ï¼ˆåªå¯¹æ¨¡å‹æœ¬èº«ï¼Œä¸åŒºåˆ† SAE ç»„åˆï¼‰
    if CIRCUITS_SERVICE_AVAILABLE and get_cached_models is not None:
        cached_hooked_model, _, _, _ = get_cached_models(model_name)
        if cached_hooked_model is not None:
            print(f"âœ… ä»circuits_serviceç¼“å­˜è·å–HookedTransformeræ¨¡å‹: {model_name}")
            return cached_hooked_model
    
    # ä½¿ç”¨æ¡ä»¶å˜é‡å’Œé”æ¥ä¿æŠ¤æ¨¡å‹åŠ è½½è¿‡ç¨‹
    with _hooked_model_loading_condition:
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆå¯èƒ½åœ¨ç­‰å¾…æœŸé—´å·²ç»è¢«å…¶ä»–çº¿ç¨‹åŠ è½½ï¼‰
        if model_name in _hooked_models:
            print(f"âœ… ä»æœ¬åœ°ç¼“å­˜è·å–HookedTransformeræ¨¡å‹: {model_name}")
            return _hooked_models[model_name]
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åŠ è½½
        if _hooked_model_loading_status.get(model_name, False):
            print(f"â³ æ£€æµ‹åˆ°æ¨¡å‹ {model_name} æ­£åœ¨åŠ è½½ä¸­ï¼Œç­‰å¾…åŠ è½½å®Œæˆ...")
            # ç­‰å¾…ç›´åˆ°æ¨¡å‹åŠ è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
            max_wait_time = 60
            start_time = time.time()
            while _hooked_model_loading_status.get(model_name, False) and (time.time() - start_time) < max_wait_time:
                _hooked_model_loading_condition.wait(timeout=1.0)
            
            # å†æ¬¡æ£€æŸ¥ç¼“å­˜
            if model_name in _hooked_models:
                print(f"âœ… ç­‰å¾…åä»ç¼“å­˜è·å–HookedTransformeræ¨¡å‹: {model_name}")
                return _hooked_models[model_name]
            elif (time.time() - start_time) >= max_wait_time:
                raise TimeoutError(f"ç­‰å¾…æ¨¡å‹ {model_name} åŠ è½½è¶…æ—¶ï¼ˆ{max_wait_time}ç§’ï¼‰")
            
            # å¦‚æœç­‰å¾…åä»ç„¶æ²¡æœ‰ï¼Œç»§ç»­åŠ è½½æµç¨‹
            if model_name in _hooked_models:
                return _hooked_models[model_name]
        
        # æ ‡è®°ä¸ºæ­£åœ¨åŠ è½½
        _hooked_model_loading_status[model_name] = True
        print(f"ğŸ” å¼€å§‹åŠ è½½HookedTransformeræ¨¡å‹: {model_name} (é¦–æ¬¡åŠ è½½)")
    
    # åœ¨é”å¤–æ‰§è¡Œå®é™…çš„åŠ è½½æ“ä½œï¼ˆé¿å…é•¿æ—¶é—´æŒæœ‰é”ï¼‰
    try:
        if not HOOKED_TRANSFORMER_AVAILABLE:
            raise ValueError("HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        model = HookedTransformer.from_pretrained_no_processing(
            model_name,
            dtype=torch.float32,
        ).eval()
        
        # åŠ è½½å®Œæˆåï¼Œä½¿ç”¨æ¡ä»¶å˜é‡ä¿æŠ¤ç¼“å­˜æ›´æ–°
        with _hooked_model_loading_condition:
            _hooked_models[model_name] = model
            
            # å¦‚æœcircuits_serviceå¯ç”¨ï¼Œä¹Ÿæ›´æ–°å…±äº«ç¼“å­˜
            if CIRCUITS_SERVICE_AVAILABLE and set_cached_models is not None:
                # éœ€è¦transcoderså’Œlorsasæ‰èƒ½è°ƒç”¨set_cached_modelsï¼Œè¿™é‡Œåªç¼“å­˜æ¨¡å‹
                _global_hooked_models[model_name] = model
            
            # æ ‡è®°åŠ è½½å®Œæˆ
            _hooked_model_loading_status[model_name] = False
            
            # é€šçŸ¥ç­‰å¾…çš„çº¿ç¨‹
            _hooked_model_loading_condition.notify_all()
        
        print(f"âœ… HookedTransformeræ¨¡å‹ {model_name} åŠ è½½æˆåŠŸå¹¶å·²ç¼“å­˜")
        return model
        
    except Exception as e:
        # åŠ è½½å¤±è´¥ï¼Œæ¸…é™¤åŠ è½½çŠ¶æ€
        with _hooked_model_loading_condition:
            _hooked_model_loading_status[model_name] = False
            _hooked_model_loading_condition.notify_all()
        raise e


def get_cached_sae(sae_path: str, is_lorsa: bool, device: str = "cuda"):
    """è·å–æˆ–åŠ è½½å•ä¸ªSAEï¼ˆå¸¦å…¨å±€ç¼“å­˜ï¼‰"""
    global _single_sae_cache
    
    # ä½¿ç”¨è·¯å¾„ä½œä¸ºç¼“å­˜é”®
    cache_key = f"{sae_path}::{is_lorsa}::{device}"
    
    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    if cache_key not in _single_sae_cache:
        if not HOOKED_TRANSFORMER_AVAILABLE:
            raise ValueError("HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        print(f"ğŸ” æ­£åœ¨åŠ è½½SAE: {sae_path} (ç±»å‹: {'Lorsa' if is_lorsa else 'Transcoder'})")
        
        if is_lorsa:
            from lm_saes import LowRankSparseAttention
            sae = LowRankSparseAttention.from_pretrained(
                sae_path,
                device=device,
            )
        else:
            sae = SparseAutoEncoder.from_pretrained(
                sae_path,
                dtype=torch.float32,
                device=device,
            )
        
        _single_sae_cache[cache_key] = sae
        print(f"âœ… SAEåŠ è½½æˆåŠŸ: {sae_path}")
    
    return _single_sae_cache[cache_key]

def get_cached_transcoders_and_lorsas(
    model_name: str,
    sae_combo_id: str | None = None,
) -> Tuple[Optional[Dict[int, SparseAutoEncoder]], Optional[List[LowRankSparseAttention]]]:
    """è·å–ç¼“å­˜çš„ transcoders å’Œ lorsasï¼ˆä¼˜å…ˆä½¿ç”¨ circuits_service çš„å…±äº«ç¼“å­˜ï¼‰"""

    combo_id = sae_combo_id or CURRENT_BT4_SAE_COMBO_ID
    cache_key = _make_combo_cache_key(model_name, combo_id)

    # å…ˆæ£€æŸ¥circuits_serviceçš„ç¼“å­˜
    if CIRCUITS_SERVICE_AVAILABLE and get_cached_models is not None:
        _, cached_transcoders, cached_lorsas, _ = get_cached_models(cache_key)
        if cached_transcoders is not None and cached_lorsas is not None:
            return cached_transcoders, cached_lorsas

    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    global _transcoders_cache, _lorsas_cache
    return _transcoders_cache.get(cache_key), _lorsas_cache.get(cache_key)

def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ - ä»…æ”¯æŒBT4"""
    return [
        {'name': 'lc0/BT4-1024x15x32h', 'display_name': 'BT4-1024x15x32h'},
    ]


def make_serializable(obj):
    if isinstance(obj, (torch.Tensor, np.ndarray)):
        return obj.tolist()
    if isinstance(obj, dict):
        return {k: make_serializable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [make_serializable(v) for v in obj]
    return obj


def trim_minimum(
    origins: list[dict[str, Any] | None],
    feature_acts_indices: np.ndarray,
    feature_acts_values: np.ndarray,
) -> tuple[list[dict[str, Any] | None], np.ndarray, np.ndarray]:
    """Trim multiple arrays to the length of the shortest non-None array.

    Args:
        origins: Origins
        feature_acts_indices: Feature acts indices
        feature_acts_values: Feature acts values

    Returns:
        list: List of trimmed arrays
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºå›½é™…è±¡æ£‹æ¨¡å‹ï¼ˆé€šè¿‡æ£€æŸ¥originsä¸­æ˜¯å¦åŒ…å«FENæ•°æ®ï¼‰
    has_fen_data = any(
        origin is not None and origin.get("key") == "fen" 
        for origin in origins if origin is not None
    )

    if has_fen_data:
        # å¯¹äºå›½é™…è±¡æ£‹æ¨¡å‹ï¼Œå¼ºåˆ¶æœ€å°é•¿åº¦ä¸º64ï¼ˆæ£‹ç›˜æ ¼å­æ•°ï¼‰
        min_length = max(64, feature_acts_indices[-1] + 10)
    else:
        # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        min_length = min(len(origins), feature_acts_indices[-1] + 10)
    
    feature_acts_indices_mask = feature_acts_indices <= min_length
    return origins[:int(min_length)], feature_acts_indices[feature_acts_indices_mask], feature_acts_values[feature_acts_indices_mask]


@app.exception_handler(AssertionError)
async def assertion_error_handler(request, exc):
    return Response(content=str(exc), status_code=400)


@app.exception_handler(torch.cuda.OutOfMemoryError)
async def oom_error_handler(request, exc):
    print("CUDA Out of memory. Clearing cache.")
    # Clear LRU caches
    get_model.cache_clear()
    get_dataset.cache_clear()
    get_sae.cache_clear()
    return Response(content="CUDA Out of memory", status_code=500)


@app.get("/dictionaries")
def list_dictionaries():
    return client.list_saes(sae_series=sae_series, has_analyses=True)


###############################################################################
# BT4 SAE ç»„åˆç›¸å…³ API
###############################################################################


@app.get("/sae/combos")
def list_sae_combos() -> Dict[str, Any]:
    """
    è¿”å›å¯é€‰çš„ BT4 SAE ç»„åˆåˆ—è¡¨åŠé»˜è®¤ç»„åˆã€‚

    è¿™äº›ç»„åˆæ¥è‡ª `exp/38mongoanalyses/ç»„åˆ.txt`ï¼Œå‰ç«¯åªèƒ½åœ¨è¿™äº›ç»„åˆä¸­é€‰æ‹©ã€‚
    """

    combos = [
        {
            "id": cfg["id"],
            "label": cfg["label"],
            "tc_base_path": cfg["tc_base_path"],
            "lorsa_base_path": cfg["lorsa_base_path"],
        }
        for cfg in BT4_SAE_COMBOS.values()
    ]

    return {
        "default_id": BT4_DEFAULT_SAE_COMBO,
        "current_id": CURRENT_BT4_SAE_COMBO_ID,
        "combos": combos,
    }


@app.get("/images/{dataset_name}")
def get_image(dataset_name: str, context_idx: int, image_idx: int, shard_idx: int = 0, n_shards: int = 1):
    dataset = get_dataset(dataset_name, shard_idx, n_shards)
    data = dataset[int(context_idx)]

    image_key = next((key for key in ["image", "images"] if key in data), None)
    if image_key is None:
        return Response(content="Image not found", status_code=404)

    if len(data[image_key]) <= image_idx:
        return Response(content="Image not found", status_code=404)

    image_tensor = data[image_key][image_idx]

    # Convert tensor to PIL Image and then to bytes
    image = transforms.ToPILImage()(image_tensor.to(torch.uint8))
    img_byte_arr = io.BytesIO()
    image.save(img_byte_arr, format="PNG")

    return Response(content=img_byte_arr.getvalue(), media_type="image/png")


@app.get("/dictionaries/{name}/features/{feature_index}")
def get_feature(
    name: str,
    feature_index: str | int,
    feature_analysis_name: str | None = None,
):
    # Parse feature_index if it's a string
    if isinstance(feature_index, str) and feature_index != "random":
        try:
            feature_index = int(feature_index)
        except ValueError:
            return Response(
                content=f"Feature index {feature_index} is not a valid integer",
                status_code=400,
            )
    print(f'{feature_analysis_name = }')
    print(f'{name = }')
    # Get feature data
    feature = (
        client.get_random_alive_feature(
            sae_name=name,
            sae_series=sae_series,
            name=feature_analysis_name,
        )
        if feature_index == "random"
        else client.get_feature(
            sae_name=name,
            sae_series=sae_series,
            index=feature_index)
    )
    print(f'{feature = }')
    
    if feature is None:
        return Response(
            content=f"Feature {feature_index} not found in SAE {name}",
            status_code=404,
        )

    analysis = next(
        (
            a for a in feature.analyses
            if a.name == feature_analysis_name or feature_analysis_name is None
        ),
        None,
    )
    if analysis is None:
        return Response(
            content=f"Feature analysis {feature_analysis_name} not found in SAE {name}"
            if feature_analysis_name is not None
            else f"No feature analysis found in SAE {name}",
            status_code=404,
        )

    def process_sample(
        *,
        sparse_feature_acts,
        context_idx,
        dataset_name,
        model_name,
        shard_idx=None,
        n_shards=None,
    ):
        """Process a sample to extract and format feature data.

        Args:
            sparse_feature_acts: Sparse feature activations,
                optional z pattern activations
            decoder_norms: Decoder norms
            context_idx: Context index in the dataset
            dataset_name: Name of the dataset
            model_name: Name of the model
            shard_idx: Index of the dataset shard, defaults to 0
            n_shards: Total number of shards, defaults to 1

        Returns:
            dict: Processed sample data
        """
        model = get_model(model_name)
        
        data = get_dataset(dataset_name, shard_idx, n_shards)[context_idx.item()]

        # Get origins for the features
        origins = model.trace({k: [v] for k, v in data.items()})[0]

        # Process image data if present
        image_key = next(
            (key for key in ["image", "images"] if key in data),
            None,
        )
        if image_key is not None:
            image_urls = [
                f"/images/{dataset_name}?context_idx={context_idx}&"
                f"shard_idx={shard_idx}&n_shards={n_shards}&"
                f"image_idx={img_idx}"
                for img_idx in range(len(data[image_key]))
            ]
            del data[image_key]
            data["images"] = image_urls

        # Trim to matching lengths
        (
            feature_acts_indices,
            feature_acts_values,
            z_pattern_indices,
            z_pattern_values,
        ) = sparse_feature_acts

        origins, feature_acts_indices, feature_acts_values = trim_minimum(
            origins,
            feature_acts_indices,
            feature_acts_values,
        )
        assert (
            origins is not None
            and feature_acts_indices is not None
            and feature_acts_values is not None
        ), "Origins and feature acts must not be None"

        # æ£€æŸ¥æ˜¯å¦ä¸ºå›½é™…è±¡æ£‹æ¨¡å‹ï¼ˆå¤šç§æ£€æµ‹æ–¹å¼ï¼‰
        has_fen_data = any(
            origin is not None and origin.get("key") == "fen" 
            for origin in origins if origin is not None
        )
        
        # é€šè¿‡æ¨¡å‹åç§°æˆ–æ•°æ®é›†åç§°åˆ¤æ–­æ˜¯å¦ä¸ºæ£‹ç±»æ¨¡å‹
        is_chess_model = (
            has_fen_data or 
            "chess" in model_name.lower() or 
            "lc0" in model_name.lower() or
            "chess" in dataset_name.lower() or
            "lc0" in dataset_name.lower()
        )
        
        if is_chess_model:
            # å¯¹äºå›½é™…è±¡æ£‹æ¨¡å‹ï¼Œåˆ›å»ºé•¿åº¦ä¸º64çš„å¯†é›†æ¿€æ´»æ•°ç»„
            dense_feature_acts = np.zeros(64)
            
            # å¼ºåˆ¶ç±»å‹
            feature_acts_indices = np.asarray(feature_acts_indices, dtype=np.int64)
            feature_acts_values = np.asarray(feature_acts_values, dtype=np.float32)

            # å¯é€‰ï¼šè¿‡æ»¤éæ³•ç´¢å¼•
            valid_mask = (feature_acts_indices >= 0) & (feature_acts_indices < 64)
            feature_acts_indices = feature_acts_indices[valid_mask]
            feature_acts_values = feature_acts_values[valid_mask]

            # ç„¶åå† zip å¾ªç¯æˆ–ç›´æ¥å‘é‡åŒ–å†™å…¥
            for idx, val in zip(feature_acts_indices, feature_acts_values):
                        dense_feature_acts[idx] = val
            
            # ç¡®ä¿FENæ•°æ®å­˜åœ¨
            if "fen" not in data:
                # å¦‚æœæ²¡æœ‰FENæ•°æ®ï¼Œå°è¯•ä»originsä¸­æå–
                fen_origins = [origin for origin in origins if origin is not None and origin.get("key") == "fen"]
                if fen_origins:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªFEN originçš„èŒƒå›´æ¥æå–æ–‡æœ¬
                    fen_origin = fen_origins[0]
                    if "range" in fen_origin and "text" in data:
                        start, end = fen_origin["range"]
                        data["fen"] = data["text"][start:end]
                    else:
                        # å¦‚æœæ²¡æœ‰rangeä¿¡æ¯ï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
                        data["fen"] = data.get("text", "")
                else:
                    # å¦‚æœå®Œå…¨æ²¡æœ‰FENä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
                    data["fen"] = "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
            
        else:
            # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            dense_feature_acts = np.zeros(len(origins))
            
            for i, (idx, val) in enumerate(zip(feature_acts_indices, feature_acts_values)):
                try:
                    # ç¡®ä¿idxæ˜¯æœ‰æ•ˆçš„æ•´æ•°
                    if hasattr(idx, 'item'):
                        idx = idx.item()
                    elif hasattr(idx, '__int__'):
                        idx = int(idx)
                    else:
                        idx = int(float(idx))
                    
                    # ç¡®ä¿valæ˜¯æœ‰æ•ˆçš„æ•°å€¼
                    if hasattr(val, 'item'):
                        val = val.item()
                    elif hasattr(val, '__float__'):
                        val = float(val)
                    else:
                        val = float(val)
                    
                    # æ£€æŸ¥ç´¢å¼•èŒƒå›´
                    if 0 <= idx < len(origins):
                        dense_feature_acts[idx] = val
                        
                except (ValueError, TypeError, IndexError):
                    continue

        # Process text data if present
        if "text" in data:
            text_ranges = [
                origin["range"] for origin in origins
                if origin is not None and origin["key"] == "text"
            ]
            if text_ranges:
                max_text_origin = max(text_ranges, key=lambda x: x[1])
                data["text"] = data["text"][: max_text_origin[1]]

        # å¯¹äºå›½é™…è±¡æ£‹æ¨¡å‹ï¼Œä½¿ç”¨FENä½œä¸ºæ–‡æœ¬
        if is_chess_model:
            data["text"] = data.get("fen", "No FEN data")

        return {
            **data,
            "origins": origins,
            "feature_acts": dense_feature_acts,  # è¿”å›å¯†é›†æ¿€æ´»æ•°ç»„
            "feature_acts_indices": feature_acts_indices,
            "feature_acts_values": feature_acts_values,
            "z_pattern_indices": z_pattern_indices,
            "z_pattern_values": z_pattern_values,
        }
    
    def process_sparse_feature_acts(
        feature_acts_indices: np.ndarray,
        feature_acts_values: np.ndarray,
        z_pattern_indices: np.ndarray | None = None,
        z_pattern_values: np.ndarray | None = None,
    ) -> list[tuple[np.ndarray, np.ndarray, np.ndarray | None, np.ndarray | None]]:
        """Process sparse feature acts.
        
        Args:
            feature_acts_indices: Feature acts indices
            feature_acts_values: Feature acts values
            z_pattern_indices: Z pattern indices
            z_pattern_values: Z pattern values
        
        TODO: This is really ugly, we should find a better way to do this.
        """

        if feature_acts_indices.size == 0 or feature_acts_indices.shape[1] == 0:
            return


        _, feature_acts_counts = np.unique(
            feature_acts_indices[0],
            return_counts=True,
        )

        _, z_pattern_counts = (
            np.unique(z_pattern_indices[0], return_counts=True)
            if z_pattern_indices is not None
            else (None, None)
        )

        feature_acts_sample_ranges = np.concatenate(
            [[0], np.cumsum(feature_acts_counts)]
        )

        z_pattern_sample_ranges = (
            np.concatenate([[0], np.cumsum(z_pattern_counts)])
            if z_pattern_counts is not None
            else None
        )

        feature_acts_sample_ranges = list(
            zip(feature_acts_sample_ranges[:-1], feature_acts_sample_ranges[1:])
        )

        if z_pattern_sample_ranges is not None:
            z_pattern_sample_ranges = list(
                zip(z_pattern_sample_ranges[:-1], z_pattern_sample_ranges[1:])
            )
            if len(feature_acts_sample_ranges) != len(z_pattern_sample_ranges):
                z_pattern_sample_ranges = [(None, None)] * len(feature_acts_sample_ranges)
        else:
            z_pattern_sample_ranges = [(None, None)] * len(feature_acts_sample_ranges)

        for (feature_acts_start, feature_acts_end), (z_pattern_start, z_pattern_end) in zip(feature_acts_sample_ranges, z_pattern_sample_ranges):
            feature_acts_indices_i = feature_acts_indices[1, feature_acts_start:feature_acts_end]
            feature_acts_values_i = feature_acts_values[feature_acts_start:feature_acts_end]
            z_pattern_indices_i = z_pattern_indices[1:, z_pattern_start:z_pattern_end] if z_pattern_indices is not None else None
            z_pattern_values_i = z_pattern_values[z_pattern_start:z_pattern_end] if z_pattern_values is not None else None

            yield feature_acts_indices_i, feature_acts_values_i, z_pattern_indices_i, z_pattern_values_i


    sample_groups = []
    for sampling in analysis.samplings:
        try:
            # Using zip to process correlated data instead of indexing
            samples = [
                process_sample(
                    sparse_feature_acts=sparse_feature_acts,
                    context_idx=context_idx,
                    dataset_name=dataset_name,
                    model_name=model_name,
                    shard_idx=shard_idx,
                    n_shards=n_shards,
                )
                for sparse_feature_acts, context_idx, dataset_name, model_name, shard_idx, n_shards in zip(
                    process_sparse_feature_acts(
                        sampling.feature_acts_indices,
                        sampling.feature_acts_values,
                        sampling.z_pattern_indices,
                        sampling.z_pattern_values,
                    ),
                    sampling.context_idx,
                    sampling.dataset_name,
                    sampling.model_name,
                    sampling.shard_idx if sampling.shard_idx is not None else [0] * len(sampling.feature_acts_indices),
                    sampling.n_shards if sampling.n_shards is not None else [1] * len(sampling.feature_acts_indices),
                )
            ]
            

            sample_groups.append(
                {
                    "analysis_name": sampling.name,
                    "samples": samples,
                }
            )
        except Exception as e:
            # è¿”å›400é”™è¯¯å“åº”
            return Response(
                content=f"å¤„ç†sampling '{sampling.name}' æ—¶å‡ºé”™: {str(e)}", 
                status_code=400
            )

    # Prepare response
    response_data = {
        "feature_index": feature.index,
        "analysis_name": analysis.name,
        "interpretation": feature.interpretation,
        "dictionary_name": feature.sae_name,
        "decoder_norms": analysis.decoder_norms,
        "decoder_similarity_matrices": analysis.decoder_similarity_matrices,
        "decoder_inner_product_matrices": analysis.decoder_inner_product_matrices,
        "act_times": analysis.act_times,
        "max_feature_act": analysis.max_feature_acts,
        "n_analyzed_tokens": analysis.n_analyzed_tokens,
        "sample_groups": sample_groups,
        "is_bookmarked": client.is_bookmarked(sae_name=name, sae_series=sae_series, feature_index=feature.index),
    }

    return Response(
        content=msgpack.packb(make_serializable(response_data)),
        media_type="application/x-msgpack",
    )


@app.get("/dictionaries/{name}")
def get_dictionary(name: str):
    # Get feature activation times
    feature_activation_times = client.get_feature_act_times(name, sae_series=sae_series)
    if feature_activation_times is None:
        return Response(content=f"Dictionary {name} not found", status_code=404)

    # Create histogram of log activation times
    log_act_times = np.log10(np.array(list(feature_activation_times.values())))
    feature_activation_times_histogram = go.Histogram(
        x=log_act_times,
        nbinsx=100,
        hovertemplate="Count: %{y}<br>Range: %{x}<extra></extra>",
        marker_color="#636EFA",
        showlegend=False,
    ).to_plotly_json()

    # Get alive feature count
    alive_feature_count = client.get_alive_feature_count(name, sae_series=sae_series)
    if alive_feature_count is None:
        return Response(content=f"SAE {name} not found", status_code=404)

    # Prepare and return response
    response_data = {
        "dictionary_name": name,
        "feature_activation_times_histogram": [feature_activation_times_histogram],
        "alive_feature_count": alive_feature_count,
    }

    return Response(
        content=msgpack.packb(make_serializable(response_data)),
        media_type="application/x-msgpack",
    )


@app.get("/dictionaries/{name}/analyses")
def get_analyses(name: str):
    """Get all available analyses for a dictionary.

    Args:
        name: Name of the dictionary/SAE

    Returns:
        List of analysis names
    """
    # Get a random feature to check its available analyses
    feature = client.get_random_alive_feature(sae_name=name, sae_series=sae_series)
    if feature is None:
        return Response(content=f"Dictionary {name} not found", status_code=404)

    # Extract unique analysis names from feature
    analyses = list(set(analysis.name for analysis in feature.analyses))
    return analyses


@app.post("/dictionaries/{name}/features/{feature_index}/analyze_fen")
def analyze_fen_for_feature(name: str, feature_index: int, request: dict):
    fen = request.get("fen")
    if not fen:
        raise HTTPException(status_code=400, detail="FENå­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
    
    try:
        if not HOOKED_TRANSFORMER_AVAILABLE:
            raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        # ä»SAEåç§°ä¸­æå–å±‚å·å’Œç»„åˆä¿¡æ¯
        import re
        layer_match = re.search(r'L(\d+)', name)
        if not layer_match:
            raise HTTPException(status_code=400, detail=f"æ— æ³•ä»SAEåç§° {name} ä¸­æå–å±‚å·")
        layer = int(layer_match.group(1))
        
        # åˆ¤æ–­æ˜¯lorsaè¿˜æ˜¯transcoder
        is_lorsa_name = 'lorsa' in name.lower()
        is_tc_name = 'tc' in name.lower() or 'transcoder' in name.lower()
        
        # ä»SAEåç§°ä¸­æå–ç»„åˆä¿¡æ¯ï¼ˆä¾‹å¦‚ k30_e16 -> k_30_e_16ï¼‰
        # æˆ–è€…å°è¯•åŒ¹é…æ‰€æœ‰å·²çŸ¥çš„ç»„åˆ
        combo_id = None
        combo_match = re.search(r'k(\d+)_e(\d+)', name)
        if combo_match:
            k_val = combo_match.group(1)
            e_val = combo_match.group(2)
            combo_id = f"k_{k_val}_e_{e_val}"
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»„åˆä¿¡æ¯ï¼Œå°è¯•é€šè¿‡åŒ¹é…SAEåç§°æ¨¡æ¿æ¥ç¡®å®šç»„åˆ
            # éå†æ‰€æœ‰ç»„åˆï¼Œçœ‹å“ªä¸ªæ¨¡æ¿åŒ¹é…
            for test_combo_id, test_combo_cfg in BT4_SAE_COMBOS.items():
                if is_lorsa_name:
                    template = test_combo_cfg.get("lorsa_sae_name_template", "")
                else:
                    template = test_combo_cfg.get("tc_sae_name_template", "")
                
                # å°è¯•ç”¨å±‚å·æ›¿æ¢æ¨¡æ¿ï¼Œçœ‹æ˜¯å¦åŒ¹é…
                if template:
                    template_with_layer = template.format(layer=layer)
                    # æ£€æŸ¥åç§°æ˜¯å¦åŒ¹é…ï¼ˆå…è®¸éƒ¨åˆ†åŒ¹é…ï¼Œå› ä¸ºå¯èƒ½æœ‰å…¶ä»–åç¼€ï¼‰
                    if template_with_layer in name or name.startswith(template_with_layer.split('{')[0]):
                        combo_id = test_combo_id
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤ç»„åˆ
            if combo_id is None:
                combo_id = BT4_DEFAULT_SAE_COMBO
        
        # è·å–ç»„åˆé…ç½®
        combo_cfg = get_bt4_sae_combo(combo_id)
        
        # è·å–æ¨¡å‹
        model_name = "lc0/BT4-1024x15x32h"
        model = get_hooked_model(model_name)
        
        # æ ¹æ®ç»„åˆé…ç½®åŠ è½½SAEï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        if is_lorsa_name:
            # åŠ è½½Lorsa
            lorsa_base_path = combo_cfg["lorsa_base_path"]
            lorsa_path = f"{lorsa_base_path}/L{layer}"
            
            if not os.path.exists(lorsa_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Lorsa not found at {lorsa_path} for layer {layer}"
                )
            
            if not HOOKED_TRANSFORMER_AVAILABLE:
                raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½Lorsa")
            
            # ä½¿ç”¨ç¼“å­˜åŠ è½½SAE
            sae = get_cached_sae(lorsa_path, is_lorsa=True, device=device)
        elif is_tc_name:
            # åŠ è½½Transcoder
            tc_base_path = combo_cfg["tc_base_path"]
            tc_path = f"{tc_base_path}/L{layer}"
            
            if not os.path.exists(tc_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Transcoder not found at {tc_path} for layer {layer}"
                )
            
            # ä½¿ç”¨ç¼“å­˜åŠ è½½SAE
            sae = get_cached_sae(tc_path, is_lorsa=False, device=device)
        else:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ³•ç¡®å®šSAEç±»å‹ï¼Œåç§°åº”åŒ…å«'lorsa'æˆ–'tc'/'transcoder'"
            )
        
        # è¿è¡Œæ¨¡å‹è·å–æ¿€æ´»å€¼
        with torch.no_grad():
            # ç¡®å®šè¦hookçš„ç‚¹
            if is_lorsa_name:
                # Lorsa ä½¿ç”¨ hook_attn_in
                hook_name = f"blocks.{layer}.hook_attn_in"
            else:
                # Transcoder ä½¿ç”¨ resid_mid_after_ln
                hook_name = f"blocks.{layer}.resid_mid_after_ln"
            
            _, cache = model.run_with_cache(fen, prepend_bos=False)            
            
            if cache is None or len(cache) == 0:
                raise HTTPException(
                    status_code=500,
                    detail=f"æ¨¡å‹è¿è¡Œåcacheä¸ºç©ºï¼Œæ— æ³•è·å–æ¿€æ´»å€¼ã€‚è¯·æ£€æŸ¥FENå­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆã€‚FEN: {fen}"
                )
            print(f'{cache.keys() = }')
            try:
                all_hooks = list(cache.keys())
            except Exception as e:
                all_hooks = []
            
            layer_hooks = [k for k in all_hooks if f"blocks.{layer}" in str(k)]
            if layer_hooks == []:
                # æ£€æŸ¥ç›¸é‚»å±‚æ˜¯å¦æœ‰hookç‚¹
                for test_layer in [layer-1, layer+1, 0, model.cfg.n_layers-1]:
                    if 0 <= test_layer < model.cfg.n_layers:
                        test_hooks = [k for k in all_hooks if f"blocks.{test_layer}" in str(k)]
                        if test_hooks:
                            print(f"   - å¯¹æ¯”: å±‚ {test_layer} æœ‰ {len(test_hooks)} ä¸ªhookç‚¹ï¼Œç¤ºä¾‹: {test_hooks[:3]}")
                            break
            
            print(f"   - æœŸæœ›çš„hookç‚¹: {hook_name}")
            
            # æ£€æŸ¥hookç‚¹æ˜¯å¦å­˜åœ¨
            hook_exists = False
            try:
                hook_exists = hook_name in cache
                print(f"   - Hookç‚¹æ˜¯å¦å­˜åœ¨: {hook_exists}")
            except Exception as e:
                print(f"   - æ£€æŸ¥hookç‚¹å­˜åœ¨æ€§æ—¶å‡ºé”™: {e}")
            
            if not hook_exists:
                # å°è¯•æŸ¥æ‰¾ç±»ä¼¼çš„hookç‚¹
                similar_hooks = [k for k in all_hooks if f"blocks.{layer}" in str(k)]
                # ä¹Ÿå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"attn"æˆ–"resid"çš„hookç‚¹ï¼ˆç”¨äºLorsaå’ŒTranscoderï¼‰
                if is_lorsa_name:
                    attn_hooks = [k for k in all_hooks if f"blocks.{layer}" in str(k) and "attn" in str(k).lower()]
                    print(f"   - åŒ…å«'attn'çš„hookç‚¹: {attn_hooks[:10]}")
                else:
                    resid_hooks = [k for k in all_hooks if f"blocks.{layer}" in str(k) and "resid" in str(k).lower()]
                    print(f"   - åŒ…å«'resid'çš„hookç‚¹: {resid_hooks[:10]}")
                
                error_detail = (
                    f"æ— æ³•æ‰¾åˆ°å±‚ {layer} çš„æ¿€æ´»å€¼ã€‚SAEç±»å‹: {'Lorsa' if is_lorsa_name else 'Transcoder'}ã€‚"
                    f"æœŸæœ›çš„hookç‚¹: {hook_name}ã€‚"
                    f"æ€»hookç‚¹æ•°é‡: {len(all_hooks)}ã€‚"
                    f"åŒ…å«'blocks.{layer}'çš„hookç‚¹: {similar_hooks[:20]}ã€‚"
                    f"æ‰€æœ‰hookç‚¹ç¤ºä¾‹: {all_hooks[:20] if len(all_hooks) > 0 else 'æ— '}"
                )
                raise HTTPException(status_code=500, detail=error_detail)
            
            activations = cache[hook_name]  # shape: [batch, seq, ...] é€šå¸¸æ˜¯ [1, seq_len, d_model]
            
            # ç¡®ä¿activationsæœ‰æ­£ç¡®çš„ç»´åº¦
            # Lorsa å’Œ Transcoder çš„ encode æ–¹æ³•éƒ½éœ€è¦ batch ç»´åº¦
            # å¦‚æœç¼ºå°‘ batch ç»´åº¦ï¼Œæ·»åŠ ä¸€ä¸ª
            if activations.dim() == 1:
                # [d_model] -> [1, d_model]
                activations = activations.unsqueeze(0).unsqueeze(0)  # [1, 1, d_model]
            elif activations.dim() == 2:
                # [seq_len, d_model] -> [1, seq_len, d_model]
                activations = activations.unsqueeze(0)  # [1, seq_len, d_model]
            # å¦‚æœå·²ç»æ˜¯3ç»´ [batch, seq_len, d_model]ï¼Œç›´æ¥ä½¿ç”¨
            
            print(f"   - Activationså½¢çŠ¶: {activations.shape}")
            
            # å¯¹äºBT4æ¨¡å‹ï¼ŒFENè¾“å…¥åï¼Œseq_lené€šå¸¸æ˜¯64ï¼ˆ64ä¸ªæ ¼å­ï¼‰
            seq_len = activations.shape[1] if activations.dim() >= 2 else activations.shape[0]
            print(f"   - åºåˆ—é•¿åº¦: {seq_len}")
        
        # ä½¿ç”¨SAEç¼–ç 
        # æ£€æŸ¥SAEç±»å‹ï¼ˆæˆ‘ä»¬å·²ç»çŸ¥é“æ˜¯lorsaè¿˜æ˜¯transcoderï¼‰
        # ä½†ä¸ºäº†å®‰å…¨ï¼Œä¹Ÿæ£€æŸ¥ä¸€ä¸‹ç±»å‹
        sae_type_str = str(type(sae))
        is_lorsa = is_lorsa_name or 'LowRankSparseAttention' in sae_type_str
        
        if is_lorsa:
            # Lorsaç¼–ç ï¼Œè·å–featureæ¿€æ´»å€¼
            # Lorsaçš„encodeæ–¹æ³•æœŸæœ›è¾“å…¥æ˜¯ [batch, seq_len, d_model] å½¢çŠ¶
            feature_acts = sae.encode(
                activations,  # ä½¿ç”¨å¸¦batchç»´åº¦çš„activations
                return_hidden_pre=False,
                return_attention_pattern=False
            )
            
            print(f"   - Feature actså½¢çŠ¶ï¼ˆç¼–ç åï¼‰: {feature_acts.shape}")
            
            # ç§»é™¤batchç»´åº¦
            if feature_acts.dim() == 3:
                feature_acts = feature_acts[0]  # [seq_len, d_sae] - ä½¿ç”¨ç´¢å¼•è€Œä¸æ˜¯squeezeï¼Œæ›´å®‰å…¨
            elif feature_acts.dim() == 2:
                # å·²ç»æ˜¯ [seq_len, d_sae]ï¼Œä¸éœ€è¦å¤„ç†
                pass
            else:
                raise ValueError(f"æ„å¤–çš„feature_actsç»´åº¦: {feature_acts.shape}")
            
            # è·å–æŒ‡å®šfeatureçš„æ¿€æ´»å€¼
            # feature_acts shape: [seq_len, d_sae]
            if feature_acts.dim() == 2:
                # å–æ‰€æœ‰ä½ç½®çš„æ¿€æ´»å€¼ï¼Œshape: [seq_len]
                feature_activation_values = feature_acts[:, feature_index].detach().cpu().numpy()
            else:
                feature_activation_values = feature_acts[feature_index].detach().cpu().unsqueeze(0).numpy()
            
            # æ„å»º64ä¸ªæ ¼å­çš„æ¿€æ´»å€¼æ•°ç»„
            seq_len = len(feature_activation_values)
            if seq_len == 64:
                activations_64 = feature_activation_values
            elif seq_len == 1:
                # å¦‚æœåªæœ‰1ä¸ªå€¼ï¼Œå¤åˆ¶åˆ°æ‰€æœ‰64ä¸ªä½ç½®
                # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨æ¨¡å‹è¾“å‡ºåªæœ‰1ä¸ªtokenæ—¶
                activations_64 = np.full(64, feature_activation_values[0])
            else:
                # å¦‚æœé•¿åº¦ä¸æ˜¯64ï¼Œå¡«å……æˆ–æˆªæ–­åˆ°64
                activations_64 = np.zeros(64)
                min_len = min(seq_len, 64)
                activations_64[:min_len] = feature_activation_values[:min_len]
            
            # ä½¿ç”¨ encode_z_pattern_for_head è®¡ç®—æŒ‡å®šfeatureçš„z_pattern
            # è¿™ä¸ªæ–¹æ³•ä¼šé’ˆå¯¹ç‰¹å®šçš„headï¼ˆfeature_indexï¼‰è®¡ç®—z_patternï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰headå–å¹³å‡
            z_pattern_indices = None
            z_pattern_values = None
            try:
                # ç¡®ä¿ activations åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if activations.device != sae.cfg.device:
                    activations = activations.to(sae.cfg.device)
                
                # ä½¿ç”¨ encode_z_pattern_for_head è®¡ç®—è¯¥featureçš„z_pattern
                # head_idx æ˜¯ feature_indexï¼ˆåœ¨Lorsaä¸­ï¼Œæ¯ä¸ªfeatureå¯¹åº”ä¸€ä¸ªheadï¼‰
                head_idx = torch.tensor([feature_index], device=activations.device)
                z_pattern = sae.encode_z_pattern_for_head(activations, head_idx)
                # z_pattern shape: [n_active_features, q_pos, k_pos]ï¼Œè¿™é‡Œæ˜¯ [1, seq_len, seq_len]
                
                print(f"   - Z patternå½¢çŠ¶: {z_pattern.shape}")
                
                # è·å–è¯¥featureåœ¨æ‰€æœ‰ä½ç½®çš„z_pattern
                # z_pattern[0] shape: [q_pos, k_pos]ï¼Œå³ [seq_len, seq_len]
                z_pattern_2d = z_pattern[0]  # [seq_len, seq_len]
                
                # æ‰¾å‡ºæ‰€æœ‰æ¿€æ´»çš„ä½ç½®ï¼ˆéé›¶æ¿€æ´»å€¼çš„ä½ç½®ï¼‰
                active_positions = np.where(activations_64 != 0)[0]
                
                if len(active_positions) > 0:
                    # å¯¹äºæ¯ä¸ªæ¿€æ´»çš„ä½ç½®ï¼Œæå–å…¶z_patternå¹¶åˆå¹¶
                    all_z_pattern_indices = []
                    all_z_pattern_values = []
                    
                    for pos in active_positions:
                        if pos < z_pattern_2d.shape[0]:
                            # è·å–è¯¥ä½ç½®ï¼ˆä½œä¸ºqueryï¼‰å¯¹æ‰€æœ‰keyä½ç½®çš„z_pattern
                            z_pattern_for_pos = z_pattern_2d[pos, :].detach().cpu().numpy()  # [seq_len]
                            
                            # æ‰¾å‡ºéé›¶å€¼
                            nonzero_mask = np.abs(z_pattern_for_pos) > 1e-6  # è¿‡æ»¤å¾ˆå°çš„å€¼
                            if np.any(nonzero_mask):
                                nonzero_indices = np.where(nonzero_mask)[0]
                                nonzero_values = z_pattern_for_pos[nonzero_indices]
                                
                                # æ·»åŠ  [query_pos, key_pos] å¯¹
                                for key_pos, value in zip(nonzero_indices, nonzero_values):
                                    all_z_pattern_indices.append([int(pos), int(key_pos)])
                                    all_z_pattern_values.append(float(value))
                    
                    if len(all_z_pattern_indices) > 0:
                        z_pattern_indices = all_z_pattern_indices
                        z_pattern_values = all_z_pattern_values
                        print(f"   - Z pattern: æ‰¾åˆ° {len(z_pattern_indices)} ä¸ªéé›¶è¿æ¥")
                    else:
                        print(f"   - Z pattern: æœªæ‰¾åˆ°éé›¶è¿æ¥")
                else:
                    print(f"   - Z pattern: æ²¡æœ‰æ¿€æ´»çš„ä½ç½®")
                    
            except Exception as e:
                print(f"   - è®¡ç®—z_patternæ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                z_pattern_indices = None
                z_pattern_values = None
            
            # æ„å»ºç¨€ç–æ ¼å¼çš„æ¿€æ´»å€¼ï¼ˆåªè¿”å›éé›¶å€¼ï¼‰
            non_zero_mask = activations_64 != 0
            feature_acts_indices = np.where(non_zero_mask)[0].tolist()
            feature_acts_values = activations_64[non_zero_mask].tolist()
            
            return {
                "feature_acts_indices": feature_acts_indices,
                "feature_acts_values": feature_acts_values,
                "z_pattern_indices": z_pattern_indices,
                "z_pattern_values": z_pattern_values,
            }
        else:
            # Transcoderç¼–ç ï¼Œä¹Ÿéœ€è¦batchç»´åº¦
            # Transcoderçš„encodeæ–¹æ³•æœŸæœ›è¾“å…¥æ˜¯ [batch, seq_len, d_model] å½¢çŠ¶
            encode_result = sae.encode(activations)  # ä½¿ç”¨å¸¦batchç»´åº¦çš„activations
            feature_acts = encode_result  # shape: [batch, seq_len, d_sae]ï¼Œé€šå¸¸æ˜¯ [1, seq_len, d_sae]
            
            print(f"   - Feature actså½¢çŠ¶ï¼ˆç¼–ç åï¼‰: {feature_acts.shape}")
            
            # ç§»é™¤batchç»´åº¦
            if feature_acts.dim() == 3:
                feature_acts = feature_acts[0]  # [seq_len, d_sae]
            elif feature_acts.dim() == 2:
                # å·²ç»æ˜¯ [seq_len, d_sae]ï¼Œä¸éœ€è¦å¤„ç†
                pass
            else:
                raise ValueError(f"æ„å¤–çš„feature_actsç»´åº¦: {feature_acts.shape}")
            
            # è·å–æŒ‡å®šfeatureçš„æ¿€æ´»å€¼
            # feature_acts shape: [seq_len, d_sae]
            if feature_acts.dim() == 2:
                feature_activation_values = feature_acts[:, feature_index].detach().cpu().numpy()
            else:
                feature_activation_values = feature_acts[feature_index].detach().cpu().unsqueeze(0).numpy()
            
            # æ„å»º64ä¸ªæ ¼å­çš„æ¿€æ´»å€¼æ•°ç»„
            seq_len = len(feature_activation_values)
            if seq_len == 64:
                activations_64 = feature_activation_values
            elif seq_len == 1:
                # å¦‚æœåªæœ‰1ä¸ªå€¼ï¼Œå¤åˆ¶åˆ°æ‰€æœ‰64ä¸ªä½ç½®
                activations_64 = np.full(64, feature_activation_values[0])
            else:
                # å¦‚æœé•¿åº¦ä¸æ˜¯64ï¼Œå¡«å……æˆ–æˆªæ–­åˆ°64
                activations_64 = np.zeros(64)
                min_len = min(seq_len, 64)
                activations_64[:min_len] = feature_activation_values[:min_len]
            
            # æ„å»ºç¨€ç–æ ¼å¼
            non_zero_mask = activations_64 != 0
            feature_acts_indices = np.where(non_zero_mask)[0].tolist()
            feature_acts_values = activations_64[non_zero_mask].tolist()
            
            return {
                "feature_acts_indices": feature_acts_indices,
                "feature_acts_values": feature_acts_values,
                "z_pattern_indices": None,
                "z_pattern_values": None,
            }
            
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ†æFENæ—¶å‡ºé”™: {str(e)}")


@app.post("/activation/get_features_at_position")
def get_features_at_position(request: dict):
    """
    è·å–æŒ‡å®šå±‚å’Œä½ç½®æ¿€æ´»çš„æ‰€æœ‰ features
    
    Args:
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - fen: FEN å­—ç¬¦ä¸²
            - layer: å±‚å·ï¼ˆ0-14ï¼‰
            - pos: ä½ç½®ç´¢å¼•ï¼ˆ0-63ï¼‰
            - component_type: ç»„ä»¶ç±»å‹ï¼Œ"attn" æˆ– "mlp"
            - model_name: å¯é€‰ï¼Œæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "lc0/BT4-1024x15x32h"
            - sae_combo_id: å¯é€‰ï¼ŒSAEç»„åˆIDï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç»„åˆ
    
    Returns:
        å­—å…¸ï¼ŒåŒ…å«ï¼š
        - "attn_features": å¦‚æœæ˜¯ attnï¼Œè¿”å›æ¿€æ´»çš„ Lorsa featuresï¼ˆåˆ—è¡¨ï¼‰
        - "mlp_features": å¦‚æœæ˜¯ mlpï¼Œè¿”å›æ¿€æ´»çš„ Transcoder featuresï¼ˆåˆ—è¡¨ï¼‰
        æ¯ä¸ª feature åŒ…å«ï¼š
        - "feature_index": feature ç´¢å¼•
        - "activation_value": æ¿€æ´»å€¼
    """
    try:
        if not HOOKED_TRANSFORMER_AVAILABLE:
            raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        fen = request.get("fen")
        layer = request.get("layer")
        pos = request.get("pos")
        component_type = request.get("component_type")
        model_name = request.get("model_name", "lc0/BT4-1024x15x32h")
        sae_combo_id = request.get("sae_combo_id")
        
        if not fen:
            raise HTTPException(status_code=400, detail="FENå­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
        if layer is None:
            raise HTTPException(status_code=400, detail="å±‚å·ä¸èƒ½ä¸ºç©º")
        if pos is None:
            raise HTTPException(status_code=400, detail="ä½ç½®ç´¢å¼•ä¸èƒ½ä¸ºç©º")
        if not component_type:
            raise HTTPException(status_code=400, detail="component_typeä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ˜¯'attn'æˆ–'mlp'")
        
        if component_type not in ["attn", "mlp"]:
            raise HTTPException(status_code=400, detail="component_typeå¿…é¡»æ˜¯'attn'æˆ–'mlp'")
        
        # è·å–æ¨¡å‹
        model = get_hooked_model(model_name)
        
        # è·å– transcoders å’Œ lorsas
        cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(model_name, sae_combo_id)
        
        if cached_transcoders is None or cached_lorsas is None:
            raise HTTPException(
                status_code=503,
                detail="Transcoders/LorsasæœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ /circuit/preload_models é¢„åŠ è½½"
            )
        
        if not ACTIVATION_MODULE_AVAILABLE or get_activated_features_at_position is None:
            raise HTTPException(
                status_code=503,
                detail="activationæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è·å–æ¿€æ´»features"
            )
        
        # è°ƒç”¨å‡½æ•°è·å–æ¿€æ´»çš„ features
        result = get_activated_features_at_position(
            model=model,
            transcoders=cached_transcoders,
            lorsas=cached_lorsas,
            fen=fen,
            layer=layer,
            pos=pos,
            component_type=component_type
        )
        
        return result
        
    except HTTPException:
        raise
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è·å–æ¿€æ´»featureså¤±è´¥: {str(e)}")


@app.post("/dictionaries/{name}/features/{feature_index}/analyze_fen_all_positions")
def analyze_fen_all_positions(name: str, feature_index: int, request: dict):
    fen = request.get("fen")
    if not fen:
        raise HTTPException(status_code=400, detail="FENå­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
    
    try:
        if not HOOKED_TRANSFORMER_AVAILABLE:
            raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        # ä»SAEåç§°ä¸­æå–å±‚å·å’Œç»„åˆä¿¡æ¯
        import re
        layer_match = re.search(r'L(\d+)', name)
        if not layer_match:
            raise HTTPException(status_code=400, detail=f"æ— æ³•ä»SAEåç§° {name} ä¸­æå–å±‚å·")
        layer = int(layer_match.group(1))
        
        # åˆ¤æ–­æ˜¯lorsaè¿˜æ˜¯transcoder
        is_lorsa_name = 'lorsa' in name.lower()
        is_tc_name = 'tc' in name.lower() or 'transcoder' in name.lower()
        
        # ä»SAEåç§°ä¸­æå–ç»„åˆä¿¡æ¯
        combo_id = None
        combo_match = re.search(r'k(\d+)_e(\d+)', name)
        if combo_match:
            k_val = combo_match.group(1)
            e_val = combo_match.group(2)
            combo_id = f"k_{k_val}_e_{e_val}"
        else:
            # å°è¯•é€šè¿‡åŒ¹é…SAEåç§°æ¨¡æ¿æ¥ç¡®å®šç»„åˆ
            for test_combo_id, test_combo_cfg in BT4_SAE_COMBOS.items():
                if is_lorsa_name:
                    template = test_combo_cfg.get("lorsa_sae_name_template", "")
                else:
                    template = test_combo_cfg.get("tc_sae_name_template", "")
                
                if template:
                    template_with_layer = template.format(layer=layer)
                    if template_with_layer in name or name.startswith(template_with_layer.split('{')[0]):
                        combo_id = test_combo_id
                        break
            
            if combo_id is None:
                combo_id = BT4_DEFAULT_SAE_COMBO
        
        # è·å–ç»„åˆé…ç½®
        combo_cfg = get_bt4_sae_combo(combo_id)
        
        # è·å–æ¨¡å‹
        model_name = "lc0/BT4-1024x15x32h"
        model = get_hooked_model(model_name)
        
        # æ ¹æ®ç»„åˆé…ç½®åŠ è½½SAEï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        if is_lorsa_name:
            lorsa_base_path = combo_cfg["lorsa_base_path"]
            lorsa_path = f"{lorsa_base_path}/L{layer}"
            
            if not os.path.exists(lorsa_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Lorsa not found at {lorsa_path} for layer {layer}"
                )
            
            # ä½¿ç”¨ç¼“å­˜åŠ è½½SAE
            sae = get_cached_sae(lorsa_path, is_lorsa=True, device=device)
        elif is_tc_name:
            tc_base_path = combo_cfg["tc_base_path"]
            tc_path = f"{tc_base_path}/L{layer}"
            
            if not os.path.exists(tc_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Transcoder not found at {tc_path} for layer {layer}"
                )
            
            # ä½¿ç”¨ç¼“å­˜åŠ è½½SAE
            sae = get_cached_sae(tc_path, is_lorsa=False, device=device)
        else:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ³•ç¡®å®šSAEç±»å‹ï¼Œåç§°åº”åŒ…å«'lorsa'æˆ–'tc'/'transcoder'"
            )
        
        # ä¸€æ¬¡æ€§è¿è¡Œæ¨¡å‹è·å–æ‰€æœ‰hookç‚¹çš„æ¿€æ´»å€¼ï¼ˆåªè¿è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼‰
        with torch.no_grad():
            if is_lorsa_name:
                hook_name = f"blocks.{layer}.hook_attn_in"
            else:
                hook_name = f"blocks.{layer}.resid_mid_after_ln"
            
            # è¿è¡Œæ¨¡å‹ï¼Œè·å–æ‰€æœ‰hookç‚¹çš„cache
            _, cache = model.run_with_cache(fen, prepend_bos=False)
            
            if hook_name not in cache:
                available_hooks = [k for k in cache.keys() if f"blocks.{layer}" in str(k)]
                raise HTTPException(
                    status_code=500,
                    detail=f"æ— æ³•æ‰¾åˆ°å±‚ {layer} çš„æ¿€æ´»å€¼ã€‚SAEç±»å‹: {'Lorsa' if is_lorsa_name else 'Transcoder'}ã€‚æœŸæœ›çš„hookç‚¹: {hook_name}ã€‚å¯ç”¨çš„hookç‚¹: {available_hooks[:10]}"
                )
            
            activations = cache[hook_name]  # shape: [batch, seq_len, d_model]ï¼Œé€šå¸¸æ˜¯ [1, seq_len, d_model]
            
            # ç¡®ä¿activationsæœ‰æ­£ç¡®çš„ç»´åº¦
            if activations.dim() == 1:
                activations = activations.unsqueeze(0).unsqueeze(0)  # [1, 1, d_model]
            elif activations.dim() == 2:
                activations = activations.unsqueeze(0)  # [1, seq_len, d_model]
            
            seq_len = activations.shape[1] if activations.dim() >= 2 else activations.shape[0]
            print(f"ğŸ” åˆ†ææ‰€æœ‰ä½ç½®: FEN={fen}, Layer={layer}, Feature={feature_index}, SeqLen={seq_len}")
        
        # ä½¿ç”¨SAEç¼–ç ï¼ˆä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰ä½ç½®ï¼‰
        sae_type_str = str(type(sae))
        is_lorsa = is_lorsa_name or 'LowRankSparseAttention' in sae_type_str
        
        if is_lorsa:
            # Lorsaç¼–ç ï¼Œè·å–featureæ¿€æ´»å€¼
            feature_acts = sae.encode(
                activations,  # [1, seq_len, d_model]
                return_hidden_pre=False,
                return_attention_pattern=False
            )
            
            # ç§»é™¤batchç»´åº¦
            if feature_acts.dim() == 3:
                feature_acts = feature_acts[0]  # [seq_len, d_sae]
            
            # ä½¿ç”¨ encode_z_pattern_for_head è®¡ç®—æŒ‡å®šfeatureçš„z_pattern
            z_pattern_2d = None
            try:
                # ç¡®ä¿ activations åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if activations.device != sae.cfg.device:
                    activations = activations.to(sae.cfg.device)
                
                # ä½¿ç”¨ encode_z_pattern_for_head è®¡ç®—è¯¥featureçš„z_pattern
                head_idx = torch.tensor([feature_index], device=activations.device)
                z_pattern = sae.encode_z_pattern_for_head(activations, head_idx)
                # z_pattern shape: [n_active_features, q_pos, k_pos]ï¼Œè¿™é‡Œæ˜¯ [1, seq_len, seq_len]
                z_pattern_2d = z_pattern[0]  # [seq_len, seq_len]
            except Exception as e:
                print(f"   - è®¡ç®—z_patternæ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            
            # æå–æ‰€æœ‰ä½ç½®çš„æ¿€æ´»æ•°æ®
            positions_data = []
            for pos in range(min(seq_len, 64)):  # æœ€å¤š64ä¸ªä½ç½®
                # è·å–è¯¥ä½ç½®çš„featureæ¿€æ´»å€¼
                if feature_acts.dim() == 2:
                    pos_activations = feature_acts[pos, feature_index].detach().cpu().item()
                else:
                    pos_activations = feature_acts[feature_index].detach().cpu().item()
                
                # æ„å»º64ä¸ªæ ¼å­çš„æ¿€æ´»å€¼æ•°ç»„ï¼ˆå½“å‰åªæœ‰è¿™ä¸ªä½ç½®æœ‰æ¿€æ´»å€¼ï¼‰
                activations_64 = np.zeros(64)
                if pos < 64:
                    activations_64[pos] = pos_activations
                
                # æå–è¯¥ä½ç½®çš„z_patternï¼ˆä½¿ç”¨ encode_z_pattern_for_head çš„ç»“æœï¼‰
                z_pattern_indices = None
                z_pattern_values = None
                if z_pattern_2d is not None:
                    # z_pattern_2d[query_pos, key_pos] è¡¨ç¤ºä»query_posåˆ°key_posçš„z_patternå€¼
                    # å¯¹äºä½ç½®posä½œä¸ºqueryï¼Œæˆ‘ä»¬æå–ä»posåˆ°æ‰€æœ‰keyä½ç½®çš„z_pattern
                    query_pos = pos
                    if query_pos < z_pattern_2d.shape[0]:
                        # è·å–ä»å½“å‰queryä½ç½®åˆ°æ‰€æœ‰keyä½ç½®çš„z_pattern
                        key_z_patterns = z_pattern_2d[query_pos, :].detach().cpu().numpy()  # [seq_len]
                        
                        # æ‰¾å‡ºéé›¶çš„z_patternå€¼ï¼ˆè¿‡æ»¤å¾ˆå°çš„å€¼ï¼‰
                        nonzero_mask = np.abs(key_z_patterns) > 1e-6
                        nonzero_indices = np.where(nonzero_mask)[0]
                        if len(nonzero_indices) > 0:
                            # æ ¼å¼ï¼š[query_pos, key_pos] è¡¨ç¤ºä»query_posåˆ°key_posçš„z_pattern
                            z_pattern_indices = [[int(query_pos), int(k_pos)] for k_pos in nonzero_indices if k_pos < 64]
                            z_pattern_values = [float(key_z_patterns[k_pos]) for k_pos in nonzero_indices if k_pos < 64]
                
                positions_data.append({
                    "position": pos,
                    "activations": activations_64.tolist(),
                    "z_pattern_indices": z_pattern_indices,
                    "z_pattern_values": z_pattern_values,
                })
            
            # å¦‚æœseq_len < 64ï¼Œå¡«å……å‰©ä½™ä½ç½®ä¸º0
            for pos in range(seq_len, 64):
                positions_data.append({
                    "position": pos,
                    "activations": [0.0] * 64,
                    "z_pattern_indices": None,
                    "z_pattern_values": None,
                })
            
            return {
                "positions": positions_data,
                "total_positions": len(positions_data),
                "feature_index": feature_index,
                "layer": layer,
                "sae_type": "Lorsa" if is_lorsa else "Transcoder"
            }
        else:
            # Transcoderç¼–ç 
            encode_result = sae.encode(activations)
            feature_acts = encode_result  # [1, seq_len, d_sae]
            
            # ç§»é™¤batchç»´åº¦
            if feature_acts.dim() == 3:
                feature_acts = feature_acts[0]  # [seq_len, d_sae]
            
            # æå–æ‰€æœ‰ä½ç½®çš„æ¿€æ´»æ•°æ®
            positions_data = []
            for pos in range(min(seq_len, 64)):
                # è·å–è¯¥ä½ç½®çš„featureæ¿€æ´»å€¼
                if feature_acts.dim() == 2:
                    pos_activations = feature_acts[pos, feature_index].detach().cpu().item()
                else:
                    pos_activations = feature_acts[feature_index].detach().cpu().item()
                
                # æ„å»º64ä¸ªæ ¼å­çš„æ¿€æ´»å€¼æ•°ç»„
                activations_64 = np.zeros(64)
                if pos < 64:
                    activations_64[pos] = pos_activations
                
                positions_data.append({
                    "position": pos,
                    "activations": activations_64.tolist(),
                    "z_pattern_indices": None,  # Transcoderæ²¡æœ‰z_pattern
                    "z_pattern_values": None,
                })
            
            # å¡«å……å‰©ä½™ä½ç½®
            for pos in range(seq_len, 64):
                positions_data.append({
                    "position": pos,
                    "activations": [0.0] * 64,
                    "z_pattern_indices": None,
                    "z_pattern_values": None,
                })
            
            return {
                "positions": positions_data,
                "total_positions": len(positions_data),
                "feature_index": feature_index,
                "layer": layer,
                "sae_type": "Transcoder"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ†æFENæ‰€æœ‰ä½ç½®æ—¶å‡ºé”™: {str(e)}")


@app.post("/dictionaries/{name}/features/{feature_index}/bookmark")
def add_bookmark(name: str, feature_index: int):
    """Add a bookmark for a feature.

    Args:
        name: Name of the dictionary/SAE
        feature_index: Index of the feature to bookmark

    Returns:
        Success response or error
    """
    try:
        success = client.add_bookmark(sae_name=name, sae_series=sae_series, feature_index=feature_index)
        if success:
            return {"message": "Bookmark added successfully"}
        else:
            return Response(content="Feature is already bookmarked", status_code=409)
    except ValueError as e:
        return Response(content=str(e), status_code=404)


@app.delete("/dictionaries/{name}/features/{feature_index}/bookmark")
def remove_bookmark(name: str, feature_index: int):
    """Remove a bookmark for a feature.

    Args:
        name: Name of the dictionary/SAE
        feature_index: Index of the feature to remove bookmark from

    Returns:
        Success response or error
    """
    success = client.remove_bookmark(sae_name=name, sae_series=sae_series, feature_index=feature_index)
    if success:
        return {"message": "Bookmark removed successfully"}
    else:
        return Response(content="Bookmark not found", status_code=404)


@app.get("/dictionaries/{name}/features/{feature_index}/bookmark")
def check_bookmark(name: str, feature_index: int):
    """Check if a feature is bookmarked.

    Args:
        name: Name of the dictionary/SAE
        feature_index: Index of the feature

    Returns:
        Bookmark status
    """
    is_bookmarked = client.is_bookmarked(sae_name=name, sae_series=sae_series, feature_index=feature_index)
    return {"is_bookmarked": is_bookmarked}


@app.get("/bookmarks")
def list_bookmarks(sae_name: Optional[str] = None, sae_series: Optional[str] = None, limit: int = 100, skip: int = 0):
    """List bookmarks with optional filtering.

    Args:
        sae_name: Optional SAE name filter
        sae_series: Optional SAE series filter
        limit: Maximum number of bookmarks to return
        skip: Number of bookmarks to skip (for pagination)

    Returns:
        List of bookmarks
    """
    bookmarks = client.list_bookmarks(sae_name=sae_name, sae_series=sae_series, limit=limit, skip=skip)

    # Convert to dict for JSON serialization
    bookmark_data = []
    for bookmark in bookmarks:
        bookmark_dict = bookmark.model_dump()
        # Convert datetime to ISO string for JSON
        bookmark_dict["created_at"] = bookmark.created_at.isoformat()
        bookmark_data.append(bookmark_dict)

    return {
        "bookmarks": bookmark_data,
        "total_count": client.get_bookmark_count(sae_name=sae_name, sae_series=sae_series),
    }


@app.post("/circuit/sync_clerps_to_interpretations")
def sync_clerps_to_interpretations(request: dict):

    try:
        nodes = request.get("nodes", [])
        lorsa_analysis_name = request.get("lorsa_analysis_name")
        tc_analysis_name = request.get("tc_analysis_name")
        
        if not isinstance(nodes, list):
            raise HTTPException(status_code=400, detail="nodes must be a list")
        
        # æ ¹æ®analysis_nameæ‰¾åˆ°å¯¹åº”çš„ç»„åˆé…ç½®
        combo_cfg = None
        if lorsa_analysis_name or tc_analysis_name:
            for combo_id, cfg in BT4_SAE_COMBOS.items():
                if (lorsa_analysis_name and cfg.get("lorsa_analysis_name") == lorsa_analysis_name) or \
                   (tc_analysis_name and cfg.get("tc_analysis_name") == tc_analysis_name):
                    combo_cfg = cfg
                    break
        
        print(f"ğŸ”„ å¼€å§‹åŒæ­¥clerpsåˆ°interpretations:")
        print(f"   - èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
        print(f"   - Lorsa analysis_name: {lorsa_analysis_name}")
        print(f"   - TC analysis_name: {tc_analysis_name}")
        if combo_cfg:
            print(f"   - æ‰¾åˆ°ç»„åˆé…ç½®: {combo_cfg.get('id')}")
            print(f"   - Lorsaæ¨¡æ¿: {combo_cfg.get('lorsa_sae_name_template')}")
            print(f"   - TCæ¨¡æ¿: {combo_cfg.get('tc_sae_name_template')}")
        
        synced_count = 0
        skipped_count = 0
        error_count = 0
        results = []
        
        for node in nodes:
            node_id = node.get('node_id')
            clerp = node.get('clerp')
            feature_idx = node.get('feature')
            layer = node.get('layer')
            feature_type = node.get('feature_type', '').lower()
            
            # è·³è¿‡æ²¡æœ‰clerpæˆ–clerpä¸ºç©ºçš„èŠ‚ç‚¹
            if not clerp or not isinstance(clerp, str) or clerp.strip() == '':
                skipped_count += 1
                continue
            
            # æ„å»ºSAEåç§°ï¼ˆä½¿ç”¨æ¨¡æ¿ï¼‰
            sae_name = None
            if 'lorsa' in feature_type:
                if combo_cfg and combo_cfg.get('lorsa_sae_name_template'):
                    # ä½¿ç”¨æ¨¡æ¿ï¼Œæ›¿æ¢{layer}ä¸ºå®é™…å±‚å·
                    sae_name = combo_cfg['lorsa_sae_name_template'].format(layer=layer)
                elif lorsa_analysis_name:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»„åˆé…ç½®ï¼Œå°è¯•ä½¿ç”¨æ—§çš„æ–¹å¼
                    sae_name = lorsa_analysis_name.replace("{}", str(layer))
                else:
                    sae_name = f"BT4_lorsa_L{layer}A"
            elif 'transcoder' in feature_type or 'cross layer transcoder' in feature_type:
                if combo_cfg and combo_cfg.get('tc_sae_name_template'):
                    # ä½¿ç”¨æ¨¡æ¿ï¼Œæ›¿æ¢{layer}ä¸ºå®é™…å±‚å·
                    sae_name = combo_cfg['tc_sae_name_template'].format(layer=layer)
                elif tc_analysis_name:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»„åˆé…ç½®ï¼Œå°è¯•ä½¿ç”¨æ—§çš„æ–¹å¼
                    sae_name = tc_analysis_name.replace("{}", str(layer))
                else:
                    sae_name = f"BT4_tc_L{layer}M"
            
            if not sae_name or feature_idx is None:
                skipped_count += 1
                continue
            
            try:
                # è§£ç clerpï¼ˆå¦‚æœæ˜¯URLç¼–ç çš„ï¼‰
                import urllib.parse
                decoded_clerp = urllib.parse.unquote(clerp)
                
                # åˆ›å»ºinterpretationå­—å…¸
                interpretation_dict = {
                    "text": decoded_clerp,
                    "method": "circuit_clerp",
                    "validation": []
                }
                
                # ä¿å­˜åˆ°MongoDB
                client.update_feature(
                    sae_name=sae_name,
                    sae_series=sae_series,
                    feature_index=feature_idx,
                    update_data={"interpretation": interpretation_dict}
                )
                
                synced_count += 1
                results.append({
                    "node_id": node_id,
                    "sae_name": sae_name,
                    "feature_index": feature_idx,
                    "status": "synced"
                })
                
                print(f"âœ… å·²åŒæ­¥èŠ‚ç‚¹ {node_id}: {sae_name}[{feature_idx}]")
                
            except Exception as e:
                error_count += 1
                results.append({
                    "node_id": node_id,
                    "sae_name": sae_name,
                    "feature_index": feature_idx,
                    "status": "error",
                    "error": str(e)
                })
                print(f"âŒ åŒæ­¥èŠ‚ç‚¹ {node_id} å¤±è´¥: {e}")
        
        summary = {
            "total_nodes": len(nodes),
            "synced": synced_count,
            "skipped": skipped_count,
            "errors": error_count,
            "results": results[:50]  # åªè¿”å›å‰50ä¸ªè¯¦ç»†ç»“æœ
        }
        
        print(f"âœ… åŒæ­¥å®Œæˆ: {synced_count} æˆåŠŸ, {skipped_count} è·³è¿‡, {error_count} å¤±è´¥")
        
        return summary
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")


@app.post("/circuit/sync_interpretations_to_clerps")
def sync_interpretations_to_clerps(request: dict):
    try:
        nodes = request.get("nodes", [])
        lorsa_analysis_name = request.get("lorsa_analysis_name")
        tc_analysis_name = request.get("tc_analysis_name")
        
        if not isinstance(nodes, list):
            raise HTTPException(status_code=400, detail="nodes must be a list")
        
        # æ ¹æ®analysis_nameæ‰¾åˆ°å¯¹åº”çš„ç»„åˆé…ç½®
        combo_cfg = None
        if lorsa_analysis_name or tc_analysis_name:
            for combo_id, cfg in BT4_SAE_COMBOS.items():
                if (lorsa_analysis_name and cfg.get("lorsa_analysis_name") == lorsa_analysis_name) or \
                   (tc_analysis_name and cfg.get("tc_analysis_name") == tc_analysis_name):
                    combo_cfg = cfg
                    break
        
        print(f"ğŸ”„ å¼€å§‹ä»interpretationsåŒæ­¥åˆ°clerps:")
        print(f"   - èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
        print(f"   - Lorsa analysis_name: {lorsa_analysis_name}")
        print(f"   - TC analysis_name: {tc_analysis_name}")
        if combo_cfg:
            print(f"   - æ‰¾åˆ°ç»„åˆé…ç½®: {combo_cfg.get('id')}")
            print(f"   - Lorsaæ¨¡æ¿: {combo_cfg.get('lorsa_sae_name_template')}")
            print(f"   - TCæ¨¡æ¿: {combo_cfg.get('tc_sae_name_template')}")
        
        updated_nodes = []
        found_count = 0
        not_found_count = 0
        
        for node in nodes:
            node_id = node.get('node_id')
            feature_idx = node.get('feature')
            layer = node.get('layer')
            feature_type = node.get('feature_type', '').lower()
            
            # æ„å»ºSAEåç§°ï¼ˆä½¿ç”¨æ¨¡æ¿ï¼‰
            sae_name = None
            if 'lorsa' in feature_type:
                if combo_cfg and combo_cfg.get('lorsa_sae_name_template'):
                    # ä½¿ç”¨æ¨¡æ¿ï¼Œæ›¿æ¢{layer}ä¸ºå®é™…å±‚å·
                    sae_name = combo_cfg['lorsa_sae_name_template'].format(layer=layer)
                elif lorsa_analysis_name:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»„åˆé…ç½®ï¼Œå°è¯•ä½¿ç”¨æ—§çš„æ–¹å¼
                    sae_name = lorsa_analysis_name.replace("{}", str(layer))
                else:
                    sae_name = f"BT4_lorsa_L{layer}A"
            elif 'transcoder' in feature_type or 'cross layer transcoder' in feature_type:
                if combo_cfg and combo_cfg.get('tc_sae_name_template'):
                    # ä½¿ç”¨æ¨¡æ¿ï¼Œæ›¿æ¢{layer}ä¸ºå®é™…å±‚å·
                    sae_name = combo_cfg['tc_sae_name_template'].format(layer=layer)
                elif tc_analysis_name:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»„åˆé…ç½®ï¼Œå°è¯•ä½¿ç”¨æ—§çš„æ–¹å¼
                    sae_name = tc_analysis_name.replace("{}", str(layer))
                else:
                    sae_name = f"BT4_tc_L{layer}M"
            
            updated_node = {**node}  # å¤åˆ¶åŸèŠ‚ç‚¹æ•°æ®
            
            if sae_name and feature_idx is not None:
                try:
                    # ä»MongoDBè¯»å–feature
                    feature = client.get_feature(
                        sae_name=sae_name,
                        sae_series=sae_series,
                        index=feature_idx
                    )
                    
                    if feature and feature.interpretation:
                        interp = feature.interpretation
                        if isinstance(interp, dict):
                            clerp_text = interp.get("text", "")
                        else:
                            clerp_text = getattr(interp, "text", "")
                        
                        if clerp_text:
                            updated_node["clerp"] = clerp_text
                            found_count += 1
                            print(f"âœ… æ‰¾åˆ°èŠ‚ç‚¹ {node_id} çš„interpretation: {sae_name}[{feature_idx}]")
                        else:
                            not_found_count += 1
                    else:
                        not_found_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ è¯»å–èŠ‚ç‚¹ {node_id} çš„interpretationå¤±è´¥: {e}")
                    not_found_count += 1
            else:
                not_found_count += 1
            
            updated_nodes.append(updated_node)
        
        summary = {
            "total_nodes": len(nodes),
            "found": found_count,
            "not_found": not_found_count,
            "updated_nodes": updated_nodes
        }
        
        print(f"âœ… åŒæ­¥å®Œæˆ: {found_count} æ‰¾åˆ°, {not_found_count} æœªæ‰¾åˆ°")
        
        return summary
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")


@app.post("/dictionaries/{name}/features/{feature_index}/interpret")
def interpret_feature(
    name: str,
    feature_index: int,
    type: str,
    custom_interpretation: Optional[str] = None
):
    """
    å¤„ç†ç‰¹å¾è§£é‡Šï¼šè‡ªåŠ¨ç”Ÿæˆã€è‡ªå®šä¹‰ä¿å­˜æˆ–éªŒè¯
    
    Args:
        name: SAEåç§°
        feature_index: ç‰¹å¾ç´¢å¼•
        type: è§£é‡Šç±»å‹ (auto/custom/validate)
        custom_interpretation: è‡ªå®šä¹‰è§£é‡Šæ–‡æœ¬ï¼ˆtype=customæ—¶éœ€è¦ï¼‰
    
    Returns:
        Interpretationå¯¹è±¡ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    """
    try:
        # è·å–ç‰¹å¾
        feature = client.get_feature(
            sae_name=name,
            sae_series=sae_series,
            index=feature_index
        )
        
        if feature is None:
            raise HTTPException(
                status_code=404,
                detail=f"Feature {feature_index} not found in SAE {name}"
            )
        
        if type == "custom":
            # ä¿å­˜è‡ªå®šä¹‰è§£é‡Š
            if not custom_interpretation:
                raise HTTPException(
                    status_code=400,
                    detail="custom_interpretation is required for type=custom"
                )
            
            # FastAPIåº”è¯¥å·²ç»è‡ªåŠ¨è§£ç äº†URLç¼–ç çš„å‚æ•°
            # å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ urllib.parse.unquote è§£ç 
            import urllib.parse
            decoded_interpretation = urllib.parse.unquote(custom_interpretation)
            
            print(f"ğŸ“ æ”¶åˆ°è§£é‡Šæ–‡æœ¬:")
            print(f"   - åŸå§‹: {custom_interpretation}")
            print(f"   - è§£ç : {decoded_interpretation}")
            
            # åˆ›å»ºè§£é‡Šå­—å…¸ï¼ˆåªåŒ…å«å¿…éœ€å­—æ®µï¼Œå…¶ä»–å­—æ®µä¸è¿”å›ä»¥ç¬¦åˆå‰ç«¯schemaçš„optionalå®šä¹‰ï¼‰
            interpretation_dict = {
                "text": decoded_interpretation,
                "method": "custom",
                "validation": []
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            try:
                client.update_feature(
                    sae_name=name,
                    sae_series=sae_series,
                    feature_index=feature_index,
                    update_data={"interpretation": interpretation_dict}
                )
            except Exception as update_error:
                print(f"Failed to update feature interpretation: {update_error}")
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to save interpretation: {str(update_error)}"
                )
            
            return interpretation_dict
        
        elif type == "auto":
            raise HTTPException(
                status_code=501,
                detail="Automatic interpretation is not yet implemented. Please use custom interpretation."
            )
        
        elif type == "validate":
            if not feature.interpretation:
                raise HTTPException(
                    status_code=400,
                    detail="No interpretation available to validate"
                )
            
            interp = feature.interpretation
            print(f"ğŸ“– è¯»å–è§£é‡Šæ–‡æœ¬: {interp.get('text', '') if isinstance(interp, dict) else getattr(interp, 'text', '')}")
            
            if isinstance(interp, dict):
                result = {
                    "text": interp.get("text", ""),
                    "method": interp.get("method", "unknown"),
                    "validation": interp.get("validation", [])
                }
                if interp.get("passed") is not None:
                    result["passed"] = interp.get("passed")
                if interp.get("complexity") is not None:
                    result["complexity"] = interp.get("complexity")
                if interp.get("consistency") is not None:
                    result["consistency"] = interp.get("consistency")
                return result
            else:
                # å¦‚æœæ˜¯å¯¹è±¡ï¼Œå°è¯•è®¿é—®å±æ€§
                result = {
                    "text": getattr(interp, "text", ""),
                    "method": getattr(interp, "method", "unknown"),
                    "validation": getattr(interp, "validation", [])
                }
                # åªæœ‰å½“å€¼ä¸æ˜¯Noneæ—¶æ‰æ·»åŠ å¯é€‰å­—æ®µ
                passed = getattr(interp, "passed", None)
                if passed is not None:
                    result["passed"] = passed
                complexity = getattr(interp, "complexity", None)
                if complexity is not None:
                    result["complexity"] = complexity
                consistency = getattr(interp, "consistency", None)
                if consistency is not None:
                    result["consistency"] = consistency
                return result
        
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid type: {type}. Must be 'auto', 'custom', or 'validate'"
            )
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process interpretation: {str(e)}"
        )


@app.put("/dictionaries/{name}/features/{feature_index}/bookmark")
def update_bookmark(name: str, feature_index: int, tags: Optional[list[str]] = None, notes: Optional[str] = None):
    """Update a bookmark with new tags or notes.

    Args:
        name: Name of the dictionary/SAE
        feature_index: Index of the feature
        tags: Optional new tags for the bookmark
        notes: Optional new notes for the bookmark

    Returns:
        Success response or error
    """
    success = client.update_bookmark(
        sae_name=name, sae_series=sae_series, feature_index=feature_index, tags=tags, notes=notes
    )
    if success:
        return {"message": "Bookmark updated successfully"}
    else:
        return Response(content="Bookmark not found", status_code=404)


# LC0 å¼•æ“ç±»
class LC0Engine:
    def __init__(self, model):
        self.model = model
        self.model.eval()

    def play(self, chess_board):
        try:
            # ä½¿ç”¨ notebook åŒæ¬¾æ¥å£è¿›è¡Œæ¨ç†
            fen = chess_board.fen()
            print(f"ğŸ” å¤„ç†FEN: {fen}")

            # åˆ›å»º LeelaBoard å®ä¾‹æ¥å¤„ç†æ˜ å°„
            lboard = LeelaBoard.from_fen(fen, history_synthesis=True)
            lboard.pc_board = chess_board  # ä½¿ç”¨ç°æœ‰çš„æ£‹ç›˜çŠ¶æ€

            with torch.no_grad():
                output, cache = self.model.run_with_cache(fen, prepend_bos=False)
                if isinstance(output, (list, tuple)) and len(output) >= 1:
                    policy_output = output[0]
                else:
                    policy_output = output
                if policy_output.dim() == 2:
                    policy_logits = policy_output[0]
                else:
                    policy_logits = policy_output

            legal_moves = list(chess_board.legal_moves)
            legal_uci_set = set(move.uci() for move in legal_moves)
            sorted_indices = torch.argsort(policy_logits, descending=True)

            top10 = []
            for idx in sorted_indices[:10].tolist():
                uci = lboard.idx2uci(idx)
                logit = float(policy_logits[idx].item())
                top10.append((uci, logit))
            
            print("ğŸ” æ¨¡å‹è¾“å‡ºè°ƒè¯•ä¿¡æ¯:")
            print(f"   - policy_logits shape: {tuple(policy_logits.shape)}")
            print(f"   - åˆæ³•ç§»åŠ¨æ•°é‡: {len(legal_moves)}")
            print("   - å‰10ä¸ªæœ€é«˜æ¦‚ç‡move (uci, logit):")
            print("     " + ", ".join([f"{uci}:{logit:.4f}" for uci, logit in top10]))

            # ä¾æ¬¡å°è¯•æœ€é«˜æ¦‚ç‡ç´¢å¼•å¯¹åº”çš„ UCIï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªåˆæ³•ç§»åŠ¨
            for rank, idx in enumerate(sorted_indices.tolist(), start=1):
                uci = lboard.idx2uci(idx)
                if uci in legal_uci_set:
                    move = chess.Move.from_uci(uci)
                    print(f"âœ… é€‰æ‹©æœ€å¤§æ¦‚ç‡åˆæ³•ç§»åŠ¨: {uci} (æ¦‚ç‡æ’å: {rank}, logit: {policy_logits[idx].item():.4f})")
                    return move

            # å¦‚æœæœªæ‰¾åˆ°åˆæ³•ç§»åŠ¨ï¼Œæ‰“å°æŠ¥é”™å¹¶æŠ›å¼‚å¸¸
            print("âŒ é”™è¯¯ï¼šæ¨¡å‹æœªèƒ½æ‰¾åˆ°ä»»ä½•åˆæ³•ç§»åŠ¨ï¼")
            print(f"   - å½“å‰å±€é¢ FEN: {fen}")
            print(f"   - ç¤ºä¾‹åˆæ³•ç§»åŠ¨: {[m.uci() for m in legal_moves[:10]]}")
            print(f"   - å°è¯•äº†å‰ {min(len(sorted_indices), 50)} ä¸ªæœ€é«˜æ¦‚ç‡çš„token")
            raise ValueError("æ¨¡å‹æœªèƒ½æ‰¾åˆ°ä»»ä½•åˆæ³•ç§»åŠ¨")

        except Exception as e:
            print(f"âŒ LC0Engine.play() å‡ºé”™: {e}")
            raise e


@app.post("/play_game")
def play_game(request: dict):
    """
    ä¸æ¨¡å‹å¯¹æˆ˜ï¼šè¾“å…¥å½“å‰å±€é¢ FENï¼Œè¿”å›æ¨¡å‹å»ºè®®çš„ä¸‹ä¸€æ­¥ç§»åŠ¨ (UCI æ ¼å¼)
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ç›´æ¥ä½¿ç”¨ç¥ç»ç½‘ç»œç­–ç•¥è¾“å‡ºï¼ˆuse_search=Falseï¼Œé»˜è®¤ï¼‰
    2. ä½¿ç”¨ MCTS æœç´¢ï¼ˆuse_search=Trueï¼‰
    
    Args:
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - fen: FEN å­—ç¬¦ä¸²ï¼ˆå¿…éœ€ï¼‰
            - use_search: æ˜¯å¦ä½¿ç”¨ MCTS æœç´¢ï¼ˆå¯é€‰ï¼Œé»˜è®¤ Falseï¼‰
            - search_params: æœç´¢å‚æ•°ï¼ˆå¯é€‰ï¼Œuse_search=True æ—¶æœ‰æ•ˆï¼‰
                - max_playouts: æœ€å¤§æ¨¡æ‹Ÿæ¬¡æ•°ï¼ˆé»˜è®¤ 100ï¼‰
                - target_minibatch_size: minibatch å¤§å°ï¼ˆé»˜è®¤ 8ï¼‰
                - cpuct: UCT æ¢ç´¢ç³»æ•°ï¼ˆé»˜è®¤ 1.0ï¼‰
                - max_depth: æœ€å¤§æœç´¢æ·±åº¦ï¼ˆé»˜è®¤ 10ï¼‰
    """
    fen = request.get("fen")
    use_search = request.get("use_search", False)
    search_params = request.get("search_params", {})
    # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
    model_name = "lc0/BT4-1024x15x32h"
    
    save_trace = bool(request.get("save_trace", False))
    trace_output_dir = request.get("trace_output_dir") or str(SEARCH_TRACE_OUTPUT_DIR)
    # trace_max_edges: 0 æˆ– None è¡¨ç¤ºä¸é™åˆ¶ï¼ˆä¿å­˜å®Œæ•´æœç´¢æ ‘ï¼‰ï¼Œå…¶ä»–å€¼è¡¨ç¤ºæœ€å¤§è¾¹æ•°
    trace_max_edges_raw = request.get("trace_max_edges", 1000)
    trace_max_edges = None if (trace_max_edges_raw == 0 or trace_max_edges_raw is None) else int(trace_max_edges_raw)

    if not fen:
        raise HTTPException(status_code=400, detail="FEN å­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
    
    try:
        board = chess.Board(fen)
    except Exception as e:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ FEN å­—ç¬¦ä¸²")
    
    try:
        # æ£€æŸ¥HookedTransformeræ˜¯å¦å¯ç”¨
        if not HOOKED_TRANSFORMER_AVAILABLE:
            print("âŒ é”™è¯¯ï¼šHookedTransformerä¸å¯ç”¨")
            raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        if use_search:
            # ä½¿ç”¨ MCTS æœç´¢
            print(f"ğŸ” ä½¿ç”¨ MCTS æœç´¢æ¨¡å¼: {fen[:50]}...")
            
            # å¯¼å…¥æœç´¢æ¨¡å—
            try:
                from search.model_interface import run_mcts_search, set_model_getter
                # è®¾ç½®æ¨¡å‹è·å–å™¨ä»¥å¤ç”¨ç¼“å­˜
                set_model_getter(get_hooked_model)
            except ImportError as e:
                print(f"âŒ å¯¼å…¥æœç´¢æ¨¡å—å¤±è´¥: {e}")
                raise HTTPException(status_code=503, detail="MCTS æœç´¢æ¨¡å—ä¸å¯ç”¨")
            
            # è§£ææœç´¢å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            max_playouts = search_params.get("max_playouts", 100)
            target_minibatch_size = search_params.get("target_minibatch_size", 8)
            cpuct = search_params.get("cpuct", 1.0)
            max_depth = search_params.get("max_depth", 10)
            
            print(f"   æœç´¢å‚æ•°: max_playouts={max_playouts}, cpuct={cpuct}, max_depth={max_depth}")
            
            # è¿è¡Œæœç´¢
            search_result = run_mcts_search(
                fen=fen,
                max_playouts=max_playouts,
                target_minibatch_size=target_minibatch_size,
                cpuct=cpuct,
                max_depth=max_depth,
                model_name=model_name,
            )
            
            best_move = search_result.get("best_move")
            if not best_move:
                raise ValueError("MCTS æœç´¢æœªèƒ½æ‰¾åˆ°åˆæ³•ç§»åŠ¨")
            
            print(f"âœ… MCTS æœç´¢å®Œæˆ: {best_move}, playouts={search_result.get('total_playouts')}")
            
            return {
                "move": best_move,
                "model_used": model_name,
                "search_used": True,
                "search_stats": {
                    "total_playouts": search_result.get("total_playouts"),
                    "max_depth_reached": search_result.get("max_depth_reached"),
                    "root_visits": search_result.get("root_visits"),
                    "top_moves": search_result.get("top_moves", [])[:5],  # åªè¿”å›å‰5ä¸ª
                }
            }
        else:
            # ç›´æ¥ä½¿ç”¨ç¥ç»ç½‘ç»œç­–ç•¥è¾“å‡º
            model = get_hooked_model(model_name)
            engine = LC0Engine(model)
            move = engine.play(board)
            return {"move": move.uci(), "model_used": model_name, "search_used": False}
        
    except ValueError as e:
        print(f"âŒ æ¨¡å‹æ‰¾ä¸åˆ°åˆæ³•ç§»åŠ¨: {e}")
        raise HTTPException(status_code=400, detail=f"æ¨¡å‹æ‰¾ä¸åˆ°åˆæ³•ç§»åŠ¨: {str(e)}")
    except Exception as e:
        print(f"âŒ å¤„ç†ç§»åŠ¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¤„ç†ç§»åŠ¨æ—¶å‡ºé”™: {str(e)}")


@app.post("/play_game_with_search")
def play_game_with_search(request: dict):
    """
    ä¸æ¨¡å‹å¯¹æˆ˜ï¼ˆä½¿ç”¨ MCTS æœç´¢ï¼‰ï¼šè¾“å…¥å½“å‰å±€é¢ FEN å’Œæœç´¢å‚æ•°ï¼Œè¿”å›æ¨¡å‹å»ºè®®çš„ä¸‹ä¸€æ­¥ç§»åŠ¨ (UCI æ ¼å¼)
    
    è¯·æ±‚å‚æ•°:
        - fen: FEN å­—ç¬¦ä¸²
        - max_playouts: æœ€å¤§æ¨¡æ‹Ÿæ¬¡æ•°ï¼ˆé»˜è®¤ 100ï¼‰
        - target_minibatch_size: ç›®æ ‡ minibatch å¤§å°ï¼ˆé»˜è®¤ 8ï¼‰
        - cpuct: UCT æ¢ç´¢ç³»æ•°ï¼ˆé»˜è®¤ 1.0ï¼‰
        - max_depth: æœ€å¤§æœç´¢æ·±åº¦ï¼ˆé»˜è®¤ 10ï¼Œ0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰
        - low_q_exploration_enabled: æ˜¯å¦å¯ç”¨ä½Qå€¼æ¢ç´¢å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
        - low_q_threshold: Qå€¼é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ˜¯"ä½Qå€¼"ï¼ˆé»˜è®¤ 0.3ï¼‰
        - low_q_exploration_bonus: æ¢ç´¢å¥–åŠ±çš„åŸºç¡€å€¼ï¼ˆé»˜è®¤ 0.1ï¼‰
        - low_q_visit_threshold: è®¿é—®æ¬¡æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ˜¯"æœªå……åˆ†æ¢ç´¢"ï¼ˆé»˜è®¤ 5ï¼‰
    """
    fen = request.get("fen")
    # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
    model_name = "lc0/BT4-1024x15x32h"
    
    # æœç´¢å‚æ•°ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
    max_playouts = request.get("max_playouts", 100)
    target_minibatch_size = request.get("target_minibatch_size", 8)
    cpuct = request.get("cpuct", 1.0)
    max_depth = request.get("max_depth", 10)
    
    # ä½Qå€¼æ¢ç´¢å¢å¼ºå‚æ•°ï¼ˆç”¨äºå‘ç°å¼ƒåè¿æ€ç­‰éšè—èµ°æ³•ï¼‰
    low_q_exploration_enabled = request.get("low_q_exploration_enabled", False)
    low_q_threshold = request.get("low_q_threshold", 0.3)
    low_q_exploration_bonus = request.get("low_q_exploration_bonus", 0.1)
    low_q_visit_threshold = request.get("low_q_visit_threshold", 5)
    
    save_trace = bool(request.get("save_trace", False))
    trace_slug = request.get("trace_slug")
    trace_output_dir = request.get("trace_output_dir") or str(SEARCH_TRACE_OUTPUT_DIR)
    # trace_max_edges: 0 æˆ– None è¡¨ç¤ºä¸é™åˆ¶ï¼ˆä¿å­˜å®Œæ•´æœç´¢æ ‘ï¼‰ï¼Œå…¶ä»–å€¼è¡¨ç¤ºæœ€å¤§è¾¹æ•°
    trace_max_edges_raw = request.get("trace_max_edges", 1000)
    trace_max_edges = None if (trace_max_edges_raw == 0 or trace_max_edges_raw is None) else int(trace_max_edges_raw)
    
    if not fen:
        raise HTTPException(status_code=400, detail="FEN å­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
    
    try:
        board = chess.Board(fen)
    except Exception as e:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ FEN å­—ç¬¦ä¸²")
    
    try:
        # æ£€æŸ¥HookedTransformeræ˜¯å¦å¯ç”¨
        if not HOOKED_TRANSFORMER_AVAILABLE:
            print("âŒ é”™è¯¯ï¼šHookedTransformerä¸å¯ç”¨")
            raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        # å¯¼å…¥æœç´¢æ¨¡å—
        from search import (
            SearchParams, Search, SimpleBackend, Node, SearchTracer,
            get_wl, get_d, get_m, get_policy,
            policy_tensor_to_move_dict, set_model_getter,
        )
        
        # è®¾ç½®æ¨¡å‹è·å–å‡½æ•°ï¼Œä½¿ç”¨å…±äº«ç¼“å­˜
        set_model_getter(get_hooked_model)
        
        # åˆ›å»ºæ¨¡å‹è¯„ä¼°å‡½æ•°
        def model_eval_fn(fen_str: str) -> dict:
            """æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼Œè¿”å› q, d, m, p"""
            wl = get_wl(fen_str, model_name)
            d = get_d(fen_str, model_name)
            m_tensor = get_m(fen_str, model_name)
            m_value = m_tensor.item() if hasattr(m_tensor, 'item') else float(m_tensor)
            
            # è·å–ç­–ç•¥
            policy_tensor = get_policy(fen_str, model_name)
            policy_dict = policy_tensor_to_move_dict(policy_tensor, fen_str)
            
            return {
                'q': wl,
                'd': d,
                'm': m_value,
                'p': policy_dict
            }
        
        # åˆ›å»ºæœç´¢å‚æ•°
        params = SearchParams(
            max_playouts=max_playouts,
            target_minibatch_size=target_minibatch_size,
            cpuct=cpuct,
            max_depth=max_depth,
            low_q_exploration_enabled=low_q_exploration_enabled,
            low_q_threshold=low_q_threshold,
            low_q_exploration_bonus=low_q_exploration_bonus,
            low_q_visit_threshold=low_q_visit_threshold,
        )
        
        # åˆ›å»ºåç«¯å’Œæ ¹èŠ‚ç‚¹
        backend = SimpleBackend(model_eval_fn)
        root_node = Node(fen=fen)
        
        tracer = SearchTracer() if save_trace else None
        # åˆ›å»ºæœç´¢å¯¹è±¡å¹¶è¿è¡Œ
        search = Search(
            root_node=root_node,
            backend=backend,
            params=params,
            tracer=tracer,
        )
        
        print(f"ğŸ” å¼€å§‹ MCTS æœç´¢: max_playouts={max_playouts}, max_depth={max_depth}")
        search.run_blocking()
        
        # è·å–æœ€ä½³ç§»åŠ¨
        best_move = search.get_best_move()
        total_playouts = search.get_total_playouts()
        current_max_depth = search.get_current_max_depth()
        
        if best_move is None:
            raise ValueError("æœç´¢æœªèƒ½æ‰¾åˆ°åˆæ³•ç§»åŠ¨")
        
        print(f"âœ… MCTS æœç´¢å®Œæˆ: playouts={total_playouts}, depth={current_max_depth}, best_move={best_move.uci()}")
        
        trace_file_path = None
        if save_trace and tracer:
            trace_file_path = search.export_trace_json(
                output_dir=trace_output_dir,
                max_edges=trace_max_edges,
            )

        response_data = {
            "move": best_move.uci(),
            "model_used": model_name,
            "search_info": {
                "total_playouts": total_playouts,
                "max_depth_reached": current_max_depth,
                "max_depth_limit": max_depth,
            }
        }
        if trace_file_path:
            response_data["trace_file_path"] = trace_file_path
            response_data["trace_filename"] = Path(trace_file_path).name

        return response_data
        
    except ValueError as e:
        print(f"âŒ æœç´¢æ‰¾ä¸åˆ°åˆæ³•ç§»åŠ¨: {e}")
        raise HTTPException(status_code=400, detail=f"æœç´¢æ‰¾ä¸åˆ°åˆæ³•ç§»åŠ¨: {str(e)}")
    except Exception as e:
        print(f"âŒ æœç´¢å¤„ç†æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤„ç†æ—¶å‡ºé”™: {str(e)}")


@app.get("/search_trace/files/{filename}")
def download_search_trace_file(filename: str):
    """ä¸‹è½½ä¿å­˜çš„MCTSæœç´¢traceæ–‡ä»¶"""
    safe_name = os.path.basename(filename)
    if safe_name != filename:
        raise HTTPException(status_code=400, detail="Invalid filename")
    target_path = SEARCH_TRACE_OUTPUT_DIR / safe_name
    if not target_path.exists() or not target_path.is_file():
        raise HTTPException(status_code=404, detail="Trace file not found")
    return FileResponse(
        path=target_path,
        media_type="application/json",
        filename=safe_name,
    )


# åœ¨play_gameæ¥å£åæ·»åŠ å±€é¢åˆ†ææ¥å£
@app.post("/analyze/board")
def analyze_board(request: dict):
    """ä½¿ç”¨HookedTransformeræ¨¡å‹åˆ†æå½“å‰å±€é¢ï¼Œå¹¶è¿”å›è¡Œæ£‹æ–¹èƒœç‡ã€å’Œæ£‹ç‡åŠå¯¹æ–¹èƒœç‡"""
    fen = request.get("fen")
    # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
    model_name = "lc0/BT4-1024x15x32h"
    
    if not fen:
        raise HTTPException(status_code=400, detail="FENå­—ç¬¦ä¸²ä¸èƒ½ä¸ºç©º")
    try:
        if not HOOKED_TRANSFORMER_AVAILABLE:
            raise HTTPException(status_code=503, detail="HookedTransformerä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformer_lens")
        
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        model = get_hooked_model(model_name)
        
        with torch.no_grad():
            output, _ = model.run_with_cache(fen, prepend_bos=False)
        
        # æ¨¡å‹è¾“å‡ºæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«ä¸‰ä¸ªå…ƒç´ ï¼š
        # output[0]: logits, shape [1, 1858]
        # output[1]: WDL, shape [1, 3] - [å½“å‰è¡Œæ£‹æ–¹èƒœç‡, å’Œæ£‹ç‡, å½“å‰è¡Œæ£‹æ–¹è´¥ç‡]
        # output[2]: å…¶ä»–è¾“å‡º, shape [1, 1]
        
        if isinstance(output, (list, tuple)) and len(output) >= 2:
            wdl_tensor = output[1]  # è·å–WDLè¾“å‡º
            if wdl_tensor.shape == torch.Size([1, 3]):
                # WDLå·²ç»æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œä¸éœ€è¦softmax
                current_player_win = wdl_tensor[0][0].item()  # å½“å‰è¡Œæ£‹æ–¹èƒœç‡
                draw_prob = wdl_tensor[0][1].item()  # å’Œæ£‹ç‡
                current_player_loss = wdl_tensor[0][2].item()  # å½“å‰è¡Œæ£‹æ–¹è´¥ç‡
                
                # ç›´æ¥è¿”å›å½“å‰è¡Œæ£‹æ–¹çš„èƒœç‡ä¿¡æ¯ï¼Œä¸è¿›è¡Œç¿»è½¬
                # [å½“å‰è¡Œæ£‹æ–¹èƒœç‡, å’Œæ£‹ç‡, å¯¹æ–¹èƒœç‡]
                evaluation = [current_player_win, draw_prob, current_player_loss]
            else:
                print(f"WDLè¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®: {wdl_tensor.shape}, æœŸæœ› [1, 3]")
                evaluation = [0.5, 0.2, 0.3]
        else:
            print(f"æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›åŒ…å«è‡³å°‘2ä¸ªå…ƒç´ çš„åˆ—è¡¨ï¼Œå®é™…å¾—åˆ°: {type(output)}")
            evaluation = [0.5, 0.2, 0.3]
        
        return {"evaluation": evaluation, "model_used": model_name}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å±€é¢åˆ†æå‡ºé”™: {str(e)}")


@app.get("/models")
def get_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return {"models": get_available_models()}


# å¯¼å…¥circuits_service
try:
    from circuits_service import (
        run_circuit_trace, 
        check_dense_features, 
        load_model_and_transcoders,
        get_cached_models,
        set_cached_models,
        _global_hooked_models,
        _global_transcoders_cache,
        _global_lorsas_cache,
        _global_replacement_models_cache
    )
    from lm_saes.circuit.replacement_lc0_model import ReplacementModel
    CIRCUITS_SERVICE_AVAILABLE = True
    # å¦‚æœcircuits_serviceå¯ç”¨ï¼Œå°†æœ¬åœ°ç¼“å­˜æŒ‡å‘å…±äº«ç¼“å­˜
    _hooked_models = _global_hooked_models
    _transcoders_cache = _global_transcoders_cache
    _lorsas_cache = _global_lorsas_cache
    _replacement_models_cache = _global_replacement_models_cache
except ImportError as e:
    run_circuit_trace = None
    check_dense_features = None
    load_model_and_transcoders = None
    get_cached_models = None
    set_cached_models = None
    _global_hooked_models = {}
    _global_transcoders_cache = {}
    _global_lorsas_cache = {}
    _global_replacement_models_cache = {}
    ReplacementModel = None
    CIRCUITS_SERVICE_AVAILABLE = False
    print(f"WARNING: circuits_service not found, circuit tracing will not be available: {e}")

# å¯¼å…¥patchingæœåŠ¡
try:
    from patching import run_patching_analysis
    PATCHING_SERVICE_AVAILABLE = True
except ImportError:
    run_patching_analysis = None
    PATCHING_SERVICE_AVAILABLE = False
    print("WARNING: patching service not found, patching analysis will not be available")

# å¯¼å…¥interventionæœåŠ¡
try:
    from intervention import run_feature_steering_analysis, run_multi_feature_steering_analysis
    INTERVENTION_SERVICE_AVAILABLE = True
except ImportError:
    run_feature_steering_analysis = None
    run_multi_feature_steering_analysis = None
    INTERVENTION_SERVICE_AVAILABLE = False
    print("WARNING: intervention service not found, steering analysis will not be available")

# å¯¼å…¥interactionæœåŠ¡
try:
    from interaction import analyze_node_interaction_impl
    INTERACTION_SERVICE_AVAILABLE = True
except ImportError:
    analyze_node_interaction_impl = None
    INTERACTION_SERVICE_AVAILABLE = False
    print("WARNING: interaction service not found, node interaction analysis will not be available")

# å¯¼å…¥è‡ªå¯¹å¼ˆæœåŠ¡
try:
    from self_play import run_self_play, analyze_game_positions
    SELF_PLAY_SERVICE_AVAILABLE = True
except ImportError:
    run_self_play = None
    analyze_game_positions = None
    SELF_PLAY_SERVICE_AVAILABLE = False
    print("WARNING: self-play service not found, self-play functionality will not be available")

# å¯¼å…¥Logit LensæœåŠ¡
try:
    from logit_lens import IntegratedPolicyLens
    LOGIT_LENS_AVAILABLE = True
except ImportError:
    IntegratedPolicyLens = None
    LOGIT_LENS_AVAILABLE = False
    print("WARNING: logit_lens not found, logit lens functionality will not be available")

# å…¨å±€Logit Lensç¼“å­˜
_logit_lens_instances = {}

# Circuit tracingè¿›ç¨‹è·Ÿè¸ªï¼ˆé˜²æ­¢åŒæ—¶è¿è¡Œå¤šä¸ªtraceï¼‰
_circuit_tracing_lock = threading.Lock()
_is_circuit_tracing = False


@app.post("/circuit/preload_models")
def preload_circuit_models(request: dict):
    """
    é¢„åŠ è½½ transcoders å’Œ lorsas æ¨¡å‹ï¼Œä»¥ä¾¿åç»­çš„ circuit trace èƒ½å¤Ÿå¿«é€Ÿä½¿ç”¨ã€‚

    Args:
        request: åŒ…å«æ¨¡å‹ä¿¡æ¯çš„è¯·æ±‚ä½“
            - model_name: æ¨¡å‹åç§° (å¯é€‰ï¼Œé»˜è®¤: "lc0/BT4-1024x15x32h")
            - sae_combo_id: SAE ç»„åˆ IDï¼ˆä¾‹å¦‚ "k_64_e_32"ï¼Œå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åç«¯å½“å‰ç»„åˆï¼‰

    è¡Œä¸ºï¼š
        - å¦‚æœé€‰æ‹©äº†ä¸å½“å‰ä¸åŒçš„ç»„åˆï¼Œä¼šå…ˆæ¸…ç†ä¹‹å‰ç»„åˆçš„ SAE ç¼“å­˜å¹¶å°è¯•é‡Šæ”¾æ˜¾å­˜ï¼›
        - åŒä¸€ç»„åˆåœ¨å·²åŠ è½½ä¸”å®Œæ•´æ—¶ç›´æ¥è¿”å› already_loadedï¼›
        - åŠ è½½è¿‡ç¨‹ä¸­çš„è¿›åº¦æ—¥å¿—ä¼šå†™å…¥å…¨å±€ _loading_logsï¼Œå‰ç«¯å¯è½®è¯¢æŸ¥çœ‹ã€‚
    """

    global CURRENT_BT4_SAE_COMBO_ID, _loading_locks, _loading_status, _loading_logs, _cancel_loading
    global _transcoders_cache, _lorsas_cache, _replacement_models_cache, _global_loading_lock

    model_name = request.get("model_name", "lc0/BT4-1024x15x32h")
    
    # URLè§£ç ï¼Œå¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜ï¼ˆä¸ /circuit/loading_logs ä¿æŒä¸€è‡´ï¼‰
    import urllib.parse
    
    decoded_model_name = urllib.parse.unquote(model_name)
    if "%" in decoded_model_name:
        decoded_model_name = urllib.parse.unquote(decoded_model_name)
    
    requested_combo_id = request.get("sae_combo_id") or CURRENT_BT4_SAE_COMBO_ID

    # å½’ä¸€åŒ–ç»„åˆé…ç½®ï¼ˆå¦‚æœä¼ å…¥äº†æœªçŸ¥ IDï¼Œä¼šå›é€€åˆ°é»˜è®¤ç»„åˆï¼‰
    combo_cfg = get_bt4_sae_combo(requested_combo_id)
    combo_id = combo_cfg["id"]
    # ä½¿ç”¨è§£ç åçš„ model_name ç”Ÿæˆç¼“å­˜é”®
    combo_key = _make_combo_cache_key(decoded_model_name, combo_id)
    
    # å¦‚æœåˆ‡æ¢ç»„åˆï¼Œå…ˆä¸­æ–­å½“å‰æ­£åœ¨åŠ è½½çš„å…¶ä»–ç»„åˆ
    if combo_id != CURRENT_BT4_SAE_COMBO_ID:
        # ä¸­æ–­æ‰€æœ‰å…¶ä»–ç»„åˆçš„åŠ è½½
        for other_combo_key in list(_cancel_loading.keys()):
            if other_combo_key != combo_key:
                _cancel_loading[other_combo_key] = True
                print(f"ğŸ›‘ æ ‡è®°ä¸­æ–­åŠ è½½: {other_combo_key}")
                # å¦‚æœè¯¥ç»„åˆæ­£åœ¨åŠ è½½ï¼Œä¹Ÿåœ¨æ—¥å¿—ä¸­è®°å½•
                if other_combo_key in _loading_logs:
                    _loading_logs[other_combo_key].append({
                        "timestamp": time.time(),
                        "message": f"ğŸ›‘ åŠ è½½è¢«ä¸­æ–­ï¼ˆåˆ‡æ¢åˆ°æ–°ç»„åˆ {combo_id}ï¼‰",
                    })

    try:
        if not CIRCUITS_SERVICE_AVAILABLE or load_model_and_transcoders is None:
            raise HTTPException(status_code=503, detail="Circuit tracing service not available")

        # å¦‚æœåˆ‡æ¢ç»„åˆï¼Œåˆ™æ¸…ç©ºä¹‹å‰ç»„åˆçš„ SAE ç¼“å­˜å¹¶å°è¯•é‡Šæ”¾æ˜¾å­˜
        if combo_id != CURRENT_BT4_SAE_COMBO_ID:
            print(f"ğŸ” æ£‹ç±» SAE ç»„åˆåˆ‡æ¢: {CURRENT_BT4_SAE_COMBO_ID} -> {combo_id}ï¼Œå¼€å§‹æ¸…ç†æ—§ç¼“å­˜")

            # æ¸…ç©ºæ‰€æœ‰ SAE ç¼“å­˜ï¼ˆåŒ…æ‹¬ circuits_service çš„å…¨å±€ç¼“å­˜ï¼‰ï¼Œä»…ä¿ç•™ HookedTransformer æ¨¡å‹æœ¬èº«
            for cache_name, cache in [
                ("_transcoders_cache", _transcoders_cache),
                ("_lorsas_cache", _lorsas_cache),
                ("_replacement_models_cache", _replacement_models_cache),
            ]:
                try:
                    for cache_key, v in list(cache.items()):
                        # å°è¯•æŠŠ SAE æŒªåˆ° CPUï¼Œå†åˆ é™¤å¼•ç”¨
                        if isinstance(v, dict):
                            for sae in v.values():
                                try:
                                    if hasattr(sae, "to"):
                                        sae.to("cpu")
                                except Exception:
                                    continue
                        elif isinstance(v, list):
                            for sae in v:
                                try:
                                    if hasattr(sae, "to"):
                                        sae.to("cpu")
                                except Exception:
                                    continue
                        del cache[cache_key]
                    print(f"   - å·²æ¸…ç©ºç¼“å­˜ {cache_name}")
                except Exception as clear_err:
                    print(f"   âš ï¸ æ¸…ç†ç¼“å­˜ {cache_name} æ—¶å‡ºé”™: {clear_err}")
            
            # åŒæ—¶æ¸…ç† circuits_service çš„å…¨å±€ç¼“å­˜
            if CIRCUITS_SERVICE_AVAILABLE:
                try:
                    for cache_key in list(_global_transcoders_cache.keys()):
                        if cache_key != decoded_model_name:  # ä¿ç•™ HookedTransformer çš„ç¼“å­˜é”®ï¼ˆåªæœ‰ model_nameï¼‰
                            del _global_transcoders_cache[cache_key]
                    for cache_key in list(_global_lorsas_cache.keys()):
                        if cache_key != decoded_model_name:
                            del _global_lorsas_cache[cache_key]
                    for cache_key in list(_global_replacement_models_cache.keys()):
                        if cache_key != decoded_model_name:
                            del _global_replacement_models_cache[cache_key]
                    print("   - å·²æ¸…ç©º circuits_service å…¨å±€ç¼“å­˜")
                except Exception as clear_err:
                    print(f"   âš ï¸ æ¸…ç† circuits_service å…¨å±€ç¼“å­˜æ—¶å‡ºé”™: {clear_err}")

            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    print("   - å·²è°ƒç”¨ torch.cuda.empty_cache() é‡Šæ”¾æ˜¾å­˜")
            except Exception as e:
                print(f"   âš ï¸ è°ƒç”¨ empty_cache å¤±è´¥: {e}")

            # æ¸…ç†æ—§çš„patchingåˆ†æå™¨
            try:
                from intervention import clear_patching_analyzer
                clear_patching_analyzer(CURRENT_BT4_SAE_COMBO_ID)
                print("   - å·²æ¸…ç†æ—§çš„patchingåˆ†æå™¨")
            except (ImportError, Exception) as e:
                print(f"   âš ï¸ æ¸…ç†patchingåˆ†æå™¨å¤±è´¥: {e}")

            CURRENT_BT4_SAE_COMBO_ID = combo_id

        # ä¸ºå½“å‰ç»„åˆåˆ›å»º/è·å–åŠ è½½é”
        if combo_key not in _loading_locks:
            _loading_locks[combo_key] = threading.Lock()

        # æ£€æŸ¥æ˜¯å¦å·²ç»é¢„åŠ è½½
        cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(decoded_model_name, combo_id)
        if cached_transcoders is not None and cached_lorsas is not None:
            if len(cached_transcoders) == 15 and len(cached_lorsas) == 15:
                print(f"âœ… Transcoders å’Œ Lorsas å·²ç»é¢„åŠ è½½: {decoded_model_name} @ {combo_id}")
                return {
                    "status": "already_loaded",
                    "message": f"æ¨¡å‹ {decoded_model_name} ç»„åˆ {combo_id} çš„ transcoders å’Œ lorsas å·²ç»é¢„åŠ è½½",
                    "model_name": decoded_model_name,
                    "sae_combo_id": combo_id,
                    "n_layers": len(cached_lorsas),
                    "transcoders_count": len(cached_transcoders),
                    "lorsas_count": len(cached_lorsas),
                }

        # ä½¿ç”¨å…¨å±€é”ç¡®ä¿åŒä¸€æ—¶é—´åªåŠ è½½ä¸€ä¸ªé…ç½®ï¼ˆé¿å…GPUå†…å­˜åŒæ—¶è¢«å¤šä¸ªé…ç½®å ç”¨ï¼‰
        # ç„¶åå†ä½¿ç”¨ç»„åˆé”é¿å…åŒä¸€ç»„åˆçš„å¹¶å‘åŠ è½½
        with _global_loading_lock:
            with _loading_locks[combo_key]:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½ï¼ˆå¯èƒ½åœ¨ç­‰å¾…é”çš„è¿‡ç¨‹ä¸­å·²ç»åŠ è½½å®Œæˆï¼‰
                cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(decoded_model_name, combo_id)
                if cached_transcoders is not None and cached_lorsas is not None:
                    if len(cached_transcoders) == 15 and len(cached_lorsas) == 15:
                        print(f"âœ… Transcoders å’Œ Lorsas å·²ç»é¢„åŠ è½½ï¼ˆåœ¨é”å†…æ£€æŸ¥ï¼‰: {decoded_model_name} @ {combo_id}")
                        return {
                            "status": "already_loaded",
                            "message": f"æ¨¡å‹ {decoded_model_name} ç»„åˆ {combo_id} çš„ transcoders å’Œ lorsas å·²ç»é¢„åŠ è½½",
                            "model_name": decoded_model_name,
                            "sae_combo_id": combo_id,
                            "n_layers": len(cached_lorsas),
                            "transcoders_count": len(cached_transcoders),
                            "lorsas_count": len(cached_lorsas),
                        }

                # æ ‡è®°æ­£åœ¨åŠ è½½ï¼Œå¹¶æ¸…é™¤ä¸­æ–­æ ‡å¿—ï¼ˆåœ¨å…¨å±€é”å†…è®¾ç½®ï¼Œç¡®ä¿å…¶ä»–è¯·æ±‚èƒ½æ£€æµ‹åˆ°ï¼‰
                _loading_status[combo_key] = {"is_loading": True}
                _cancel_loading[combo_key] = False
                print(f"ğŸ” å¼€å§‹é¢„åŠ è½½ transcoders å’Œ lorsas: {decoded_model_name} @ {combo_id} (å…¨å±€é”å·²è·å–)")

                try:
                    # è·å– HookedTransformer æ¨¡å‹
                    hooked_model = get_hooked_model(decoded_model_name)

                    # ä»…æ”¯æŒ BT4
                    if "BT4" not in decoded_model_name:
                        raise HTTPException(status_code=400, detail="Unsupported Model!")

                    tc_base_path = combo_cfg["tc_base_path"]
                    lorsa_base_path = combo_cfg["lorsa_base_path"]
                    n_layers = 15

                    # åˆå§‹åŒ–åŠ è½½æ—¥å¿—
                    if combo_key not in _loading_logs:
                        _loading_logs[combo_key] = []
                    loading_logs = _loading_logs[combo_key]
                    loading_logs.clear()
                    # æ·»åŠ åˆå§‹æ—¥å¿—
                    loading_logs.append({
                        "timestamp": time.time(),
                        "message": f"ğŸ” å¼€å§‹é¢„åŠ è½½ transcoders å’Œ lorsas: {decoded_model_name} @ {combo_id}",
                    })
                    print(f"ğŸ“ åˆå§‹åŒ–åŠ è½½æ—¥å¿—åˆ—è¡¨: combo_key={combo_key}, åˆ—è¡¨ID={id(loading_logs)}")

                    # åŠ è½½ transcoders å’Œ lorsas
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                    # åˆ›å»ºå–æ¶ˆæ ‡å¿—å­—å…¸ï¼ˆé€šè¿‡å¼•ç”¨ä¼ é€’ï¼Œå¯ä»¥åœ¨å¾ªç¯ä¸­æ£€æŸ¥ï¼‰
                    # ä½¿ç”¨ä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥å®šæœŸæ£€æŸ¥å–æ¶ˆæ ‡å¿—
                    def check_cancel():
                        return _cancel_loading.get(combo_key, False)
                    
                    cancel_flag = {"should_cancel": False, "combo_key": combo_key, "check_fn": check_cancel}
                    replacement_model, transcoders, lorsas = load_model_and_transcoders(
                        model_name=decoded_model_name,
                        device=device,
                        tc_base_path=tc_base_path,
                        lorsa_base_path=lorsa_base_path,
                        n_layers=n_layers,
                        hooked_model=hooked_model,
                        loading_logs=loading_logs,
                        cancel_flag=cancel_flag,
                        cache_key=combo_key,  # ä¼ é€’ cache_key ä»¥åŒºåˆ†ä¸åŒç»„åˆ
                    )

                    print(f"ğŸ“ åŠ è½½å®Œæˆåçš„æ—¥å¿—æ•°é‡: {len(loading_logs)}")

                    # ç¼“å­˜ transcoders å’Œ lorsasï¼ˆåŒæ—¶æ›´æ–°å…±äº«ç¼“å­˜å’Œæœ¬åœ°ç¼“å­˜ï¼‰
                    _transcoders_cache[combo_key] = transcoders
                    _lorsas_cache[combo_key] = lorsas
                    _replacement_models_cache[combo_key] = replacement_model

                    # å¦‚æœ circuits_service å¯ç”¨ï¼Œä¹Ÿæ›´æ–°å…±äº«ç¼“å­˜ï¼ˆä½¿ç”¨ combo_key ä½œä¸ºç¼“å­˜é”®ï¼‰
                    if CIRCUITS_SERVICE_AVAILABLE and set_cached_models is not None:
                        set_cached_models(combo_key, hooked_model, transcoders, lorsas, replacement_model)

                    print(f"âœ… é¢„åŠ è½½å®Œæˆ: {model_name} @ {combo_id}")
                    print(f"   - Transcoders: {len(transcoders)} å±‚")
                    print(f"   - Lorsas: {len(lorsas)} å±‚")

                    # æ·»åŠ å®Œæˆæ—¥å¿—
                    if combo_key in _loading_logs:
                        _loading_logs[combo_key].append(
                            {
                                "timestamp": time.time(),
                                "message": f"âœ… é¢„åŠ è½½å®Œæˆ: {model_name} @ {combo_id}",
                            }
                        )
                        _loading_logs[combo_key].append(
                            {
                                "timestamp": time.time(),
                                "message": f"   - Transcoders: {len(transcoders)} å±‚",
                            }
                        )
                        _loading_logs[combo_key].append(
                            {
                                "timestamp": time.time(),
                                "message": f"   - Lorsas: {len(lorsas)} å±‚",
                            }
                        )

                    _loading_status[combo_key] = {"is_loading": False}

                    return {
                        "status": "loaded",
                        "message": f"æˆåŠŸé¢„åŠ è½½æ¨¡å‹ {decoded_model_name} ç»„åˆ {combo_id} çš„ transcoders å’Œ lorsas",
                        "model_name": decoded_model_name,
                        "sae_combo_id": combo_id,
                        "n_layers": n_layers,
                        "transcoders_count": len(transcoders),
                        "lorsas_count": len(lorsas),
                        "device": device,
                    }
                except InterruptedError as e:
                    # åŠ è½½è¢«ä¸­æ–­ï¼Œæ¸…ç©ºå·²åŠ è½½çš„éƒ¨åˆ†ç¼“å­˜
                    _loading_status[combo_key] = {"is_loading": False}
                    _cancel_loading[combo_key] = False
                    # æ¸…ç©ºè¯¥ç»„åˆçš„ç¼“å­˜
                    if combo_key in _transcoders_cache:
                        del _transcoders_cache[combo_key]
                    if combo_key in _lorsas_cache:
                        del _lorsas_cache[combo_key]
                    if combo_key in _replacement_models_cache:
                        del _replacement_models_cache[combo_key]
                    if combo_key in _loading_logs:
                        _loading_logs[combo_key].append({
                            "timestamp": time.time(),
                            "message": f"ğŸ›‘ åŠ è½½å·²ä¸­æ–­å¹¶æ¸…ç©ºç¼“å­˜: {str(e)}",
                        })
                    print(f"ğŸ›‘ åŠ è½½è¢«ä¸­æ–­ï¼Œå·²æ¸…ç©ºç¼“å­˜: {combo_key}")
                    raise HTTPException(status_code=499, detail=f"åŠ è½½è¢«ä¸­æ–­: {str(e)}")
                except Exception:
                    _loading_status[combo_key] = {"is_loading": False}
                    raise

    except HTTPException:
        raise
    except Exception as e:
        import traceback

        traceback.print_exc()
        if combo_key in _loading_logs:
            _loading_logs[combo_key].append(
                {
                    "timestamp": time.time(),
                    "message": f"âŒ é¢„åŠ è½½å¤±è´¥: {str(e)}",
                }
            )
        if combo_key in _loading_status:
            _loading_status[combo_key] = {"is_loading": False}
        raise HTTPException(status_code=500, detail=f"é¢„åŠ è½½å¤±è´¥: {str(e)}")


@app.post("/circuit/cancel_loading")
def cancel_loading(request: dict):
    """
    ä¸­æ–­æ­£åœ¨è¿›è¡Œçš„æ¨¡å‹åŠ è½½
    
    Args:
        request: åŒ…å«æ¨¡å‹ä¿¡æ¯çš„è¯·æ±‚ä½“
            - model_name: æ¨¡å‹åç§° (å¯é€‰ï¼Œé»˜è®¤: "lc0/BT4-1024x15x32h")
            - sae_combo_id: SAE ç»„åˆ IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä¸­æ–­æ‰€æœ‰æ­£åœ¨åŠ è½½çš„ç»„åˆï¼‰
    
    Returns:
        ä¸­æ–­ç»“æœ
    """
    global _cancel_loading, _loading_status, _loading_logs
    global _transcoders_cache, _lorsas_cache, _replacement_models_cache
    
    model_name = request.get("model_name", "lc0/BT4-1024x15x32h")
    requested_combo_id = request.get("sae_combo_id")
    
    if requested_combo_id:
        # ä¸­æ–­æŒ‡å®šçš„ç»„åˆ
        combo_cfg = get_bt4_sae_combo(requested_combo_id)
        combo_id = combo_cfg["id"]
        combo_key = _make_combo_cache_key(model_name, combo_id)
        
        if combo_key in _loading_status and _loading_status[combo_key].get("is_loading", False):
            _cancel_loading[combo_key] = True
            print(f"ğŸ›‘ æ ‡è®°ä¸­æ–­åŠ è½½: {combo_key}")
            return {
                "status": "cancelled",
                "message": f"å·²æ ‡è®°ä¸­æ–­ç»„åˆ {combo_id} çš„åŠ è½½",
                "model_name": model_name,
                "sae_combo_id": combo_id,
            }
        else:
            return {
                "status": "not_loading",
                "message": f"ç»„åˆ {combo_id} å½“å‰æ²¡æœ‰æ­£åœ¨åŠ è½½",
                "model_name": model_name,
                "sae_combo_id": combo_id,
            }
    else:
        # ä¸­æ–­æ‰€æœ‰æ­£åœ¨åŠ è½½çš„ç»„åˆ
        cancelled_keys = []
        for combo_key, status in _loading_status.items():
            if status.get("is_loading", False):
                _cancel_loading[combo_key] = True
                cancelled_keys.append(combo_key)
                print(f"ğŸ›‘ æ ‡è®°ä¸­æ–­åŠ è½½: {combo_key}")
        
        return {
            "status": "cancelled" if cancelled_keys else "no_loading",
            "message": f"å·²æ ‡è®°ä¸­æ–­ {len(cancelled_keys)} ä¸ªç»„åˆçš„åŠ è½½" if cancelled_keys else "å½“å‰æ²¡æœ‰æ­£åœ¨åŠ è½½çš„ç»„åˆ",
            "cancelled_keys": cancelled_keys,
        }


@app.get("/circuit/loading_logs")
def get_loading_logs(
    model_name: str = "lc0/BT4-1024x15x32h",
    sae_combo_id: str | None = None,
):
    """
    è·å–æ¨¡å‹åŠ è½½æ—¥å¿—
    
    Args:
        model_name: æ¨¡å‹åç§° (æŸ¥è¯¢å‚æ•°ï¼Œé»˜è®¤: "lc0/BT4-1024x15x32h")
        sae_combo_id: SAEç»„åˆID (æŸ¥è¯¢å‚æ•°ï¼Œå¯é€‰)
    
    Returns:
        åŠ è½½æ—¥å¿—åˆ—è¡¨
    """

    global _loading_logs, _loading_status

    # URLè§£ç ï¼Œå¤„ç†å¯èƒ½çš„åŒé‡ç¼–ç é—®é¢˜
    import urllib.parse

    decoded_model_name = urllib.parse.unquote(model_name)
    if "%" in decoded_model_name:
        decoded_model_name = urllib.parse.unquote(decoded_model_name)

    combo_id = sae_combo_id or CURRENT_BT4_SAE_COMBO_ID
    combo_cfg = get_bt4_sae_combo(combo_id)
    normalized_combo_id = combo_cfg["id"]
    combo_key = _make_combo_cache_key(decoded_model_name, normalized_combo_id)

    logs = _loading_logs.get(combo_key, [])
    is_loading = _loading_status.get(combo_key, {}).get("is_loading", False)
    
    # è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ“Š GET /circuit/loading_logs: combo_key={combo_key}, logs_count={len(logs)}, is_loading={is_loading}")

    return {
        "model_name": decoded_model_name,
        "sae_combo_id": normalized_combo_id,
        "logs": logs,
        "total_count": len(logs),
        "is_loading": is_loading,
    }



@app.post("/circuit_trace")
def circuit_trace(request: dict):
    """
    è¿è¡Œcircuit traceåˆ†æå¹¶è¿”å›graphæ•°æ®
    
    Args:
        request: åŒ…å«åˆ†æå‚æ•°çš„è¯·æ±‚ä½“
            - fen: FENå­—ç¬¦ä¸² (å¿…éœ€)
            - move_uci: è¦åˆ†æçš„UCIç§»åŠ¨ (å¿…éœ€)
            - side: åˆ†æä¾§ (q/k/both, é»˜è®¤: "k")
            - max_feature_nodes: æœ€å¤§ç‰¹å¾èŠ‚ç‚¹æ•° (é»˜è®¤: 4096)
            - node_threshold: èŠ‚ç‚¹é˜ˆå€¼ (é»˜è®¤: 0.73)
            - edge_threshold: è¾¹é˜ˆå€¼ (é»˜è®¤: 0.57)
            - max_n_logits: æœ€å¤§logitæ•°é‡ (é»˜è®¤: 1)
            - desired_logit_prob: æœŸæœ›logitæ¦‚ç‡ (é»˜è®¤: 0.95)
            - batch_size: æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1)
            - order_mode: æ’åºæ¨¡å¼ (positive/negative, é»˜è®¤: "positive")
            - encoder_demean: æ˜¯å¦å¯¹encoderè¿›è¡Œdemean (é»˜è®¤: False)
            - save_activation_info: æ˜¯å¦ä¿å­˜æ¿€æ´»ä¿¡æ¯ (é»˜è®¤: False)
    
    Returns:
        Graphæ•°æ® (JSONæ ¼å¼)
    """
    global _is_circuit_tracing
    
    try:
        # æ£€æŸ¥circuits_serviceæ˜¯å¦å¯ç”¨
        if not CIRCUITS_SERVICE_AVAILABLE:
            raise HTTPException(status_code=503, detail="Circuit tracing service not available")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„circuit tracingè¿›ç¨‹
        with _circuit_tracing_lock:
            if _is_circuit_tracing:
                raise HTTPException(status_code=409, detail="å¦ä¸€ä¸ªcircuit tracingè¿›ç¨‹æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†è¯•")
            _is_circuit_tracing = True
        
        try:
            # æå–å‚æ•°
            fen = request.get("fen")
            if not fen:
                raise HTTPException(status_code=400, detail="FEN string is required")
            
            # è§£ç FENä»¥ç¡®ä¿trace_keyçš„ä¸€è‡´æ€§
            fen = _decode_fen(fen)
            
            move_uci = request.get("move_uci")
            if move_uci:
                move_uci = _decode_fen(move_uci)  # move_uciä¹Ÿå¯èƒ½è¢«ç¼–ç 
            negative_move_uci = request.get("negative_move_uci", None)  # æ–°å¢negative_move_uciå‚æ•°
            if negative_move_uci:
                negative_move_uci = _decode_fen(negative_move_uci)  # negative_move_uciä¹Ÿå¯èƒ½è¢«ç¼–ç 
            
            side = request.get("side", "k")
            max_feature_nodes = request.get("max_feature_nodes", 4096)
            node_threshold = request.get("node_threshold", 0.73)
            edge_threshold = request.get("edge_threshold", 0.57)
            max_n_logits = request.get("max_n_logits", 1)
            desired_logit_prob = request.get("desired_logit_prob", 0.95)
            batch_size = request.get("batch_size", 1)
            order_mode = request.get("order_mode", "positive")
            encoder_demean = request.get("encoder_demean", False)
            save_activation_info = request.get("save_activation_info", True)  # é»˜è®¤å¯ç”¨æ¿€æ´»ä¿¡æ¯ä¿å­˜
            max_act_times = request.get("max_act_times", None)  # æ·»åŠ æœ€å¤§æ¿€æ´»æ¬¡æ•°å‚æ•°
            # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
            model_name = "lc0/BT4-1024x15x32h"
            
            print(f"ğŸ” Circuit Trace è¯·æ±‚å‚æ•°:")
            print(f"   - FEN: {fen}")
            print(f"   - Move UCI: {move_uci}")
            print(f"   - Negative Move UCI: {negative_move_uci}")
            print(f"   - Model Name: {model_name}")
            print(f"   - Side: {side}")
            print(f"   - Order Mode: {order_mode}")
            print(f"   - Max Act Times: {max_act_times}")
            
            # éªŒè¯ side å‚æ•°
            if side not in ["q", "k", "both"]:
                raise HTTPException(status_code=400, detail="side must be 'q', 'k', or 'both'")
            
            # éªŒè¯ order_mode å‚æ•°å’Œå¤„ç†bothæ¨¡å¼
            if order_mode == "both":
                # Bothæ¨¡å¼ï¼šéœ€è¦positive moveå’Œnegative move
                if not move_uci:
                    raise HTTPException(status_code=400, detail="move_uci (positive move) is required for 'both' mode")
                if not negative_move_uci:
                    raise HTTPException(status_code=400, detail="negative_move_uci is required for 'both' mode")
                # Bothæ¨¡å¼å¼ºåˆ¶sideä¸ºboth
                side = "both"
                # å°†order_modeè½¬æ¢ä¸ºmove_pairï¼Œä»¥ä¾¿åç«¯å¤„ç†
                order_mode = "move_pair"
            elif order_mode not in ["positive", "negative"]:
                raise HTTPException(status_code=400, detail="order_mode must be 'positive', 'negative', or 'both'")
            
            # éªŒè¯move_uci
            if not move_uci:
                raise HTTPException(status_code=400, detail="move_uci is required")
            
            # è·å–å·²ç¼“å­˜çš„HookedTransformeræ¨¡å‹
            hooked_model = get_hooked_model(model_name)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„transcoderså’Œlorsas
            cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(model_name)
            cached_replacement_model = _replacement_models_cache.get(model_name)
            
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åŠ è½½
            global _loading_status, _loading_locks
            is_loading = _loading_status.get(model_name, {}).get("is_loading", False)
            
            # å¦‚æœç¼“å­˜ä¸å®Œæ•´ä¸”æ­£åœ¨åŠ è½½ï¼Œç­‰å¾…åŠ è½½å®Œæˆ
            cache_complete = (cached_transcoders is not None and cached_lorsas is not None and 
                             cached_replacement_model is not None and
                             len(cached_transcoders) == 15 and len(cached_lorsas) == 15)
            
            if not cache_complete and is_loading:
                print(f"â³ æ£€æµ‹åˆ°æ­£åœ¨åŠ è½½TC/Lorsaï¼Œç­‰å¾…åŠ è½½å®Œæˆï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰: {model_name}")
                # è·å–åŠ è½½é”ï¼ˆç­‰å¾…åŠ è½½å®Œæˆï¼‰
                if model_name not in _loading_locks:
                    _loading_locks[model_name] = threading.Lock()
                
                # ç­‰å¾…åŠ è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…10åˆ†é’Ÿï¼Œå› ä¸ºåŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
                max_wait_time = 600  # 10åˆ†é’Ÿ
                wait_start = time.time()
                wait_interval = 1  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                while (time.time() - wait_start) < max_wait_time:
                    is_loading = _loading_status.get(model_name, {}).get("is_loading", False)
                    # é‡æ–°æ£€æŸ¥ç¼“å­˜
                    cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(model_name)
                    cached_replacement_model = _replacement_models_cache.get(model_name)
                    cache_complete = (cached_transcoders is not None and cached_lorsas is not None and 
                                     cached_replacement_model is not None and
                                     len(cached_transcoders) == 15 and len(cached_lorsas) == 15)
                    if cache_complete:
                        print(f"âœ… ç­‰å¾…åŠ è½½å®Œæˆï¼Œå·²è·å–å®Œæ•´ç¼“å­˜: {model_name} (ç­‰å¾…æ—¶é—´: {time.time() - wait_start:.1f}ç§’)")
                        break
                    if not is_loading and not cache_complete:
                        # åŠ è½½å·²å®Œæˆä½†ç¼“å­˜ä¸å®Œæ•´ï¼Œå¯èƒ½æ˜¯åŠ è½½å¤±è´¥
                        print(f"âš ï¸ åŠ è½½å·²å®Œæˆä½†ç¼“å­˜ä¸å®Œæ•´ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ è½½: {model_name}")
                        break
                    time.sleep(wait_interval)
                    elapsed = time.time() - wait_start
                    if int(elapsed) % 10 == 0 and int(elapsed) > 0:  # æ¯10ç§’æ‰“å°ä¸€æ¬¡
                        print(f"â³ ä»åœ¨ç­‰å¾…åŠ è½½å®Œæˆ... (å·²ç­‰å¾… {elapsed:.1f}ç§’, TC: {len(cached_transcoders) if cached_transcoders else 0}, Lorsa: {len(cached_lorsas) if cached_lorsas else 0})")
                
                if not cache_complete:
                    elapsed = time.time() - wait_start
                    if elapsed >= max_wait_time:
                        print(f"âš ï¸ ç­‰å¾…åŠ è½½è¶…æ—¶ï¼ˆ{elapsed:.1f}ç§’ï¼‰ï¼Œä½†å°†ç»§ç»­ä½¿ç”¨å½“å‰ç¼“å­˜æˆ–æŠ¥é”™: {model_name}")
                    else:
                        print(f"âš ï¸ åŠ è½½å®Œæˆä½†ç¼“å­˜ä¸å®Œæ•´ï¼Œå°†ä½¿ç”¨å½“å‰ç¼“å­˜æˆ–æŠ¥é”™: {model_name}")
            
            # è·å–å½“å‰ä½¿ç”¨çš„SAEç»„åˆIDï¼ˆä»è¯·æ±‚ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å½“å‰å…¨å±€ç»„åˆï¼‰
            sae_combo_id = request.get("sae_combo_id") or CURRENT_BT4_SAE_COMBO_ID
            combo_cfg = get_bt4_sae_combo(sae_combo_id)
            normalized_combo_id = combo_cfg["id"]
            
            # æ ¹æ®ç»„åˆIDè®¾ç½®æ­£ç¡®çš„è·¯å¾„ï¼ˆå³ä½¿ä½¿ç”¨ç¼“å­˜ï¼Œä¹Ÿéœ€è¦è·¯å¾„ç”¨äºå…¼å®¹æ€§ï¼‰
            if 'BT4' in model_name:
                tc_base_path = combo_cfg["tc_base_path"]
                lorsa_base_path = combo_cfg["lorsa_base_path"]
            else:
                raise HTTPException(status_code=400, detail="Unsupported Model!")
            
            # ä½¿ç”¨ç»„åˆIDè·å–æ­£ç¡®çš„ç¼“å­˜ï¼ˆå› ä¸ºä¸åŒç»„åˆä½¿ç”¨ä¸åŒçš„ç¼“å­˜é”®ï¼‰
            combo_key = _make_combo_cache_key(model_name, normalized_combo_id)
            cached_transcoders = _transcoders_cache.get(combo_key)
            cached_lorsas = _lorsas_cache.get(combo_key)
            cached_replacement_model = _replacement_models_cache.get(combo_key)
            
            # é‡æ–°æ£€æŸ¥ç¼“å­˜å®Œæ•´æ€§
            cache_complete = (cached_transcoders is not None and cached_lorsas is not None and 
                             cached_replacement_model is not None and
                             len(cached_transcoders) == 15 and len(cached_lorsas) == 15)
            
            if cache_complete:
                # ä½¿ç”¨ç¼“å­˜çš„transcoderså’Œlorsasï¼Œä¸éœ€è¦é‡æ–°åŠ è½½
                print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„transcodersã€lorsaså’Œreplacement_model: {model_name} @ {normalized_combo_id}")
            else:
                # æ£€æŸ¥æ˜¯å¦ä»åœ¨åŠ è½½
                is_still_loading = _loading_status.get(combo_key, {}).get("is_loading", False)
                if is_still_loading:
                    # å¦‚æœä»åœ¨åŠ è½½ï¼Œç»§ç»­ç­‰å¾…
                    print(f"â³ ç¼“å­˜ä¸å®Œæ•´ä½†ä»åœ¨ä½¿ç”¨ä¸­åŠ è½½ï¼Œå°†ç»§ç»­ç­‰å¾…...")
                    raise HTTPException(status_code=503, detail=f"æ¨¡å‹ {model_name} ç»„åˆ {normalized_combo_id} æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•ã€‚å½“å‰è¿›åº¦: TC {len(cached_transcoders) if cached_transcoders else 0}/15, Lorsa {len(cached_lorsas) if cached_lorsas else 0}/15")
                elif cached_transcoders is None or cached_lorsas is None:
                    # å®Œå…¨æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦åŠ è½½
                    print(f"âš ï¸ æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå°†é‡æ–°åŠ è½½transcoderså’Œlorsas: {model_name} @ {normalized_combo_id}")
                    print("   æç¤ºï¼šå»ºè®®å…ˆè°ƒç”¨ /circuit/preload_models è¿›è¡Œé¢„åŠ è½½ä»¥åŠ é€Ÿ")
                else:
                    # æœ‰éƒ¨åˆ†ç¼“å­˜ä½†ä¸å®Œæ•´ï¼Œä¹Ÿé‡æ–°åŠ è½½ï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºåº”è¯¥ç­‰å¾…åŠ è½½å®Œæˆï¼‰
                    print(f"âš ï¸ ç¼“å­˜ä¸å®Œæ•´ï¼ˆTC: {len(cached_transcoders)}, Lorsa: {len(cached_lorsas)}ï¼‰ï¼Œå°†é‡æ–°åŠ è½½: {model_name} @ {normalized_combo_id}")
            
            # åˆ›å»ºtrace_keyç”¨äºæ—¥å¿—å­˜å‚¨ï¼ˆç¡®ä¿ä½¿ç”¨è§£ç åçš„FENå’Œmove_uciï¼‰
            # fenå’Œmove_uciå·²ç»åœ¨å‰é¢è¢«è§£ç äº†
            trace_key = f"{model_name}::{normalized_combo_id}::{fen}::{move_uci}"
            
            # åˆå§‹åŒ–æ—¥å¿—åˆ—è¡¨
            if trace_key not in _circuit_trace_logs:
                _circuit_trace_logs[trace_key] = []
            trace_logs = _circuit_trace_logs[trace_key]
            trace_logs.clear()  # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—
            
            # è®¾ç½®tracingçŠ¶æ€
            _circuit_trace_status[trace_key] = {"is_tracing": True}
            
            # æ·»åŠ åˆå§‹æ—¥å¿—
            trace_logs.append({
                "timestamp": time.time(),
                "message": f"ğŸ” å¼€å§‹Circuit Trace: FEN={fen}, Move={move_uci}, Side={side}, OrderMode={order_mode}"
            })
            
            try:
                # è¿è¡Œcircuit traceï¼Œä¼ é€’å·²ç¼“å­˜çš„æ¨¡å‹å’Œtranscoders/lorsasä»¥åŠæ—¥å¿—åˆ—è¡¨
                graph_data = run_circuit_trace(
                    prompt=fen,
                    move_uci=move_uci,
                    negative_move_uci=negative_move_uci,  # ä¼ é€’negative_move_uci
                    model_name=model_name,  # æ·»åŠ æ¨¡å‹åç§°å‚æ•°
                    tc_base_path=tc_base_path,  # ä¼ é€’æ­£ç¡®çš„TCè·¯å¾„
                    lorsa_base_path=lorsa_base_path,  # ä¼ é€’æ­£ç¡®çš„LORSAè·¯å¾„
                    side=side,
                    max_feature_nodes=max_feature_nodes,
                    node_threshold=node_threshold,
                    edge_threshold=edge_threshold,
                    max_n_logits=max_n_logits,
                    desired_logit_prob=desired_logit_prob,
                    batch_size=batch_size,
                    order_mode=order_mode,
                    encoder_demean=encoder_demean,
                    save_activation_info=save_activation_info,
                    act_times_max=max_act_times,  # ä¼ é€’æœ€å¤§æ¿€æ´»æ¬¡æ•°å‚æ•°
                    log_level="INFO",
                    hooked_model=hooked_model,  # ä¼ é€’å·²ç¼“å­˜çš„æ¨¡å‹
                    cached_transcoders=cached_transcoders,  # ä¼ é€’ç¼“å­˜çš„transcoders
                    cached_lorsas=cached_lorsas,  # ä¼ é€’ç¼“å­˜çš„lorsas
                    cached_replacement_model=cached_replacement_model,  # ä¼ é€’ç¼“å­˜çš„replacement_model
                    sae_combo_id=normalized_combo_id,  # ä¼ é€’å½’ä¸€åŒ–åçš„SAEç»„åˆIDï¼Œç”¨äºç”Ÿæˆæ­£ç¡®çš„analysis_nameæ¨¡æ¿
                    trace_logs=trace_logs  # ä¼ é€’æ—¥å¿—åˆ—è¡¨
                )
                
                # æ·»åŠ å®Œæˆæ—¥å¿—
                finished_ts = time.time()
                trace_logs.append({
                    "timestamp": finished_ts,
                    "message": "âœ… Circuit Traceå®Œæˆ!"
                })

                result_data = {
                    "graph_data": graph_data,
                    "finished_at": finished_ts,
                    "logs": list(trace_logs),
                }
                
                # ä¿å­˜åˆ°å†…å­˜
                _circuit_trace_results[trace_key] = result_data
                
                # æŒä¹…åŒ–åˆ°ç£ç›˜ï¼ˆç¡®ä¿å³ä½¿æœåŠ¡å™¨é‡å¯ä¹Ÿèƒ½æ¢å¤ï¼‰
                try:
                    _save_trace_result_to_disk(trace_key, result_data)
                except Exception as e:
                    print(f"âš ï¸ æŒä¹…åŒ–traceç»“æœå¤±è´¥ï¼ˆä½†ç»“æœå·²ä¿å­˜åœ¨å†…å­˜ä¸­ï¼‰: {e}")
                
            except Exception as trace_error:
                # å³ä½¿traceå¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿å­˜éƒ¨åˆ†ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                print(f"âš ï¸ Circuit traceè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {trace_error}")
                # å¦‚æœæœ‰éƒ¨åˆ†ç»“æœï¼Œå°è¯•ä¿å­˜
                if trace_logs:
                    try:
                        partial_result = {
                            "graph_data": None,
                            "finished_at": time.time(),
                            "logs": list(trace_logs),
                            "error": str(trace_error)
                        }
                        _circuit_trace_results[trace_key] = partial_result
                        _save_trace_result_to_disk(trace_key, partial_result)
                    except:
                        pass
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                raise
            finally:
                # æ›´æ–°tracingçŠ¶æ€
                _circuit_trace_status[trace_key] = {"is_tracing": False}
            
            return graph_data
        
        finally:
            # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è¦æ¸…é™¤æ ‡å¿—
            with _circuit_tracing_lock:
                _is_circuit_tracing = False
        
    except HTTPException:
        # HTTPExceptionéœ€è¦é‡æ–°æŠ›å‡ºï¼ˆæ ‡å¿—å·²åœ¨finallyä¸­æ¸…é™¤ï¼‰
        raise
    except Exception as e:
        # å…¶ä»–å¼‚å¸¸è½¬æ¢ä¸ºHTTPExceptionï¼ˆæ ‡å¿—å·²åœ¨finallyä¸­æ¸…é™¤ï¼‰
        raise HTTPException(status_code=500, detail=f"Circuit trace analysis failed: {str(e)}")


@app.get("/circuit_trace/status")
def circuit_trace_status():
    """æ£€æŸ¥circuit traceæœåŠ¡çš„çŠ¶æ€"""
    global _is_circuit_tracing
    return {
        "available": CIRCUITS_SERVICE_AVAILABLE,
        "hooked_transformer_available": HOOKED_TRANSFORMER_AVAILABLE,
        "is_tracing": _is_circuit_tracing
    }


@app.get("/circuit_trace/result")
def circuit_trace_result(
    model_name: str = "lc0/BT4-1024x15x32h",
    sae_combo_id: str | None = None,
    fen: str | None = None,
    move_uci: str | None = None,
):
    """
    è·å–æœ€è¿‘ä¸€æ¬¡å®Œæˆçš„circuit traceç»“æœ
    å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œä¼šå°è¯•ä»ç£ç›˜åŠ è½½
    """
    global _circuit_trace_results

    if fen and move_uci:
        # è§£ç FENå’Œmove_uciä»¥ç¡®ä¿trace_keyçš„ä¸€è‡´æ€§
        decoded_fen = _decode_fen(fen)
        decoded_move_uci = _decode_fen(move_uci)
        decoded_model_name = _decode_fen(model_name)
        
        combo_id = sae_combo_id or CURRENT_BT4_SAE_COMBO_ID
        combo_cfg = get_bt4_sae_combo(combo_id)
        normalized_combo_id = combo_cfg["id"]
        trace_key = f"{decoded_model_name}::{normalized_combo_id}::{decoded_fen}::{decoded_move_uci}"
        
        # å…ˆå°è¯•ä»å†…å­˜åŠ è½½
        result = _circuit_trace_results.get(trace_key)
        
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ç£ç›˜åŠ è½½
        if not result:
            print(f"ğŸ” å†…å­˜ä¸­æœªæ‰¾åˆ°traceç»“æœï¼Œå°è¯•ä»ç£ç›˜åŠ è½½: {trace_key}")
            disk_result = _load_trace_result_from_disk(trace_key)
            if disk_result:
                # åŠ è½½åˆ°å†…å­˜ä¸­ä»¥ä¾¿åç»­å¿«é€Ÿè®¿é—®
                _circuit_trace_results[trace_key] = disk_result
                result = disk_result
                print(f"âœ… æˆåŠŸä»ç£ç›˜æ¢å¤traceç»“æœ: {trace_key}")
    else:
        # å¦‚æœæ²¡æœ‰æä¾›fenå’Œmove_uciï¼Œè¿”å›æœ€è¿‘çš„ç»“æœ
        latest_key = None
        latest_ts = -1
        for key, payload in _circuit_trace_results.items():
            ts = payload.get("finished_at", 0)
            if ts > latest_ts:
                latest_ts = ts
                latest_key = key
        
        result = _circuit_trace_results.get(latest_key) if latest_key else None
        
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ç£ç›˜æŸ¥æ‰¾æœ€æ–°çš„
        if not result:
            print("ğŸ” å†…å­˜ä¸­æœªæ‰¾åˆ°æœ€è¿‘çš„traceç»“æœï¼Œå°è¯•ä»ç£ç›˜æŸ¥æ‰¾...")
            # éå†ç£ç›˜ä¸Šçš„æ‰€æœ‰traceæ–‡ä»¶ï¼Œæ‰¾åˆ°æœ€æ–°çš„
            if TRACE_RESULTS_DIR.exists():
                latest_disk_result = None
                latest_disk_ts = -1
                latest_trace_key = None
                for storage_file in TRACE_RESULTS_DIR.glob("*.json"):
                    try:
                        with open(storage_file, "r", encoding="utf-8") as f:
                            save_data = json.load(f)
                        saved_trace_key = save_data.get("trace_key")
                        disk_result = save_data.get("result")
                        if disk_result:
                            ts = disk_result.get("finished_at", 0)
                            if ts > latest_disk_ts:
                                latest_disk_ts = ts
                                latest_disk_result = disk_result
                                latest_trace_key = saved_trace_key
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½traceæ–‡ä»¶å¤±è´¥ {storage_file}: {e}")
                        continue
                
                if latest_disk_result and latest_trace_key:
                    # åŠ è½½åˆ°å†…å­˜ä¸­
                    _circuit_trace_results[latest_trace_key] = latest_disk_result
                    result = latest_disk_result
                    print(f"âœ… æˆåŠŸä»ç£ç›˜æ¢å¤æœ€æ–°çš„traceç»“æœ: {latest_trace_key}")

    if not result:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°traceç»“æœ")

    return result


@app.get("/circuit_trace/logs")
def get_circuit_trace_logs(
    model_name: str = "lc0/BT4-1024x15x32h",
    sae_combo_id: str | None = None,
    fen: str | None = None,
    move_uci: str | None = None,
):
    """
    è·å–circuit tracingçš„æ—¥å¿—
    
    Args:
        model_name: æ¨¡å‹åç§° (æŸ¥è¯¢å‚æ•°ï¼Œé»˜è®¤: "lc0/BT4-1024x15x32h")
        sae_combo_id: SAEç»„åˆID (æŸ¥è¯¢å‚æ•°ï¼Œå¯é€‰)
        fen: FENå­—ç¬¦ä¸² (æŸ¥è¯¢å‚æ•°ï¼Œå¯é€‰)
        move_uci: UCIç§»åŠ¨ (æŸ¥è¯¢å‚æ•°ï¼Œå¯é€‰)
    
    Returns:
        Circuit tracingæ—¥å¿—åˆ—è¡¨
    """
    global _circuit_trace_logs, _circuit_trace_status
    
    # å¦‚æœæä¾›äº†æ‰€æœ‰å‚æ•°ï¼Œä½¿ç”¨ç²¾ç¡®åŒ¹é…
    if fen and move_uci:
        # è§£ç FENå’Œmove_uciä»¥ç¡®ä¿trace_keyçš„ä¸€è‡´æ€§
        decoded_fen = _decode_fen(fen)
        decoded_move_uci = _decode_fen(move_uci)
        decoded_model_name = _decode_fen(model_name)
        
        combo_id = sae_combo_id or CURRENT_BT4_SAE_COMBO_ID
        combo_cfg = get_bt4_sae_combo(combo_id)
        normalized_combo_id = combo_cfg["id"]
        trace_key = f"{decoded_model_name}::{normalized_combo_id}::{decoded_fen}::{decoded_move_uci}"
        logs = _circuit_trace_logs.get(trace_key, [])
        is_tracing = _circuit_trace_status.get(trace_key, {}).get("is_tracing", False)
    else:
        # å¦åˆ™è¿”å›æœ€è¿‘çš„æ—¥å¿—ï¼ˆæŒ‰æ—¶é—´æˆ³æ’åºï¼‰
        all_logs = []
        for trace_key, log_list in _circuit_trace_logs.items():
            if log_list:
                # è·å–æœ€åä¸€æ¡æ—¥å¿—çš„æ—¶é—´æˆ³
                last_log_time = log_list[-1]["timestamp"] if log_list else 0
                all_logs.append((last_log_time, trace_key, log_list))
        
        # æŒ‰æ—¶é—´æˆ³é™åºæ’åº
        all_logs.sort(key=lambda x: x[0], reverse=True)
        
        # è¿”å›æœ€è¿‘ä¸€æ¡traceçš„æ—¥å¿—
        if all_logs:
            _, trace_key, logs = all_logs[0]
            is_tracing = _circuit_trace_status.get(trace_key, {}).get("is_tracing", False)
        else:
            logs = []
            is_tracing = False
    
    return {
        "model_name": model_name,
        "sae_combo_id": sae_combo_id or CURRENT_BT4_SAE_COMBO_ID,
        "logs": logs,
        "total_count": len(logs),
        "is_tracing": is_tracing,
    }


@app.post("/circuit/check_dense_features")
def check_dense_features_api(request: dict):
    """
    æ£€æŸ¥circuitä¸­å“ªäº›èŠ‚ç‚¹æ˜¯dense featureï¼ˆæ¿€æ´»æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼‰
    
    Args:
        request: åŒ…å«æ£€æŸ¥å‚æ•°çš„è¯·æ±‚ä½“
            - nodes: èŠ‚ç‚¹åˆ—è¡¨
            - threshold: æ¿€æ´»æ¬¡æ•°é˜ˆå€¼ï¼ˆå¯é€‰ï¼ŒNoneè¡¨ç¤ºæ— é™å¤§ï¼‰
            - sae_series: SAEç³»åˆ—åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤: BT4-exp128ï¼‰
            - lorsa_analysis_name: Lorsaåˆ†æåç§°æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
            - tc_analysis_name: TCåˆ†æåç§°æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        denseèŠ‚ç‚¹çš„IDåˆ—è¡¨
    """
    try:
        # æ£€æŸ¥circuits_serviceæ˜¯å¦å¯ç”¨
        if not CIRCUITS_SERVICE_AVAILABLE or check_dense_features is None:
            raise HTTPException(status_code=503, detail="Dense feature check service not available")
        
        # æå–å‚æ•°
        nodes = request.get("nodes", [])
        if not isinstance(nodes, list):
            raise HTTPException(status_code=400, detail="nodes must be a list")
        
        threshold = request.get("threshold")
        if threshold is not None:
            try:
                threshold = int(threshold)
            except (ValueError, TypeError):
                raise HTTPException(status_code=400, detail="threshold must be an integer or null")
        
        sae_series = request.get("sae_series", "BT4-exp128")
        lorsa_analysis_name = request.get("lorsa_analysis_name")
        tc_analysis_name = request.get("tc_analysis_name")
        
        print(f"ğŸ” æ£€æŸ¥dense features: {len(nodes)} ä¸ªèŠ‚ç‚¹, é˜ˆå€¼={threshold}")
        print(f"   - Lorsaæ¨¡æ¿: {lorsa_analysis_name}")
        print(f"   - TCæ¨¡æ¿: {tc_analysis_name}")
        
        # è®¾ç½®MongoDBè¿æ¥
        mongo_config = MongoDBConfig()
        mongo_client_instance = MongoClient(mongo_config)
        
        # è°ƒç”¨æ£€æŸ¥å‡½æ•°
        dense_node_ids = check_dense_features(
            nodes=nodes,
            threshold=threshold,
            mongo_client=mongo_client_instance,
            sae_series=sae_series,
            lorsa_analysis_name=lorsa_analysis_name,
            tc_analysis_name=tc_analysis_name
        )
        
        print(f"âœ… æ‰¾åˆ° {len(dense_node_ids)} ä¸ªdenseèŠ‚ç‚¹")
        
        return {
            "dense_nodes": dense_node_ids,
            "total_nodes": len(nodes),
            "threshold": threshold
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Dense feature check failed: {str(e)}")


@app.post("/patching_analysis")
def patching_analysis(request: dict):
    """
    è¿è¡Œpatchingåˆ†æå¹¶è¿”å›Token Predictionsç»“æœ
    
    Args:
        request: åŒ…å«åˆ†æå‚æ•°çš„è¯·æ±‚ä½“
            - fen: FENå­—ç¬¦ä¸² (å¿…éœ€)
            - feature_type: ç‰¹å¾ç±»å‹ ('transcoder' æˆ– 'lorsa') (å¿…éœ€)
            - layer: å±‚æ•° (å¿…éœ€)
            - pos: ä½ç½® (å¿…éœ€)
            - feature: ç‰¹å¾ç´¢å¼• (å¿…éœ€)
    
    Returns:
        Token Predictionsåˆ†æç»“æœ (JSONæ ¼å¼)
    """
    try:
        # æ£€æŸ¥patchingæœåŠ¡æ˜¯å¦å¯ç”¨
        if not PATCHING_SERVICE_AVAILABLE:
            raise HTTPException(status_code=503, detail="Patching service not available")
        
        # æå–å‚æ•°
        fen = request.get("fen")
        if not fen:
            raise HTTPException(status_code=400, detail="FEN string is required")
        
        feature_type = request.get("feature_type")
        if feature_type not in ['transcoder', 'lorsa']:
            raise HTTPException(status_code=400, detail="feature_type must be 'transcoder' or 'lorsa'")
        
        layer = request.get("layer")
        if layer is None or not isinstance(layer, int):
            raise HTTPException(status_code=400, detail="layer must be an integer")
        
        pos = request.get("pos")
        if pos is None or not isinstance(pos, int):
            raise HTTPException(status_code=400, detail="pos must be an integer")
        
        feature = request.get("feature")
        if feature is None or not isinstance(feature, int):
            raise HTTPException(status_code=400, detail="feature must be an integer")
        
        print(f"ğŸ” è¿è¡Œpatchingåˆ†æ: {feature_type} L{layer} pos{pos} feature{feature}")
        
        # è¿è¡Œpatchingåˆ†æ
        result = run_patching_analysis(
            fen=fen,
            feature_type=feature_type,
            layer=layer,
            pos=pos,
            feature=feature
        )
        
        if 'error' in result:
            raise HTTPException(status_code=400, detail=result['error'])
        
        print(f"âœ… Patchingåˆ†æå®Œæˆï¼Œæ‰¾åˆ° {result['statistics']['total_legal_moves']} ä¸ªåˆæ³•ç§»åŠ¨")
        
        return result
        
    except Exception as e:
        print(f"âŒ Patchingåˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Patching analysis failed: {str(e)}")


@app.get("/patching_analysis/status")
def patching_analysis_status():
    """æ£€æŸ¥patchingåˆ†ææœåŠ¡çš„çŠ¶æ€"""
    return {
        "available": PATCHING_SERVICE_AVAILABLE,
        "hooked_transformer_available": HOOKED_TRANSFORMER_AVAILABLE
    }


@app.post("/steering_analysis")
def steering_analysis(request: dict):
    """
    è¿è¡Œsteeringåˆ†æå¹¶è¿”å›Token Predictionsç»“æœï¼Œæ”¯æŒå¯è°ƒçš„steering_scale
    
    Args:
        request: åŒ…å«åˆ†æå‚æ•°çš„è¯·æ±‚ä½“
            - fen: FENå­—ç¬¦ä¸² (å¿…éœ€)
            - feature_type: ç‰¹å¾ç±»å‹ ('transcoder' æˆ– 'lorsa') (å¿…éœ€)
            - layer: å±‚æ•° (å¿…éœ€)
            - pos: ä½ç½® (å¿…éœ€)
            - feature: ç‰¹å¾ç´¢å¼• (å¿…éœ€)
            - steering_scale: æ”¾å¤§ç³»æ•° (å¯é€‰ï¼Œé»˜è®¤ 1)
    
    Returns:
        Token Predictionsåˆ†æç»“æœ (JSONæ ¼å¼)
    """
    try:
        if not INTERVENTION_SERVICE_AVAILABLE:
            raise HTTPException(status_code=503, detail="Steering service not available")

        fen = request.get("fen")
        if not fen:
            raise HTTPException(status_code=400, detail="FEN string is required")

        feature_type = request.get("feature_type")
        if feature_type not in ['transcoder', 'lorsa']:
            raise HTTPException(status_code=400, detail="feature_type must be 'transcoder' or 'lorsa'")

        layer = request.get("layer")
        if layer is None or not isinstance(layer, int):
            raise HTTPException(status_code=400, detail="layer must be an integer")

        pos = request.get("pos")
        if pos is None or not isinstance(pos, int):
            raise HTTPException(status_code=400, detail="pos must be an integer")

        feature = request.get("feature")
        if feature is None or not isinstance(feature, int):
            raise HTTPException(status_code=400, detail="feature must be an integer")

        steering_scale = request.get("steering_scale", 1)
        if not isinstance(steering_scale, (int, float)):
            raise HTTPException(status_code=400, detail="steering_scale must be a number")

        # è·å–metadataä¿¡æ¯
        metadata = request.get("metadata", {})

        print(f"ğŸ” è¿è¡Œsteeringåˆ†æ: {feature_type} L{layer} pos{pos} feature{feature} scale{steering_scale}")
        print(f"ğŸ“‹ Metadata: {metadata}")

        result = run_feature_steering_analysis(
            fen=fen,
            feature_type=feature_type,
            layer=layer,
            pos=pos,
            feature=feature,
            steering_scale=steering_scale,
            metadata=metadata
        )

        if 'error' in result:
            raise HTTPException(status_code=400, detail=result['error'])

        print(f"âœ… Steeringåˆ†æå®Œæˆï¼Œæ‰¾åˆ° {result['statistics']['total_legal_moves']} ä¸ªåˆæ³•ç§»åŠ¨")
        return result

    except Exception as e:
        print(f"âŒ Steeringåˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Steering analysis failed: {str(e)}")


@app.post("/steering_analysis/multi")
def steering_analysis_multi(request: dict):
    """
    åŒæ—¶å¯¹å¤šä¸ª featureï¼ˆæ¯ä¸ª feature å¯¹åº”ä¸€ä¸ª positionï¼‰è¿›è¡Œ steering åˆ†æã€‚

    Args:
        request:
            - fen: FEN å­—ç¬¦ä¸² (å¿…éœ€)
            - feature_type: 'transcoder' æˆ– 'lorsa' (å¿…éœ€)
            - layer: int (å¿…éœ€)
            - nodes: list[dict] (å¿…éœ€), æ¯ä¸ª node è‡³å°‘åŒ…å«:
                - pos: int
                - feature: int
                - steering_scale: float | int (å¯é€‰ï¼Œé»˜è®¤ 1)
            - metadata: dict (å¯é€‰)

    Returns:
        ä¸ /steering_analysis ç±»ä¼¼çš„åˆ†æç»“æœï¼Œä½† ablation_info.nodes ä¼šåŒ…å«æ¯ä¸ª node çš„ä¿¡æ¯ã€‚
    """
    try:
        if not INTERVENTION_SERVICE_AVAILABLE or run_multi_feature_steering_analysis is None:
            raise HTTPException(status_code=503, detail="Steering service not available")

        fen = request.get("fen")
        if not fen:
            raise HTTPException(status_code=400, detail="FEN string is required")

        feature_type = request.get("feature_type")
        if feature_type not in ["transcoder", "lorsa"]:
            raise HTTPException(status_code=400, detail="feature_type must be 'transcoder' or 'lorsa'")

        layer = request.get("layer")
        if layer is None or not isinstance(layer, int):
            raise HTTPException(status_code=400, detail="layer must be an integer")

        nodes = request.get("nodes")
        if not isinstance(nodes, list) or len(nodes) == 0:
            raise HTTPException(status_code=400, detail="nodes must be a non-empty list")

        metadata = request.get("metadata", {})

        print(f"ğŸ” è¿è¡Œ multi steering åˆ†æ: {feature_type} L{layer}, nodes={len(nodes)}")
        result = run_multi_feature_steering_analysis(
            fen=fen,
            feature_type=feature_type,
            layer=layer,
            nodes=nodes,
            metadata=metadata,
        )
        if "error" in result:
            raise HTTPException(status_code=400, detail=result["error"])
        return result
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Multi steering åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Multi steering analysis failed: {str(e)}")


@app.get("/steering_analysis/status")
def steering_analysis_status():
    """æ£€æŸ¥steeringåˆ†ææœåŠ¡çš„çŠ¶æ€"""
    return {
        "available": INTERVENTION_SERVICE_AVAILABLE,
        "hooked_transformer_available": HOOKED_TRANSFORMER_AVAILABLE
    }


@app.post("/self_play")
def start_self_play(request: dict):
    """
    å¼€å§‹è‡ªå¯¹å¼ˆå¹¶è¿”å›æ¸¸æˆæ•°æ®
    
    Args:
        request: åŒ…å«æ¸¸æˆå‚æ•°çš„è¯·æ±‚ä½“
            - initial_fen: åˆå§‹FENå­—ç¬¦ä¸² (å¯é€‰ï¼Œé»˜è®¤èµ·å§‹å±€é¢)
            - max_moves: æœ€å¤§ç§»åŠ¨æ•° (é»˜è®¤: 10)
            - temperature: æ¸©åº¦å‚æ•° (é»˜è®¤: 1.0)
    
    Returns:
        è‡ªå¯¹å¼ˆæ¸¸æˆæ•°æ® (JSONæ ¼å¼)
    """
    try:
        # æ£€æŸ¥è‡ªå¯¹å¼ˆæœåŠ¡æ˜¯å¦å¯ç”¨
        if not SELF_PLAY_SERVICE_AVAILABLE:
            raise HTTPException(status_code=503, detail="Self-play service not available")
        
        # æå–å‚æ•°
        initial_fen = request.get("initial_fen", "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1")
        max_moves = request.get("max_moves", 10)
        temperature = request.get("temperature", 1.0)
        
        # éªŒè¯å‚æ•°
        if not isinstance(max_moves, int) or max_moves <= 0:
            raise HTTPException(status_code=400, detail="max_moves must be a positive integer")
        
        if not isinstance(temperature, (int, float)) or temperature < 0:
            raise HTTPException(status_code=400, detail="temperature must be a non-negative number")
        
        print(f"ğŸ® å¼€å§‹è‡ªå¯¹å¼ˆ: {initial_fen[:50]}..., æœ€å¤§ç§»åŠ¨æ•°: {max_moves}, æ¸©åº¦: {temperature}")
        
        # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
        model_name = "lc0/BT4-1024x15x32h"
        hooked_model = get_hooked_model(model_name)
        
        # è¿è¡Œè‡ªå¯¹å¼ˆ
        game_result = run_self_play(
            initial_fen=initial_fen,
            max_moves=max_moves,
            temperature=temperature,
            model=hooked_model
        )
        
        print(f"âœ… è‡ªå¯¹å¼ˆå®Œæˆï¼Œå…±è¿›è¡Œäº† {len(game_result['moves'])} æ­¥")
        
        return game_result
        
    except Exception as e:
        print(f"âŒ è‡ªå¯¹å¼ˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Self-play failed: {str(e)}")


@app.post("/self_play/analyze")
def analyze_self_play_positions(request: dict):
    """
    åˆ†æè‡ªå¯¹å¼ˆä¸­çš„ä½ç½®åºåˆ—
    
    Args:
        request: åŒ…å«ä½ç½®åºåˆ—çš„è¯·æ±‚ä½“
            - positions: FENå­—ç¬¦ä¸²åˆ—è¡¨
    
    Returns:
        ä½ç½®åˆ†æç»“æœ (JSONæ ¼å¼)
    """
    try:
        # æ£€æŸ¥è‡ªå¯¹å¼ˆæœåŠ¡æ˜¯å¦å¯ç”¨
        if not SELF_PLAY_SERVICE_AVAILABLE:
            raise HTTPException(status_code=503, detail="Self-play service not available")
        
        # æå–å‚æ•°
        positions = request.get("positions", [])
        
        if not isinstance(positions, list) or not positions:
            raise HTTPException(status_code=400, detail="positions must be a non-empty list of FEN strings")
        
        print(f"ğŸ” åˆ†æä½ç½®åºåˆ—ï¼Œå…± {len(positions)} ä¸ªä½ç½®")
        
        # è·å–å·²ç¼“å­˜çš„HookedTransformeræ¨¡å‹
        hooked_model = get_hooked_model()
        
        # åˆ†æä½ç½®åºåˆ—
        analysis_result = analyze_game_positions(
            positions=positions,
            model=hooked_model
        )
        
        print(f"âœ… ä½ç½®åˆ†æå®Œæˆ")
        
        return {
            "positions_analysis": analysis_result,
            "total_positions": len(positions)
        }
        
    except Exception as e:
        print(f"âŒ ä½ç½®åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Position analysis failed: {str(e)}")


@app.get("/self_play/status")
def self_play_status():
    """æ£€æŸ¥è‡ªå¯¹å¼ˆæœåŠ¡çš„çŠ¶æ€"""
    return {
        "available": SELF_PLAY_SERVICE_AVAILABLE,
        "hooked_transformer_available": HOOKED_TRANSFORMER_AVAILABLE
    }


@app.post("/logit_lens/analyze")
def logit_lens_analyze(request: dict):
    """
    è¿è¡ŒLogit Lensåˆ†æ
    
    Args:
        request: åŒ…å«åˆ†æå‚æ•°çš„è¯·æ±‚ä½“
            - fen: FENå­—ç¬¦ä¸² (å¿…éœ€)
            - target_move: ç›®æ ‡ç§»åŠ¨UCI (å¯é€‰)
            - topk_vocab: è€ƒè™‘çš„é¡¶éƒ¨è¯æ±‡æ•°é‡ (å¯é€‰ï¼Œé»˜è®¤: 2000)
    
    Returns:
        Logit Lensåˆ†æç»“æœ (JSONæ ¼å¼)
    """
    try:
        # æ£€æŸ¥Logit LensæœåŠ¡æ˜¯å¦å¯ç”¨
        if not LOGIT_LENS_AVAILABLE:
            raise HTTPException(status_code=503, detail="Logit Lens service not available")
        
        # æå–å‚æ•°
        fen = request.get("fen")
        if not fen:
            raise HTTPException(status_code=400, detail="FEN string is required")
        
        # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
        model_name = "lc0/BT4-1024x15x32h"
        target_move = request.get("target_move")
        topk_vocab = request.get("topk_vocab", 2000)
        
        print(f"ğŸ” è¿è¡ŒLogit Lensåˆ†æ: FEN={fen[:50]}..., model={model_name}, target={target_move}")
        
        # è·å–æˆ–åˆ›å»ºLogit Lenså®ä¾‹
        global _logit_lens_instances
        if model_name not in _logit_lens_instances:
            # è·å–æ¨¡å‹
            hooked_model = get_hooked_model(model_name)
            # åˆ›å»ºLogit Lenså®ä¾‹
            _logit_lens_instances[model_name] = IntegratedPolicyLens(hooked_model)
        
        lens = _logit_lens_instances[model_name]
        
        # è¿è¡Œåˆ†æ
        result = lens.analyze_single_fen(
            fen=fen,
            target_move=target_move,
            topk_vocab=topk_vocab
        )
        
        if 'error' in result:
            raise HTTPException(status_code=400, detail=result['error'])
        
        print(f"âœ… Logit Lensåˆ†æå®Œæˆï¼Œåˆ†æäº† {result['num_layers']} å±‚")
        
        return {
            **result,
            "model_used": model_name
        }
        
    except Exception as e:
        print(f"âŒ Logit Lensåˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Logit Lens analysis failed: {str(e)}")


@app.get("/logit_lens/status")
def logit_lens_status():
    """æ£€æŸ¥Logit LensæœåŠ¡çš„çŠ¶æ€"""
    return {
        "available": LOGIT_LENS_AVAILABLE,
        "hooked_transformer_available": HOOKED_TRANSFORMER_AVAILABLE
    }


@app.post("/logit_lens/mean_ablation")
def logit_lens_mean_ablation(request: dict):
    """
    è¿è¡ŒMean Ablationåˆ†æ
    
    Args:
        request: åŒ…å«åˆ†æå‚æ•°çš„è¯·æ±‚ä½“
            - fen: FENå­—ç¬¦ä¸² (å¿…éœ€)
            - hook_types: hookç±»å‹åˆ—è¡¨ (å¯é€‰ï¼Œé»˜è®¤: ['attn_out', 'mlp_out'])
            - target_move: ç›®æ ‡ç§»åŠ¨UCI (å¯é€‰)
            - topk_vocab: è€ƒè™‘çš„é¡¶éƒ¨è¯æ±‡æ•°é‡ (å¯é€‰ï¼Œé»˜è®¤: 2000)
    
    Returns:
        Mean Ablationåˆ†æç»“æœ (JSONæ ¼å¼)
    """
    try:
        # æ£€æŸ¥Logit LensæœåŠ¡æ˜¯å¦å¯ç”¨
        if not LOGIT_LENS_AVAILABLE:
            raise HTTPException(status_code=503, detail="Logit Lens service not available")
        
        # æå–å‚æ•°
        fen = request.get("fen")
        if not fen:
            raise HTTPException(status_code=400, detail="FEN string is required")
        
        # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
        model_name = "lc0/BT4-1024x15x32h"
        hook_types = request.get("hook_types", ['attn_out', 'mlp_out'])
        target_move = request.get("target_move")
        topk_vocab = request.get("topk_vocab", 2000)
        
        print(f"ğŸ” è¿è¡ŒMean Ablationåˆ†æ: FEN={fen[:50]}..., model={model_name}, hooks={hook_types}, target={target_move}")
        
        # è·å–æˆ–åˆ›å»ºLogit Lenså®ä¾‹
        global _logit_lens_instances
        if model_name not in _logit_lens_instances:
            # è·å–æ¨¡å‹
            hooked_model = get_hooked_model(model_name)
            # åˆ›å»ºLogit Lenså®ä¾‹
            _logit_lens_instances[model_name] = IntegratedPolicyLens(hooked_model)
        
        lens = _logit_lens_instances[model_name]
        
        # è¿è¡ŒMean Ablationåˆ†æ
        result = lens.analyze_mean_ablation(
            fen=fen,
            hook_types=hook_types,
            target_move=target_move,
            topk_vocab=topk_vocab
        )
        
        if 'error' in result:
            raise HTTPException(status_code=400, detail=result['error'])
        
        print(f"âœ… Mean Ablationåˆ†æå®Œæˆï¼Œåˆ†æäº† {result['num_layers']} å±‚ï¼Œ{len(result['hook_types'])} ç§hookç±»å‹")
        
        return {
            **result,
            "model_used": model_name
        }
        
    except Exception as e:
        print(f"âŒ Mean Ablationåˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Mean Ablation analysis failed: {str(e)}")


# æ–°å¢ï¼šèµ°æ³•è¯„æµ‹æ¥å£ï¼ˆåŸºäºStockfishï¼‰
@app.post("/evaluate_move")
def evaluate_move(request: dict):
    """
    è¯„æµ‹ä¸€æ¬¡ç§»åŠ¨ï¼šè¾“å…¥ä¸Šä¸€æ­¥ä¹‹å‰çš„FENä¸è¯¥æ­¥UCIï¼Œè¿”å›0-100è¯„åˆ†ã€cpå·®ã€WDLç­‰ã€‚

    body: { "fen": str, "move": str, "time_limit": float? }
    """
    fen = request.get("fen")
    move = request.get("move")
    time_limit = request.get("time_limit", 0.2)
    if not fen or not move:
        raise HTTPException(status_code=400, detail="fenä¸moveå¿…å¡«")
    try:
        _ = chess.Board(fen)
    except Exception:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„FEN")

    res = evaluate_move_quality(fen, move, time_limit=time_limit)
    if res is None:
        raise HTTPException(status_code=400, detail="è¯„æµ‹å¤±è´¥æˆ–èµ°æ³•ä¸åˆæ³•")
    return res


# æˆ˜æœ¯ç‰¹å¾åˆ†ææ¥å£
@app.post("/tactic_features/analyze")
async def analyze_tactic_features_api(
    file: UploadFile = File(...),
    n_random: int = Form(200),
    n_fens: int = Form(200),
    top_k_lorsa: int = Form(10),
    top_k_tc: int = Form(10),
    specific_layer: Optional[str] = Form(None),
    specific_layer_top_k: int = Form(20),
):
    """
    åˆ†ææˆ˜æœ¯ç‰¹å¾ï¼šä¸Šä¼ FENæ–‡ä»¶ï¼Œä¸éšæœºFENæ¯”è¾ƒï¼Œæ‰¾å‡ºæœ€ç›¸å…³çš„ç‰¹å¾
    
    Args:
        file: ä¸Šä¼ çš„txtæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªFEN
        model_name: æ¨¡å‹åç§°
        n_random: éšæœºFENæ•°é‡ï¼ˆå…¼å®¹æ—§å‚æ•°ï¼‰
        n_fens: FENæ•°é‡ï¼ˆæ–°å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
        top_k_lorsa: æ˜¾ç¤ºtop kä¸ªLorsaç‰¹å¾
        top_k_tc: æ˜¾ç¤ºtop kä¸ªTCç‰¹å¾
        specific_layer: æŒ‡å®šå±‚å·ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™é¢å¤–è¿”å›è¯¥å±‚çš„è¯¦ç»†ç‰¹å¾
        specific_layer_top_k: æŒ‡å®šå±‚çš„top kç‰¹å¾æ•°
    """
    if not TACTIC_FEATURES_AVAILABLE:
        raise HTTPException(status_code=503, detail="Tactic features analysis not available")
    
    if not HOOKED_TRANSFORMER_AVAILABLE:
        raise HTTPException(status_code=503, detail="HookedTransformer not available")
    
    try:
        # å¼ºåˆ¶ä½¿ç”¨BT4æ¨¡å‹
        model_name = "lc0/BT4-1024x15x32h"
        
        # ========== è°ƒè¯•ä¿¡æ¯ï¼šå‡½æ•°å¼€å§‹ ==========
        print("=" * 80)
        print("ğŸš€ å¼€å§‹å¤„ç†æˆ˜æœ¯ç‰¹å¾åˆ†æè¯·æ±‚")
        print(f"ğŸ“¥ æ¥æ”¶åˆ°çš„åŸå§‹å‚æ•°:")
        print(f"   - model_name: {model_name} (å¼ºåˆ¶ä½¿ç”¨BT4)")
        print(f"   - n_random: {n_random}")
        print(f"   - n_fens: {n_fens}")
        print(f"   - top_k_lorsa: {top_k_lorsa}")
        print(f"   - top_k_tc: {top_k_tc}")
        print(f"   - specific_layer (åŸå§‹): {specific_layer} (ç±»å‹: {type(specific_layer)})")
        print(f"   - specific_layer_top_k: {specific_layer_top_k}")
        print("=" * 80)
        
        # è§£æspecific_layerå‚æ•°
        parsed_specific_layer = None
        print(f"ğŸ” å¼€å§‹è§£æ specific_layer å‚æ•°...")
        print(f"   - specific_layer is None: {specific_layer is None}")
        if specific_layer is not None:
            print(f"   - specific_layer å€¼: '{specific_layer}'")
            print(f"   - specific_layer.strip() å: '{specific_layer.strip() if isinstance(specific_layer, str) else specific_layer}'")
        
        if specific_layer is not None and isinstance(specific_layer, str) and specific_layer.strip():
            try:
                parsed_specific_layer = int(specific_layer.strip())
                print(f"âœ… æˆåŠŸè§£ææŒ‡å®šå±‚å‚æ•°: {parsed_specific_layer} (åŸå§‹å€¼: '{specific_layer}')")
            except (ValueError, TypeError) as e:
                print(f"âŒ è§£æå±‚å·å‚æ•°å¤±è´¥: {e}")
                print(f"âš ï¸ æ— æ•ˆçš„å±‚å·å‚æ•°: '{specific_layer}'ï¼Œå°†å¿½ç•¥æŒ‡å®šå±‚åˆ†æ")
                parsed_specific_layer = None
        elif specific_layer is None:
            print(f"â„¹ï¸ æœªæä¾› specific_layer å‚æ•°ï¼Œå°†ä¸è¿›è¡ŒæŒ‡å®šå±‚åˆ†æ")
        else:
            print(f"âš ï¸ specific_layer å‚æ•°ä¸ºç©ºå­—ç¬¦ä¸²æˆ–æ— æ•ˆï¼Œå°†å¿½ç•¥")
        
        # ä½¿ç”¨n_fenså‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨n_random
        actual_n_fens = n_fens if n_fens != 200 or n_random == 200 else n_random
        print(f"ğŸ“Š å®é™…ä½¿ç”¨çš„FENæ•°é‡: {actual_n_fens}")
        
        print(f"ğŸ¯ æœ€ç»ˆè§£æç»“æœ:")
        print(f"   - parsed_specific_layer: {parsed_specific_layer}")
        print(f"   - specific_layer_top_k: {specific_layer_top_k}")
        print(f"   - actual_n_fens: {actual_n_fens}")
        if parsed_specific_layer is not None:
            print(f"âœ… å°†åˆ†ææŒ‡å®šå±‚: Layer {parsed_specific_layer}")
        else:
            print(f"â„¹ï¸ ä¸è¿›è¡ŒæŒ‡å®šå±‚åˆ†æ")
        print("=" * 80)
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        contents = await file.read()
        text = contents.decode('utf-8')
        tactic_fens = [line.strip() for line in text.strip().split('\n') if line.strip()]
        
        if not tactic_fens:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰æœ‰æ•ˆçš„FENè¡Œ")
        
        # éªŒè¯FENæ ¼å¼
        valid_fens, invalid_fens = validate_fens(tactic_fens)
        
        # é™åˆ¶FENæ•°é‡ï¼šå¦‚æœæ–‡ä»¶ä¸­çš„FENå¤šäºè®¾ç½®çš„æ•°é‡ï¼Œå–å‰næ¡ï¼›å¦åˆ™å…¨éƒ¨ä½¿ç”¨
        if len(valid_fens) > actual_n_fens:
            print(f"ğŸ“Š æ–‡ä»¶ä¸­æœ‰ {len(valid_fens)} ä¸ªæœ‰æ•ˆFENï¼Œå–å‰ {actual_n_fens} ä¸ª")
            valid_fens = valid_fens[:actual_n_fens]
        else:
            print(f"ğŸ“Š æ–‡ä»¶ä¸­æœ‰ {len(valid_fens)} ä¸ªæœ‰æ•ˆFENï¼Œå…¨éƒ¨ä½¿ç”¨")
        
        if len(valid_fens) == 0:
            raise HTTPException(
                status_code=400,
                detail=f"æ²¡æœ‰æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²ã€‚æ— æ•ˆFENç¤ºä¾‹: {invalid_fens[:5]}"
            )
        
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        hooked_model = get_hooked_model(model_name)
        
        # æ£€æŸ¥ç¼“å­˜çš„transcoderså’Œlorsas
        cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(model_name)
        
        num_layers = 15
        if cached_transcoders is not None and cached_lorsas is not None:
            if len(cached_transcoders) == num_layers and len(cached_lorsas) == num_layers:
                print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„transcoderså’Œlorsas: {model_name}")
                transcoders = cached_transcoders
                lorsas = cached_lorsas
            else:
                # ç¼“å­˜ä¸å®Œæ•´ï¼Œéœ€è¦åŠ è½½
                print(f"âš ï¸ ç¼“å­˜ä¸å®Œæ•´ï¼Œé‡æ–°åŠ è½½: {model_name}")
                transcoders = None
                lorsas = None
        else:
            transcoders = None
            lorsas = None
        
        # å¦‚æœç¼“å­˜ä¸å¯ç”¨ï¼Œåˆ™åŠ è½½
        if transcoders is None or lorsas is None:
            if 'BT4' in model_name:
                tc_base_path = BT4_TC_BASE_PATH
                lorsa_base_path = BT4_LORSA_BASE_PATH
            else:
                raise ValueError("Unsupported Model!")
            
            transcoders = {}
            lorsas = []
            
            for layer in range(num_layers):
                # åŠ è½½Transcoder
                tc_path = f"{tc_base_path}/L{layer}"
                if os.path.exists(tc_path):
                    transcoders[layer] = SparseAutoEncoder.from_pretrained(
                        tc_path,
                        dtype=torch.float32,
                        device=device,
                    )
                else:
                    raise HTTPException(
                        status_code=404,
                        detail=f"Transcoder not found at {tc_path}"
                    )
                
                # åŠ è½½Lorsa
                lorsa_path = f"{lorsa_base_path}/L{layer}"
                if os.path.exists(lorsa_path):
                    lorsas.append(LowRankSparseAttention.from_pretrained(
                        lorsa_path,
                        device=device,
                    ))
                else:
                    raise HTTPException(
                        status_code=404,
                        detail=f"Lorsa not found at {lorsa_path}"
                    )
            
            # ç¼“å­˜åŠ è½½çš„transcoderså’Œlorsas
            if CIRCUITS_SERVICE_AVAILABLE and set_cached_models is not None:
                # éœ€è¦åˆ›å»ºreplacement_modelæ‰èƒ½ç¼“å­˜ï¼Œè¿™é‡Œå…ˆç¼“å­˜transcoderså’Œlorsas
                _global_transcoders_cache[model_name] = transcoders
                _global_lorsas_cache[model_name] = lorsas
                _global_hooked_models[model_name] = hooked_model
        
        # æ‰§è¡Œåˆ†æ
        print("=" * 80)
        print(f"ğŸ”¬ å¼€å§‹æ‰§è¡Œç‰¹å¾åˆ†æ")
        print(f"   - æˆ˜æœ¯FENæ•°é‡: {len(valid_fens)}æ¡")
        print(f"   - éšæœºFENæ•°é‡: {actual_n_fens}æ¡")
        print(f"   - æ¨¡å‹å±‚æ•°: {num_layers}å±‚ (0-{num_layers-1})")
        if parsed_specific_layer is not None:
            print(f"   âœ… æŒ‡å®šå±‚åˆ†æå·²å¯ç”¨:")
            print(f"      - å±‚å·: Layer {parsed_specific_layer}")
            print(f"      - Top K: {specific_layer_top_k}")
            if parsed_specific_layer < 0 or parsed_specific_layer >= num_layers:
                print(f"      âš ï¸ è­¦å‘Š: å±‚å· {parsed_specific_layer} è¶…å‡ºæœ‰æ•ˆèŒƒå›´!")
        else:
            print(f"   â„¹ï¸ æœªæŒ‡å®šå±‚ï¼Œå°†åªè¿”å›æ‰€æœ‰å±‚çš„Top Kç‰¹å¾")
        print("=" * 80)
        
        result = analyze_tactic_features(
            tactic_fens=valid_fens,
            model=hooked_model,
            lorsas=lorsas,
            transcoders=transcoders,
            n_random=actual_n_fens,
        )
        
        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])
        
        # æ’åºå¹¶å–top k
        lorsa_diffs = sorted(result["lorsa_diffs"], key=lambda x: x[2], reverse=True)[:top_k_lorsa]
        tc_diffs = sorted(result["tc_diffs"], key=lambda x: x[2], reverse=True)[:top_k_tc]
        
        # æ ¼å¼åŒ–ç»“æœ
        def format_diff(diff_tuple):
            layer, feature, diff, p_random, p_tactic, kind = diff_tuple
            return {
                "layer": layer,
                "feature": feature,
                "diff": float(diff),
                "p_random": float(p_random),
                "p_tactic": float(p_tactic),
                "kind": kind
            }
        
        response_data = {
            "valid_tactic_fens": result["valid_tactic_fens"],
            "invalid_tactic_fens": result["invalid_tactic_fens"],
            "random_fens": result["random_fens"],
            "tactic_fens": result["tactic_fens"],
            "top_lorsa_features": [format_diff(d) for d in lorsa_diffs],
            "top_tc_features": [format_diff(d) for d in tc_diffs],
            "invalid_fens_sample": result.get("invalid_fens_list", [])
        }
        
        # å¦‚æœæŒ‡å®šäº†å±‚å·ï¼Œè¿”å›è¯¥å±‚çš„è¯¦ç»†ç‰¹å¾
        print("=" * 80)
        print(f"ğŸ” æ£€æŸ¥æ˜¯å¦éœ€è¦è¿”å›æŒ‡å®šå±‚ç‰¹å¾...")
        print(f"   - parsed_specific_layer: {parsed_specific_layer}")
        print(f"   - num_layers: {num_layers}")
        print(f"   - æ¡ä»¶æ£€æŸ¥: parsed_specific_layer is not None = {parsed_specific_layer is not None}")
        if parsed_specific_layer is not None:
            print(f"   - æ¡ä»¶æ£€æŸ¥: 0 <= {parsed_specific_layer} < {num_layers} = {0 <= parsed_specific_layer < num_layers}")
        
        if parsed_specific_layer is not None and 0 <= parsed_specific_layer < num_layers:
            print(f"âœ… å¼€å§‹ç­›é€‰ Layer {parsed_specific_layer} çš„ç‰¹å¾...")
            
            # æ‰“å°æ‰€æœ‰ç‰¹å¾çš„æ€»æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            total_lorsa_diffs = len(result["lorsa_diffs"])
            total_tc_diffs = len(result["tc_diffs"])
            print(f"   - æ€»Lorsaç‰¹å¾æ•°: {total_lorsa_diffs}")
            print(f"   - æ€»TCç‰¹å¾æ•°: {total_tc_diffs}")
            
            # ç­›é€‰å‡ºæŒ‡å®šå±‚çš„ç‰¹å¾
            specific_lorsa = [d for d in result["lorsa_diffs"] if d[0] == parsed_specific_layer]
            specific_tc = [d for d in result["tc_diffs"] if d[0] == parsed_specific_layer]
            
            print(f"ğŸ“Š Layer {parsed_specific_layer} ç‰¹å¾ç»Ÿè®¡:")
            print(f"   - Lorsaç‰¹å¾: {len(specific_lorsa)}ä¸ª")
            print(f"   - TCç‰¹å¾: {len(specific_tc)}ä¸ª")
            
            if len(specific_lorsa) == 0:
                print(f"   âš ï¸ è­¦å‘Š: Layer {parsed_specific_layer} æ²¡æœ‰æ‰¾åˆ°ä»»ä½• Lorsa ç‰¹å¾!")
            if len(specific_tc) == 0:
                print(f"   âš ï¸ è­¦å‘Š: Layer {parsed_specific_layer} æ²¡æœ‰æ‰¾åˆ°ä»»ä½• TC ç‰¹å¾!")
            
            # æ’åºå¹¶å–top k
            specific_lorsa_sorted = sorted(specific_lorsa, key=lambda x: x[2], reverse=True)[:specific_layer_top_k]
            specific_tc_sorted = sorted(specific_tc, key=lambda x: x[2], reverse=True)[:specific_layer_top_k]
            
            print(f"   - æ’åºåå–Top {specific_layer_top_k}:")
            print(f"     * Lorsa: {len(specific_lorsa_sorted)}ä¸ª")
            print(f"     * TC: {len(specific_tc_sorted)}ä¸ª")
            
            # æ‰“å°å‰3ä¸ªç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if len(specific_lorsa_sorted) > 0:
                print(f"   - Lorsa Top 3 ç‰¹å¾ç¤ºä¾‹:")
                for i, feat in enumerate(specific_lorsa_sorted[:3]):
                    print(f"     [{i+1}] Layer={feat[0]}, Feature={feat[1]}, Diff={feat[2]:.6f}")
            
            if len(specific_tc_sorted) > 0:
                print(f"   - TC Top 3 ç‰¹å¾ç¤ºä¾‹:")
                for i, feat in enumerate(specific_tc_sorted[:3]):
                    print(f"     [{i+1}] Layer={feat[0]}, Feature={feat[1]}, Diff={feat[2]:.6f}")
            
            response_data["specific_layer"] = parsed_specific_layer
            response_data["specific_layer_lorsa"] = [format_diff(d) for d in specific_lorsa_sorted]
            response_data["specific_layer_tc"] = [format_diff(d) for d in specific_tc_sorted]
            
            print(f"âœ… å·²æ·»åŠ æŒ‡å®šå±‚ç‰¹å¾åˆ°å“åº”æ•°æ®:")
            print(f"   - specific_layer: {response_data.get('specific_layer')}")
            print(f"   - specific_layer_lorsa: {len(response_data.get('specific_layer_lorsa', []))}ä¸ª")
            print(f"   - specific_layer_tc: {len(response_data.get('specific_layer_tc', []))}ä¸ª")
        elif parsed_specific_layer is not None:
            print(f"âŒ æŒ‡å®šçš„å±‚å· {parsed_specific_layer} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (0-{num_layers-1})")
            print(f"   å°†å¿½ç•¥æŒ‡å®šå±‚åˆ†æ")
        else:
            print(f"â„¹ï¸ æœªæŒ‡å®šå±‚å·ï¼Œè·³è¿‡æŒ‡å®šå±‚ç‰¹å¾ç­›é€‰")
        
        print("=" * 80)
        print(f"ğŸ“¤ å‡†å¤‡è¿”å›å“åº”æ•°æ®:")
        print(f"   - åŸºç¡€ç»Ÿè®¡: valid_tactic_fens={response_data.get('valid_tactic_fens')}, tactic_fens={response_data.get('tactic_fens')}")
        print(f"   - Top Lorsaç‰¹å¾: {len(response_data.get('top_lorsa_features', []))}ä¸ª")
        print(f"   - Top TCç‰¹å¾: {len(response_data.get('top_tc_features', []))}ä¸ª")
        print(f"   - æŒ‡å®šå±‚: {response_data.get('specific_layer', 'æœªæŒ‡å®š')}")
        if response_data.get('specific_layer') is not None:
            print(f"   - æŒ‡å®šå±‚Lorsa: {len(response_data.get('specific_layer_lorsa', []))}ä¸ª")
            print(f"   - æŒ‡å®šå±‚TC: {len(response_data.get('specific_layer_tc', []))}ä¸ª")
        print("=" * 80)
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@app.get("/tactic_features/status")
def tactic_features_status():
    """æ£€æŸ¥æˆ˜æœ¯ç‰¹å¾åˆ†ææœåŠ¡çš„çŠ¶æ€"""
    return {
        "available": TACTIC_FEATURES_AVAILABLE,
        "hooked_transformer_available": HOOKED_TRANSFORMER_AVAILABLE
    }


# Graph Feature Diffing API
try:
    from graph_feature_diffing import compare_fen_activations, parse_node_id
    GRAPH_FEATURE_DIFFING_AVAILABLE = True
except ImportError:
    compare_fen_activations = None
    parse_node_id = None
    GRAPH_FEATURE_DIFFING_AVAILABLE = False
    print("WARNING: graph_feature_diffing not found, graph feature diffing will not be available")


@app.post("/circuit/compare_fen_activations")
def compare_fen_activations_api(request: dict):
    """
    æ¯”è¾ƒä¸¤ä¸ªFENçš„æ¿€æ´»å·®å¼‚ï¼Œæ‰¾å‡ºåœ¨perturbed FENä¸­æœªæ¿€æ´»çš„èŠ‚ç‚¹
    
    è¯·æ±‚ä½“:
    {
        "graph_json": {...},  # åŸå§‹å›¾çš„JSONæ•°æ®
        "original_fen": "2k5/4Q3/3P4/8/6p1/4p3/q1pbK3/1R6 b - - 0 32",
        "perturbed_fen": "2k5/4Q3/3P4/8/6p1/8/q1pbK3/1R6 b - - 0 32",
        "model_name": "lc0/BT4-1024x15x32h",
        "activation_threshold": 0.0
    }
    """
    if not GRAPH_FEATURE_DIFFING_AVAILABLE or not CIRCUITS_SERVICE_AVAILABLE:
        raise HTTPException(status_code=503, detail="Graph feature diffing service is not available")
    
    try:
        graph_json = request.get('graph_json')
        original_fen = request.get('original_fen')
        perturbed_fen = request.get('perturbed_fen')
        model_name = request.get('model_name', 'lc0/BT4-1024x15x32h')
        activation_threshold = request.get('activation_threshold', 0.0)
        
        if not graph_json:
            raise HTTPException(status_code=400, detail="graph_json is required")
        if not original_fen:
            raise HTTPException(status_code=400, detail="original_fen is required")
        if not perturbed_fen:
            raise HTTPException(status_code=400, detail="perturbed_fen is required")
        
        # éªŒè¯FENæ ¼å¼
        try:
            chess.Board(original_fen)
        except ValueError:
            raise HTTPException(status_code=400, detail=f"Invalid original FEN: {original_fen}")
        
        try:
            chess.Board(perturbed_fen)
        except ValueError:
            raise HTTPException(status_code=400, detail=f"Invalid perturbed FEN: {perturbed_fen}")
        
        print(f"ğŸ” å¼€å§‹æ¯”è¾ƒFENæ¿€æ´»å·®å¼‚:")
        print(f"   - åŸå§‹FEN: {original_fen}")
        print(f"   - æ‰°åŠ¨FEN: {perturbed_fen}")
        print(f"   - æ¨¡å‹: {model_name}")
        print(f"   - æ¿€æ´»é˜ˆå€¼: {activation_threshold}")
        
        # è·å–æˆ–åŠ è½½æ¨¡å‹å’Œ transcoders/lorsas
        # ä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼Œå¹¶åœ¨æœ‰åŠ è½½é”æ—¶ç¦æ­¢é‡æ–°åŠ è½½
        n_layers = 15

        # ç»Ÿä¸€ä½¿ç”¨å½“å‰ç»„åˆ IDï¼ˆä¸ SaeComboLoader / circuit_trace ä¿æŒä¸€è‡´ï¼‰
        sae_combo_id = request.get("sae_combo_id") or CURRENT_BT4_SAE_COMBO_ID
        combo_cfg = get_bt4_sae_combo(sae_combo_id)
        normalized_combo_id = combo_cfg["id"]
        combo_key = _make_combo_cache_key(model_name, normalized_combo_id)

        # è·å– HookedTransformer æ¨¡å‹ï¼ˆè‡ªèº«æœ‰ç¼“å­˜ï¼‰
        hooked_model = get_hooked_model(model_name)

        # å…ˆä»æœ¬åœ°ç¼“å­˜ä¸­å–ï¼ˆæŒ‰ combo_key åŒºåˆ†ä¸åŒç»„åˆï¼‰
        global _transcoders_cache, _lorsas_cache, _replacement_models_cache, _loading_status
        cached_transcoders = _transcoders_cache.get(combo_key)
        cached_lorsas = _lorsas_cache.get(combo_key)
        cached_replacement_model = _replacement_models_cache.get(combo_key)

        cache_complete = (
            cached_transcoders is not None
            and cached_lorsas is not None
            and cached_replacement_model is not None
            and len(cached_transcoders) == n_layers
            and len(cached_lorsas) == n_layers
        )

        is_loading = _loading_status.get(combo_key, {}).get("is_loading", False)

        # å¦‚æœå½“å‰ç»„åˆæ­£åœ¨åŠ è½½ï¼Œç›´æ¥æŠ¥é”™ï¼Œç¦æ­¢åœ¨é”æœªé‡Šæ”¾æ—¶é‡å¤åŠ è½½
        if not cache_complete and is_loading:
            raise HTTPException(
                status_code=503,
                detail=(
                    f"Transcoders/Lorsas for model {model_name} combo {normalized_combo_id} "
                    f"are still loading. è¯·ç­‰å¾…åŠ è½½å®Œæˆæˆ–å–æ¶ˆåå†æ¯”è¾ƒæ¿€æ´»å·®å¼‚ã€‚"
                ),
            )

        if cache_complete:
            # æ­£å¸¸ä½¿ç”¨å·²é¢„åŠ è½½å¥½çš„æ¨¡å‹ä¸ SAE
            print(f"âœ… ä½¿ç”¨é¢„åŠ è½½çš„ transcoders/Lorsas: {model_name} @ {normalized_combo_id}")
            replacement_model = cached_replacement_model
            transcoders = cached_transcoders
            lorsas = cached_lorsas
        else:
            # ã€ä¸¥æ ¼æ¨¡å¼ã€‘å®Œå…¨ç¦æ­¢åœ¨ compare æ¥å£é‡Œä¸»åŠ¨åŠ è½½ Lorsa / TC
            # è¦æ±‚è°ƒç”¨æ–¹å¿…é¡»å…ˆé€šè¿‡ /circuit/preload_models é¢„åŠ è½½ç›¸åº”ç»„åˆ
            msg = (
                f"No cached transcoders/Lorsas for model {model_name} combo {normalized_combo_id}. "
                "è¯·å…ˆè°ƒç”¨ /circuit/preload_models é¢„åŠ è½½è¯¥ç»„åˆåå†æ¯”è¾ƒæ¿€æ´»å·®å¼‚ã€‚"
            )
            print(f"âŒ {msg}")
            raise HTTPException(status_code=503, detail=msg)
        
        # æ‰§è¡Œæ¯”è¾ƒ
        result = compare_fen_activations(
            graph_json=graph_json,
            original_fen=original_fen,
            perturbed_fen=perturbed_fen,
            model_name=model_name,
            transcoders=transcoders,
            lorsas=lorsas,
            replacement_model=replacement_model,
            activation_threshold=activation_threshold,
            n_layers=n_layers
        )
        
        print(f"âœ… æ¯”è¾ƒå®Œæˆ:")
        print(f"   - æ€»èŠ‚ç‚¹æ•°: {result['total_nodes']}")
        print(f"   - æœªæ¿€æ´»èŠ‚ç‚¹æ•°: {result['inactive_nodes_count']}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ¯”è¾ƒå¤±è´¥: {str(e)}")


def _decode_fen(fen: str) -> str:
    """è§£ç FENå­—ç¬¦ä¸²ï¼ˆæ”¯æŒå¤šæ¬¡è§£ç ï¼Œå¤„ç†åŒé‡ç¼–ç ï¼‰"""
    import urllib.parse
    decoded = fen
    while "%" in decoded:
        new_decoded = urllib.parse.unquote(decoded)
        if new_decoded == decoded:
            break  # æ²¡æœ‰æ›´å¤šç¼–ç äº†
        decoded = new_decoded
    return decoded


# å¯¼å…¥ global_weight æ¨¡å—
try:
    from .global_weight import (
        load_max_activations,
        tc_global_weight_in,
        lorsa_global_weight_in,
        tc_global_weight_out,
        lorsa_global_weight_out,
    )
except ImportError:
    from global_weight import (
        load_max_activations,
        tc_global_weight_in,
        lorsa_global_weight_in,
        tc_global_weight_out,
        lorsa_global_weight_out,
    )


@app.get("/global_weight")
def get_global_weight(
    model_name: str = "lc0/BT4-1024x15x32h",
    sae_combo_id: str | None = None,
    feature_type: str = "tc",  # "tc" or "lorsa"
    layer_idx: int = 0,
    feature_idx: int = 0,
    k: int = 100,
    activation_type: str = "max",  # "max" or "mean"
    features_in_layer_filter: str | None = None,  # å±‚è¿‡æ»¤å™¨ (ä¾‹å¦‚: "4,5,8-9")
    features_out_layer_filter: str | None = None,  # å±‚è¿‡æ»¤å™¨ (ä¾‹å¦‚: "4,5,8-9")
):
    """
    è·å–featureçš„å…¨å±€æƒé‡ï¼ˆè¾“å…¥å’Œè¾“å‡ºï¼‰

    Args:
        model_name: æ¨¡å‹åç§°
        sae_combo_id: SAEç»„åˆID
        feature_type: ç‰¹å¾ç±»å‹ ("tc" æˆ– "lorsa")
        layer_idx: å±‚ç´¢å¼•
        feature_idx: ç‰¹å¾ç´¢å¼•
        k: è¿”å›çš„top kæ•°é‡
        activation_type: æ¿€æ´»ç±»å‹ ("max" æˆ– "mean")
        features_in_layer_filter: è¾“å…¥ç‰¹å¾å±‚è¿‡æ»¤å™¨ (ä¾‹å¦‚: "4,5,8-9" è¡¨ç¤ºåªåŒ…å«å±‚4ã€5ã€8ã€9çš„ç‰¹å¾)
        features_out_layer_filter: è¾“å‡ºç‰¹å¾å±‚è¿‡æ»¤å™¨ (ä¾‹å¦‚: "4,5,8-9" è¡¨ç¤ºåªåŒ…å«å±‚4ã€5ã€8ã€9çš„ç‰¹å¾)

    Returns:
        åŒ…å«è¾“å…¥å’Œè¾“å‡ºå…¨å±€æƒé‡çš„å­—å…¸
    """
    def parse_layer_filter(filter_str: str | None) -> list[int] | None:
        """
        è§£æå±‚è¿‡æ»¤å™¨å­—ç¬¦ä¸²

        Args:
            filter_str: è¿‡æ»¤å™¨å­—ç¬¦ä¸² (ä¾‹å¦‚: "4,5,8-9")

        Returns:
            å±‚ç´¢å¼•åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneæˆ–ç©ºå­—ç¬¦ä¸²åˆ™è¿”å›Noneè¡¨ç¤ºä¸è¿‡æ»¤
        """
        if not filter_str or not filter_str.strip():
            return None

        layers = []
        parts = filter_str.split(',')

        for part in parts:
            part = part.strip()
            if '-' in part:
                # å¤„ç†èŒƒå›´ (ä¾‹å¦‚: "8-9")
                try:
                    start, end = map(int, part.split('-'))
                    if start > end:
                        continue
                    layers.extend(range(start, end + 1))
                except ValueError:
                    continue
            else:
                # å¤„ç†å•ä¸ªæ•°å­— (ä¾‹å¦‚: "4")
                try:
                    layer = int(part)
                    layers.append(layer)
                except ValueError:
                    continue

        # å»é‡å¹¶æ’åº
        return sorted(list(set(layers)))

    try:
        # URLè§£ç ï¼Œå¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜ï¼ˆä¸ /circuit/loading_logs ä¿æŒä¸€è‡´ï¼‰
        import urllib.parse

        decoded_model_name = urllib.parse.unquote(model_name)
        if "%" in decoded_model_name:
            decoded_model_name = urllib.parse.unquote(decoded_model_name)
        
        # è·å–SAEç»„åˆé…ç½®
        combo_id = sae_combo_id or CURRENT_BT4_SAE_COMBO_ID
        combo_cfg = get_bt4_sae_combo(combo_id)
        normalized_combo_id = combo_cfg["id"]
        
        # ä½¿ç”¨ get_cached_transcoders_and_lorsas è·å–ç¼“å­˜çš„transcoderså’Œlorsas
        # è¿™ä¸ªå‡½æ•°ä¼šå…ˆæ£€æŸ¥ circuits_service çš„ç¼“å­˜ï¼Œç„¶åå†æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        # ä½¿ç”¨è§£ç åçš„ model_name
        cached_transcoders, cached_lorsas = get_cached_transcoders_and_lorsas(decoded_model_name, normalized_combo_id)
        
        if cached_transcoders is None or cached_lorsas is None:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯·æ±‚çš„ç»„åˆIDå’Œå½“å‰æœåŠ¡å™¨ç«¯çš„ç»„åˆID
            # ä½¿ç”¨è§£ç åçš„ model_name ç”Ÿæˆç¼“å­˜é”®
            cache_key = _make_combo_cache_key(decoded_model_name, normalized_combo_id)
            error_detail = (
                f"Transcoders/LorsasæœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ /circuit/preload_models é¢„åŠ è½½ã€‚"
                f"è¯·æ±‚çš„ç»„åˆID: {normalized_combo_id}, "
                f"ç¼“å­˜é”®: {cache_key}, "
                f"å½“å‰æœåŠ¡å™¨ç«¯ç»„åˆID: {CURRENT_BT4_SAE_COMBO_ID}"
            )
            print(f"âš ï¸ /global_weight è¯·æ±‚å¤±è´¥: {error_detail}")
            print(f"   åŸå§‹model_nameå‚æ•°: {model_name!r}")
            print(f"   è§£ç åmodel_name: {decoded_model_name!r}")
            # æ‰“å°å½“å‰ç¼“å­˜é”®åˆ—è¡¨ä»¥å¸®åŠ©è°ƒè¯•
            if CIRCUITS_SERVICE_AVAILABLE:
                from circuits_service import _global_transcoders_cache, _global_lorsas_cache
                print(f"   circuits_service ç¼“å­˜é”®: transcoders={list(_global_transcoders_cache.keys())}, lorsas={list(_global_lorsas_cache.keys())}")
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç±»ä¼¼çš„ç¼“å­˜é”®ï¼ˆä½¿ç”¨åŸå§‹æˆ–è§£ç åçš„model_nameï¼‰
                for key in list(_global_transcoders_cache.keys()) + list(_global_lorsas_cache.keys()):
                    if normalized_combo_id in key:
                        print(f"     æ‰¾åˆ°ç›¸å…³ç¼“å­˜é”®: {key!r}")
            print(f"   æœ¬åœ°ç¼“å­˜é”®: transcoders={list(_transcoders_cache.keys())}, lorsas={list(_lorsas_cache.keys())}")
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç±»ä¼¼çš„ç¼“å­˜é”®
            for key in list(_transcoders_cache.keys()) + list(_lorsas_cache.keys()):
                if normalized_combo_id in key:
                    print(f"     æ‰¾åˆ°ç›¸å…³ç¼“å­˜é”®: {key!r}")
            raise HTTPException(
                status_code=503,
                detail=error_detail
            )
        
        # éªŒè¯activation_typeå‚æ•°
        if activation_type not in ["max", "mean"]:
            raise HTTPException(status_code=400, detail="activation_typeå¿…é¡»æ˜¯'max'æˆ–'mean'")
        
        # åŠ è½½activationsæ•°æ®ï¼ˆmaxæˆ–meanï¼‰
        tc_acts, lorsa_acts = load_max_activations(
            normalized_combo_id, device=device, get_bt4_sae_combo=get_bt4_sae_combo,
            activation_type=activation_type
        )

        # è§£æå±‚è¿‡æ»¤å™¨
        features_in_layer_filter_parsed = parse_layer_filter(features_in_layer_filter)
        features_out_layer_filter_parsed = parse_layer_filter(features_out_layer_filter)
        
        # éªŒè¯å‚æ•°
        if layer_idx < 0 or layer_idx >= len(cached_transcoders):
            raise HTTPException(status_code=400, detail=f"layer_idxå¿…é¡»åœ¨0-{len(cached_transcoders)-1}ä¹‹é—´")
        
        if feature_type == "tc":
            if feature_idx < 0 or feature_idx >= cached_transcoders[layer_idx].cfg.d_sae:
                raise HTTPException(
                    status_code=400,
                    detail=f"feature_idxå¿…é¡»åœ¨0-{cached_transcoders[layer_idx].cfg.d_sae-1}ä¹‹é—´"
                )
            
            # è®¡ç®—TCçš„å…¨å±€æƒé‡
            features_in = tc_global_weight_in(
                cached_transcoders, cached_lorsas, layer_idx, feature_idx,
                tc_acts, lorsa_acts, k=k, layer_filter=features_in_layer_filter_parsed
            )
            features_out = tc_global_weight_out(
                cached_transcoders, cached_lorsas, layer_idx, feature_idx,
                tc_acts, lorsa_acts, k=k, layer_filter=features_out_layer_filter_parsed
            )
        elif feature_type == "lorsa":
            if feature_idx < 0 or feature_idx >= cached_lorsas[layer_idx].cfg.d_sae:
                raise HTTPException(
                    status_code=400,
                    detail=f"feature_idxå¿…é¡»åœ¨0-{cached_lorsas[layer_idx].cfg.d_sae-1}ä¹‹é—´"
                )
            
            # è®¡ç®—Lorsaçš„å…¨å±€æƒé‡
            features_in = lorsa_global_weight_in(
                cached_transcoders, cached_lorsas, layer_idx, feature_idx,
                tc_acts, lorsa_acts, k=k, layer_filter=features_in_layer_filter_parsed
            )
            features_out = lorsa_global_weight_out(
                cached_transcoders, cached_lorsas, layer_idx, feature_idx,
                tc_acts, lorsa_acts, k=k, layer_filter=features_out_layer_filter_parsed
            )
        else:
            raise HTTPException(status_code=400, detail="feature_typeå¿…é¡»æ˜¯'tc'æˆ–'lorsa'")
        
        return {
            "feature_type": feature_type,
            "layer_idx": layer_idx,
            "feature_idx": feature_idx,
            "activation_type": activation_type,
            "feature_name": f"BT4_{feature_type}_L{layer_idx}{'M' if feature_type == 'tc' else 'A'}_k30_e16#{feature_idx}",
            "features_in": [{"name": name, "weight": weight} for name, weight in features_in],
            "features_out": [{"name": name, "weight": weight} for name, weight in features_out],
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è®¡ç®—å…¨å±€æƒé‡å¤±è´¥: {str(e)}")


###############################################################################
# Circuit Annotation API
###############################################################################


@app.post("/circuit_annotations")
def create_circuit_annotation(request: dict):
    """
    åˆ›å»ºæ–°çš„circuitæ ‡æ³¨
    
    Args:
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - circuit_interpretation: å›è·¯çš„æ•´ä½“è§£é‡Š
            - sae_combo_id: SAEç»„åˆID
            - features: ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªç‰¹å¾åŒ…å«ï¼š
                - sae_name: SAEåç§°
                - sae_series: SAEç³»åˆ—
                - layer: å±‚å·ï¼ˆæ¨¡å‹ä¸­çš„å®é™…å±‚ï¼‰
                - feature_index: ç‰¹å¾ç´¢å¼•
                - feature_type: ç‰¹å¾ç±»å‹ ("transcoder" æˆ– "lorsa")
                - interpretation: è¯¥ç‰¹å¾çš„è§£é‡Šï¼ˆå¯é€‰ï¼‰
                - level: å¯é€‰çš„circuitå±‚çº§ï¼ˆç‹¬ç«‹äºlayerï¼Œç”¨äºå¯è§†åŒ–ï¼‰
                - feature_id: å¯é€‰çš„featureå”¯ä¸€æ ‡è¯†ç¬¦
            - edges: å¯é€‰çš„è¾¹åˆ—è¡¨ï¼Œæ¯æ¡è¾¹åŒ…å«ï¼š
                - source_feature_id: æºfeatureçš„ID
                - target_feature_id: ç›®æ ‡featureçš„ID
                - weight: è¾¹çš„æƒé‡
                - interpretation: å¯é€‰çš„è¾¹è§£é‡Š
            - metadata: å¯é€‰çš„å…ƒæ•°æ®å­—å…¸
    
    Returns:
        åˆ›å»ºçš„circuitæ ‡æ³¨ä¿¡æ¯
    """
    try:
        return create_circuit_annotation_service(
            client=client,
            sae_series=sae_series,
            circuit_interpretation=request.get("circuit_interpretation", ""),
            sae_combo_id=request.get("sae_combo_id"),
            features=request.get("features", []),
            edges=request.get("edges"),
            metadata=request.get("metadata"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºcircuitæ ‡æ³¨å¤±è´¥: {str(e)}")


@app.get("/circuit_annotations/by_feature")
def get_circuits_by_feature(
    sae_name: str,
    sae_series: Optional[str] = None,
    layer: int = 0,
    feature_index: int = 0,
    feature_type: Optional[str] = None,
):
    """
    è·å–åŒ…å«æŒ‡å®šç‰¹å¾çš„æ‰€æœ‰circuitæ ‡æ³¨
    
    Args:
        sae_name: SAEåç§°
        sae_series: SAEç³»åˆ—ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€sae_seriesï¼‰
        layer: å±‚å·
        feature_index: ç‰¹å¾ç´¢å¼•
        feature_type: å¯é€‰çš„ç‰¹å¾ç±»å‹è¿‡æ»¤å™¨ ("transcoder" æˆ– "lorsa")
    
    Returns:
        åŒ…å«è¯¥ç‰¹å¾çš„æ‰€æœ‰circuitæ ‡æ³¨åˆ—è¡¨
    """
    try:
        return get_circuits_by_feature_service(
            client=client,
            sae_series=globals()['sae_series'],  # å…¨å±€é»˜è®¤å€¼
            sae_name=sae_name,
            layer=layer,
            feature_index=feature_index,
            sae_series_param=sae_series,  # è·¯ç”±å‚æ•°ï¼ˆå¯èƒ½æ˜¯Noneï¼ŒæœåŠ¡å‡½æ•°ä¼šä½¿ç”¨é»˜è®¤å€¼ï¼‰
            feature_type=feature_type,
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è·å–circuitæ ‡æ³¨å¤±è´¥: {str(e)}")


@app.get("/circuit_annotations/{circuit_id}")
def get_circuit_annotation(circuit_id: str):
    """
    è·å–æŒ‡å®šçš„circuitæ ‡æ³¨
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
    
    Returns:
        Circuitæ ‡æ³¨ä¿¡æ¯
    """
    try:
        return get_circuit_annotation_service(
            client=client,
            circuit_id=circuit_id,
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è·å–circuitæ ‡æ³¨å¤±è´¥: {str(e)}")


@app.get("/circuit_annotations")
def list_circuit_annotations(
    sae_combo_id: Optional[str] = None,
    limit: int = 100,
    skip: int = 0,
):
    """
    åˆ—å‡ºæ‰€æœ‰circuitæ ‡æ³¨
    
    Args:
        sae_combo_id: å¯é€‰çš„SAEç»„åˆIDè¿‡æ»¤å™¨
        limit: è¿”å›çš„æœ€å¤§æ•°é‡
        skip: è·³è¿‡çš„æ•°é‡ï¼ˆç”¨äºåˆ†é¡µï¼‰
    
    Returns:
        Circuitæ ‡æ³¨åˆ—è¡¨
    """
    try:
        return list_circuit_annotations_service(
            client=client,
            sae_combo_id=sae_combo_id,
            limit=limit,
            skip=skip,
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºcircuitæ ‡æ³¨å¤±è´¥: {str(e)}")


@app.put("/circuit_annotations/{circuit_id}/interpretation")
def update_circuit_interpretation(circuit_id: str, request: dict):
    """
    æ›´æ–°circuitçš„æ•´ä½“è§£é‡Š
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å« circuit_interpretation å­—æ®µ
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return update_circuit_interpretation_service(
            client=client,
            circuit_id=circuit_id,
            circuit_interpretation=request.get("circuit_interpretation", ""),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°circuitè§£é‡Šå¤±è´¥: {str(e)}")


@app.post("/circuit_annotations/{circuit_id}/features")
def add_feature_to_circuit(circuit_id: str, request: dict):
    """
    å‘circuitæ·»åŠ ä¸€ä¸ªç‰¹å¾
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sae_name: SAEåç§°
            - sae_series: SAEç³»åˆ—ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€sae_seriesï¼‰
            - layer: å±‚å·
            - feature_index: ç‰¹å¾ç´¢å¼•
            - feature_type: ç‰¹å¾ç±»å‹ ("transcoder" æˆ– "lorsa")
            - interpretation: è¯¥ç‰¹å¾çš„è§£é‡Šï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return add_feature_to_circuit_service(
            client=client,
            sae_series=sae_series,
            circuit_id=circuit_id,
            sae_name=request.get("sae_name"),
            layer=request.get("layer"),
            feature_index=request.get("feature_index"),
            feature_type=request.get("feature_type"),
            sae_series_param=request.get("sae_series"),
            interpretation=request.get("interpretation", ""),
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ·»åŠ ç‰¹å¾åˆ°circuitå¤±è´¥: {str(e)}")


@app.delete("/circuit_annotations/{circuit_id}/features")
def remove_feature_from_circuit(circuit_id: str, request: dict):
    """
    ä»circuitä¸­åˆ é™¤ä¸€ä¸ªç‰¹å¾
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sae_name: SAEåç§°
            - sae_series: SAEç³»åˆ—ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€sae_seriesï¼‰
            - layer: å±‚å·
            - feature_index: ç‰¹å¾ç´¢å¼•
            - feature_type: ç‰¹å¾ç±»å‹ ("transcoder" æˆ– "lorsa")
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return remove_feature_from_circuit_service(
            client=client,
            sae_series=sae_series,
            circuit_id=circuit_id,
            sae_name=request.get("sae_name"),
            layer=request.get("layer"),
            feature_index=request.get("feature_index"),
            feature_type=request.get("feature_type"),
            sae_series_param=request.get("sae_series"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ä»circuitåˆ é™¤ç‰¹å¾å¤±è´¥: {str(e)}")


@app.put("/circuit_annotations/{circuit_id}/features/interpretation")
def update_feature_interpretation_in_circuit(circuit_id: str, request: dict):
    """
    æ›´æ–°circuitä¸­æŸä¸ªç‰¹å¾çš„è§£é‡Š
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sae_name: SAEåç§°
            - sae_series: SAEç³»åˆ—ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€sae_seriesï¼‰
            - layer: å±‚å·
            - feature_index: ç‰¹å¾ç´¢å¼•
            - feature_type: ç‰¹å¾ç±»å‹ ("transcoder" æˆ– "lorsa")
            - interpretation: æ–°çš„è§£é‡Šæ–‡æœ¬
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return update_feature_interpretation_in_circuit_service(
            client=client,
            sae_series=sae_series,
            circuit_id=circuit_id,
            sae_name=request.get("sae_name"),
            layer=request.get("layer"),
            feature_index=request.get("feature_index"),
            feature_type=request.get("feature_type"),
            interpretation=request.get("interpretation", ""),
            sae_series_param=request.get("sae_series"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ç‰¹å¾è§£é‡Šå¤±è´¥: {str(e)}")


@app.delete("/circuit_annotations/{circuit_id}")
def delete_circuit_annotation(circuit_id: str):
    """
    åˆ é™¤circuitæ ‡æ³¨
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return delete_circuit_annotation_service(
            client=client,
            circuit_id=circuit_id,
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤circuitæ ‡æ³¨å¤±è´¥: {str(e)}")


@app.post("/circuit_annotations/{circuit_id}/edges")
def add_edge_to_circuit(circuit_id: str, request: dict):
    """
    å‘circuitæ·»åŠ ä¸€æ¡è¾¹
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - source_feature_id: æºfeatureçš„ID
            - target_feature_id: ç›®æ ‡featureçš„ID
            - weight: è¾¹çš„æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º0.0ï¼‰
            - interpretation: å¯é€‰çš„è¾¹è§£é‡Š
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return add_edge_to_circuit_service(
            client=client,
            circuit_id=circuit_id,
            source_feature_id=request.get("source_feature_id"),
            target_feature_id=request.get("target_feature_id"),
            weight=request.get("weight", 0.0),
            interpretation=request.get("interpretation"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ·»åŠ è¾¹åˆ°circuitå¤±è´¥: {str(e)}")


@app.delete("/circuit_annotations/{circuit_id}/edges")
def remove_edge_from_circuit(circuit_id: str, request: dict):
    """
    ä»circuitåˆ é™¤ä¸€æ¡è¾¹
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - source_feature_id: æºfeatureçš„ID
            - target_feature_id: ç›®æ ‡featureçš„ID
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return remove_edge_from_circuit_service(
            client=client,
            circuit_id=circuit_id,
            source_feature_id=request.get("source_feature_id"),
            target_feature_id=request.get("target_feature_id"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ä»circuitåˆ é™¤è¾¹å¤±è´¥: {str(e)}")


@app.put("/circuit_annotations/{circuit_id}/edges")
def update_edge_weight(circuit_id: str, request: dict):
    """
    æ›´æ–°circuitä¸­è¾¹çš„æƒé‡
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - source_feature_id: æºfeatureçš„ID
            - target_feature_id: ç›®æ ‡featureçš„ID
            - weight: æ–°çš„æƒé‡
            - interpretation: å¯é€‰çš„æ–°è¾¹è§£é‡Š
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return update_edge_weight_service(
            client=client,
            circuit_id=circuit_id,
            source_feature_id=request.get("source_feature_id"),
            target_feature_id=request.get("target_feature_id"),
            weight=request.get("weight"),
            interpretation=request.get("interpretation"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°è¾¹æƒé‡å¤±è´¥: {str(e)}")


@app.put("/circuit_annotations/{circuit_id}/features/{feature_id}/level")
def set_feature_level(circuit_id: str, feature_id: str, request: dict):
    """
    è®¾ç½®circuitä¸­featureçš„å±‚çº§
    
    Args:
        circuit_id: Circuitæ ‡æ³¨çš„å”¯ä¸€ID
        feature_id: Featureçš„ID
        request: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - level: Circuitå±‚çº§ï¼ˆç‹¬ç«‹äºlayerï¼Œç”¨äºå¯è§†åŒ–ï¼‰
    
    Returns:
        æˆåŠŸæ¶ˆæ¯
    """
    try:
        return set_feature_level_service(
            client=client,
            circuit_id=circuit_id,
            feature_id=feature_id,
            level=request.get("level"),
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è®¾ç½®featureå±‚çº§å¤±è´¥: {str(e)}")


@app.post("/interaction/analyze_node_interaction")
def analyze_node_interaction_api(request: dict):
    """
    åˆ†æèŠ‚ç‚¹ä¹‹é—´çš„äº¤äº’å½±å“ï¼ˆæ”¯æŒå¤šä¸ªsteering nodeså’Œå¤šä¸ªtarget nodesï¼‰

    è¯·æ±‚ä½“:
    {
        "model_name": "lc0/BT4-1024x15x32h",
        "sae_combo_id": "k_128_e_128",
        "fen": "8/p3kpp1/8/3R1r2/8/4P1Q1/PPr4n/6KR b - - 9 32",
        "steering_nodes": [  # å¯ä»¥æ˜¯å•ä¸ªèŠ‚ç‚¹å¯¹è±¡æˆ–èŠ‚ç‚¹åˆ—è¡¨
            {
            "feature_type": "lorsa",
            "layer": 1,
            "feature": 3026,
            "pos": 48
            }
        ],
        "target_nodes": [  # å¯ä»¥æ˜¯å•ä¸ªèŠ‚ç‚¹å¯¹è±¡æˆ–èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ‰€æœ‰target nodeså¿…é¡»åœ¨æ¯”æ‰€æœ‰steering nodesæ›´é«˜çš„å±‚
            {
            "feature_type": "transcoder",
            "layer": 3,
            "feature": 11305,
            "pos": 34
            }
        ],
        "steering_scale": 2.0
    }

    Returns:
        åŒ…å«äº¤äº’åˆ†æç»“æœçš„å­—å…¸ï¼š
        {
            "steering_scale": float,
            "steering_nodes_count": int,
            "steering_details": list,
            "target_nodes": [
                {
                    "target_node": str,
                    "original_activation": float,
                    "modified_activation": float,
                    "activation_ratio": float,
                    "activation_change": float
                },
                ...
            ]
        }
    """
    try:
        if analyze_node_interaction_impl is None:
            raise HTTPException(status_code=503, detail="Node interaction service not available")
        return analyze_node_interaction_impl(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†æèŠ‚ç‚¹äº¤äº’å¤±è´¥: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"èŠ‚ç‚¹äº¤äº’åˆ†æå¤±è´¥: {str(e)}")
    finally:
        # Clean up
        try:
            if 'model' in locals() and locals()['model'] is not None:
                locals()['model'].reset_hooks()
        except:
            pass


# æ·»åŠ CORSä¸­é—´ä»¶ - å¿…é¡»åœ¨æ‰€æœ‰è·¯ç”±å®šä¹‰ä¹‹å
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
