"""Global weight calculation functions for feature analysis."""
from pathlib import Path
from typing import Dict, List, Tuple

import torch
import einops

from lm_saes.sae import SparseAutoEncoder
from lm_saes import LowRankSparseAttention


# Global cache for max and mean activations
_max_activations_cache: Dict[str, Dict[str, torch.Tensor]] = {}  # combo_key -> {"tc_max": tensor, "lorsa_max": tensor, "tc_mean": tensor, "lorsa_mean": tensor}


def load_max_activations(
    sae_combo_id: str,
    device: str = "cuda",
    get_bt4_sae_combo=None,
    activation_type: str = "max",  # "max" or "mean"
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    åŠ è½½maxæˆ–mean activationsæ•°æ®
    
    Args:
        sae_combo_id: SAEç»„åˆID
        device: è®¾å¤‡ï¼ˆcudaæˆ–cpuï¼‰
        get_bt4_sae_combo: è·å–SAEç»„åˆé…ç½®çš„å‡½æ•°
        activation_type: æ¿€æ´»ç±»å‹ï¼Œ"max" æˆ– "mean"
    
    Returns:
        (tc_activations, lorsa_activations) å…ƒç»„
    """
    global _max_activations_cache
    
    if get_bt4_sae_combo is None:
        raise ValueError("get_bt4_sae_combo function is required")
    
    if activation_type not in ["max", "mean"]:
        raise ValueError(f"activation_typeå¿…é¡»æ˜¯'max'æˆ–'mean'ï¼Œå½“å‰å€¼: {activation_type}")
    
    combo_cfg = get_bt4_sae_combo(sae_combo_id)
    normalized_combo_id = combo_cfg["id"]
    cache_key = f"BT4_{normalized_combo_id}"
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key_tc = f"tc_{activation_type}"
    cache_key_lorsa = f"lorsa_{activation_type}"
    
    if cache_key in _max_activations_cache:
        cached = _max_activations_cache[cache_key]
        if cache_key_tc in cached and cache_key_lorsa in cached:
            return cached[cache_key_tc], cached[cache_key_lorsa]
    
    # å°è¯•ä»æ–‡ä»¶åŠ è½½
    try:
        exp_dir = Path(__file__).parent.parent / "exp" / "44global_weight"
        
        if activation_type == "max":
            tc_path = exp_dir / "tc_feature_max_activations.pt"
            lorsa_path = exp_dir / "lorsa_feature_max_activations.pt"
        else:  # mean
            tc_path = exp_dir / "tc_feature_mean_activations.pt"
            lorsa_path = exp_dir / "lorsa_feature_mean_activations.pt"
        
        if tc_path.exists() and lorsa_path.exists():
            tc_acts = torch.load(tc_path, map_location=device)
            lorsa_acts = torch.load(lorsa_path, map_location=device)
            
            if activation_type == "max":
                # å¯¹äºmax activationsï¼Œä½¿ç”¨å¹³å‡å€¼ï¼ˆæ ¹æ®notebookä¸­çš„é€»è¾‘ï¼‰
                # max activations æ–‡ä»¶é€šå¸¸æ˜¯3ç»´çš„ [n_layers, n_samples, n_features]
                # éœ€è¦å–å¹³å‡å€¼å¾—åˆ° [n_layers, n_features]ï¼Œç„¶åæ‰©å±•ä¸º [n_layers, 16384]
                if tc_acts.dim() == 3:
                    tc_mean = torch.mean(tc_acts, dim=1)  # [n_layers, n_features]
                    tc_acts = tc_mean[:, None].repeat(1, 16384)  # [n_layers, 16384]
                elif tc_acts.dim() == 2:
                    # å¦‚æœå·²ç»æ˜¯2ç»´çš„ï¼Œç›´æ¥æ‰©å±•
                    tc_acts = tc_acts[:, None].repeat(1, 16384)
                
                if lorsa_acts.dim() == 3:
                    lorsa_mean = torch.mean(lorsa_acts, dim=1)  # [n_layers, n_features]
                    lorsa_acts = lorsa_mean[:, None].repeat(1, 16384)  # [n_layers, 16384]
                elif lorsa_acts.dim() == 2:
                    # å¦‚æœå·²ç»æ˜¯2ç»´çš„ï¼Œç›´æ¥æ‰©å±•
                    lorsa_acts = lorsa_acts[:, None].repeat(1, 16384)
            else:  # mean
                # å¯¹äºmean activationsï¼Œæ ¹æ®notebookï¼Œæ–‡ä»¶åŠ è½½å‡ºæ¥åº”è¯¥æ˜¯ [15, 16384] çš„å½¢çŠ¶
                # ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦å†å–å¹³å‡å€¼æˆ–æ‰©å±•
                if tc_acts.dim() == 2:
                    # å¦‚æœå·²ç»æ˜¯ [n_layers, n_features] çš„å½¢çŠ¶ï¼Œç›´æ¥ä½¿ç”¨
                    # æ ¹æ®notebookï¼Œåº”è¯¥æ˜¯ [15, 16384]ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
                    pass
                elif tc_acts.dim() == 3:
                    # å¦‚æœæ˜¯3ç»´çš„ï¼Œå–å¹³å‡å€¼ï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
                    print(f"âš ï¸ è­¦å‘Š: mean activations æ–‡ä»¶æ˜¯3ç»´çš„ï¼Œå°†å–å¹³å‡å€¼")
                    tc_mean = torch.mean(tc_acts, dim=1)  # [n_layers, n_features]
                    tc_acts = tc_mean
                    if tc_acts.shape[1] != 16384:
                        # å¦‚æœç‰¹å¾æ•°ä¸æ˜¯16384ï¼Œéœ€è¦æ‰©å±•
                        tc_acts = tc_acts[:, None].repeat(1, 16384)
                
                if lorsa_acts.dim() == 2:
                    # å¦‚æœå·²ç»æ˜¯ [n_layers, n_features] çš„å½¢çŠ¶ï¼Œç›´æ¥ä½¿ç”¨
                    # æ ¹æ®notebookï¼Œåº”è¯¥æ˜¯ [15, 16384]ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
                    pass
                elif lorsa_acts.dim() == 3:
                    # å¦‚æœæ˜¯3ç»´çš„ï¼Œå–å¹³å‡å€¼ï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
                    print(f"âš ï¸ è­¦å‘Š: mean activations æ–‡ä»¶æ˜¯3ç»´çš„ï¼Œå°†å–å¹³å‡å€¼")
                    lorsa_mean = torch.mean(lorsa_acts, dim=1)  # [n_layers, n_features]
                    lorsa_acts = lorsa_mean
                    if lorsa_acts.shape[1] != 16384:
                        # å¦‚æœç‰¹å¾æ•°ä¸æ˜¯16384ï¼Œéœ€è¦æ‰©å±•
                        lorsa_acts = lorsa_acts[:, None].repeat(1, 16384)
            
            # æ›´æ–°ç¼“å­˜
            if cache_key not in _max_activations_cache:
                _max_activations_cache[cache_key] = {}
            _max_activations_cache[cache_key][cache_key_tc] = tc_acts
            _max_activations_cache[cache_key][cache_key_lorsa] = lorsa_acts
            
            print(f"âœ… åŠ è½½{activation_type} activationsæ•°æ®: {cache_key}")
            return tc_acts, lorsa_acts
    except Exception as e:
        print(f"âš ï¸ åŠ è½½{activation_type} activationså¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼: {e}")
    
    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºé»˜è®¤å€¼ï¼ˆä½¿ç”¨1.0ä½œä¸ºå ä½ç¬¦ï¼‰
    n_layers = 15
    n_features = 16384
    tc_default = torch.ones(n_layers, n_features, device=device, dtype=torch.float32)
    lorsa_default = torch.ones(n_layers, n_features, device=device, dtype=torch.float32)
    
    # æ›´æ–°ç¼“å­˜
    if cache_key not in _max_activations_cache:
        _max_activations_cache[cache_key] = {}
    _max_activations_cache[cache_key][cache_key_tc] = tc_default
    _max_activations_cache[cache_key][cache_key_lorsa] = lorsa_default
    
    return tc_default, lorsa_default


def construct_topk(feature_list: List[Tuple[str, float]], k: int) -> List[Tuple[str, float]]:
    """é€‰æ‹©top kç‰¹å¾"""
    sorted_features = sorted(feature_list, key=lambda x: x[1], reverse=True)
    return sorted_features[:k]


def construct_name(global_weights: torch.Tensor, prefix: str, k: int) -> List[Tuple[str, float]]:
    """æ„å»ºç‰¹å¾åç§°åˆ—è¡¨"""
    n_features = global_weights.shape[0]
    feature_list = [(prefix.format(i), global_weights[i].item()) for i in range(n_features)]
    return construct_topk(feature_list, k)


def tc_global_weight_in(
    transcoders: Dict[int, SparseAutoEncoder],
    lorsas: List[LowRankSparseAttention],
    layer_idx: int,
    feature_idx: int,
    tc_activations: torch.Tensor,
    lorsa_activations: torch.Tensor,
    k: int = 100,
    layer_filter: List[int] | None = None,
) -> List[Tuple[str, float]]:
    """è®¡ç®—TC featureçš„è¾“å…¥å…¨å±€æƒé‡"""
    if layer_filter is not None:
        print(f"ğŸ” tc_global_weight_in: layer_filter={layer_filter}")
    # è·å–encoderå‘é‡
    f_enc = transcoders[layer_idx].W_E[:, feature_idx]  # [d_model]

    feature_list_tc = []
    # TC 0 ~ layer_idx-1
    for i in range(layer_idx):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… tc_global_weight_in: å¤„ç†TCå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        # è·å–decoderçŸ©é˜µï¼ˆå¸¦activationsæƒé‡ï¼‰
        f_dec = transcoders[i].W_D * tc_activations[i, :, None]  # [d_sae, d_model]
        V = einops.einsum(f_enc, f_dec, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_tc.extend(construct_name(V, f"BT4_tc_L{i}M_k30_e16#{{}}", k=k))

    feature_list_lorsa = []
    # LoRSA 0 ~ layer_idx
    for i in range(layer_idx + 1):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… tc_global_weight_in: å¤„ç†LoRSAå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        # è·å–decoderçŸ©é˜µï¼ˆå¸¦activationsæƒé‡ï¼‰
        f_dec = lorsas[i].W_O * lorsa_activations[i, :, None]  # [d_sae, d_model]
        V = einops.einsum(f_enc, f_dec, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_lorsa.extend(construct_name(V, f"BT4_lorsa_L{i}A_k30_e16#{{}}", k=k))

    feature_list_tc = construct_topk(feature_list_tc, k)
    feature_list_lorsa = construct_topk(feature_list_lorsa, k)
    return feature_list_tc + feature_list_lorsa


def lorsa_global_weight_in(
    transcoders: Dict[int, SparseAutoEncoder],
    lorsas: List[LowRankSparseAttention],
    layer_idx: int,
    feature_idx: int,
    tc_activations: torch.Tensor,
    lorsa_activations: torch.Tensor,
    k: int = 100,
    layer_filter: List[int] | None = None,
) -> List[Tuple[str, float]]:
    """è®¡ç®—LoRSA featureçš„è¾“å…¥å…¨å±€æƒé‡"""
    if layer_filter is not None:
        print(f"ğŸ” lorsa_global_weight_in: layer_filter={layer_filter}")
    # è·å–encoderå‘é‡
    f_enc = lorsas[layer_idx].W_V[feature_idx, :]  # [d_model]

    feature_list_tc = []
    # TC 0 ~ layer_idx-1
    for i in range(layer_idx):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… lorsa_global_weight_in: å¤„ç†TCå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        f_dec = transcoders[i].W_D * tc_activations[i, :, None]  # [d_sae, d_model]
        V = einops.einsum(f_enc, f_dec, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_tc.extend(construct_name(V, f"BT4_tc_L{i}M_k30_e16#{{}}", k=k))

    feature_list_lorsa = []
    # LoRSA 0 ~ layer_idx-1
    for i in range(layer_idx):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… lorsa_global_weight_in: å¤„ç†LoRSAå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        f_dec = lorsas[i].W_O * lorsa_activations[i, :, None]  # [d_sae, d_model]
        V = einops.einsum(f_enc, f_dec, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_lorsa.extend(construct_name(V, f"BT4_lorsa_L{i}A_k30_e16#{{}}", k=k))

    feature_list_tc = construct_topk(feature_list_tc, k)
    feature_list_lorsa = construct_topk(feature_list_lorsa, k)
    return feature_list_tc + feature_list_lorsa


def tc_global_weight_out(
    transcoders: Dict[int, SparseAutoEncoder],
    lorsas: List[LowRankSparseAttention],
    layer_idx: int,
    feature_idx: int,
    tc_activations: torch.Tensor,
    lorsa_activations: torch.Tensor,
    k: int = 100,
    layer_filter: List[int] | None = None,
) -> List[Tuple[str, float]]:
    """è®¡ç®—TC featureçš„è¾“å‡ºå…¨å±€æƒé‡"""
    if layer_filter is not None:
        print(f"ğŸ” tc_global_weight_out: layer_filter={layer_filter}")
    # è·å–decoderå‘é‡ï¼ˆå¸¦activationsæƒé‡ï¼‰
    f_dec = transcoders[layer_idx].W_D[feature_idx, :] * tc_activations[layer_idx, feature_idx]  # [d_model]

    feature_list_tc = []
    for i in range(layer_idx + 1, len(transcoders)):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… tc_global_weight_out: å¤„ç†TCå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        f_enc = transcoders[i].W_D * tc_activations[i, :, None]  # [d_sae, d_model]
        V = einops.einsum(f_dec, f_enc, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_tc.extend(construct_name(V, f"BT4_tc_L{i}M_k30_e16#{{}}", k=k))

    feature_list_lorsa = []
    for i in range(layer_idx + 1, len(transcoders)):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… tc_global_weight_out: å¤„ç†LoRSAå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        f_enc = lorsas[i].W_V  # [d_sae, d_model]
        V = einops.einsum(f_dec, f_enc, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_lorsa.extend(construct_name(V, f"BT4_lorsa_L{i}A_k30_e16#{{}}", k=k))

    feature_list_tc = construct_topk(feature_list_tc, k)
    feature_list_lorsa = construct_topk(feature_list_lorsa, k)
    return feature_list_tc + feature_list_lorsa


def lorsa_global_weight_out(
    transcoders: Dict[int, SparseAutoEncoder],
    lorsas: List[LowRankSparseAttention],
    layer_idx: int,
    feature_idx: int,
    tc_activations: torch.Tensor,
    lorsa_activations: torch.Tensor,
    k: int = 100,
    layer_filter: List[int] | None = None,
) -> List[Tuple[str, float]]:
    """è®¡ç®—LoRSA featureçš„è¾“å‡ºå…¨å±€æƒé‡"""
    if layer_filter is not None:
        print(f"ğŸ” lorsa_global_weight_out: layer_filter={layer_filter}")
    # è·å–decoderå‘é‡ï¼ˆå¸¦activationsæƒé‡ï¼‰
    f_dec = lorsas[layer_idx].W_O[feature_idx, :] * lorsa_activations[layer_idx, feature_idx]  # [d_model]

    feature_list_tc = []
    # TC layer_idx ~ n_layers-1
    for i in range(layer_idx, len(transcoders)):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… lorsa_global_weight_out: å¤„ç†TCå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        f_enc = transcoders[i].W_D * tc_activations[i, :, None]  # [d_sae, d_model]
        V = einops.einsum(f_dec, f_enc, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_tc.extend(construct_name(V, f"BT4_tc_L{i}M_k30_e16#{{}}", k=k))

    feature_list_lorsa = []
    # LoRSA layer_idx+1 ~ n_layers-1
    for i in range(layer_idx + 1, len(transcoders)):
        # å¦‚æœæœ‰å±‚è¿‡æ»¤å™¨ï¼ŒåªåŒ…å«æŒ‡å®šå±‚
        if layer_filter is not None and i not in layer_filter:
            continue
        if layer_filter is not None:  # è°ƒè¯•ä¿¡æ¯
            print(f"âœ… lorsa_global_weight_out: å¤„ç†LoRSAå±‚{i} (è¿‡æ»¤å™¨: {layer_filter})")
        f_enc = lorsas[i].W_V  # [d_sae, d_model]
        V = einops.einsum(f_dec, f_enc, "d_model, d_sae d_model -> d_sae")  # [d_sae]
        feature_list_lorsa.extend(construct_name(V, f"BT4_lorsa_L{i}A_k30_e16#{{}}", k=k))

    feature_list_tc = construct_topk(feature_list_tc, k)
    feature_list_lorsa = construct_topk(feature_list_lorsa, k)
    return feature_list_tc + feature_list_lorsa




