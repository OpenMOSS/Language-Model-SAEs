
from src.lm_saes.config import MongoDBConfig
from src.lm_saes.database import MongoClient, FeatureRecord, SAERecord, DatasetRecord, ModelRecord
from src.lm_saes.resource_loaders import load_dataset_shard
from typing import Optional, Dict, List, Tuple, Any
from datasets import load_from_disk
import os
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

DATASET_PATH = "/inspire/hdd/global_user/hezhengfu-240208120186/data/rlin_data/Chess/chess_master_data"

def get_fen_from_context_idx(context_idx: int, dataset_path: str = DATASET_PATH,
                           shard_idx: int = 0, n_shards: int = 1, mongo_client=None, dataset_name: str = "master") -> Optional[str]:
    try:
        if mongo_client:
            cfg = mongo_client.get_dataset_cfg(dataset_name)
            if cfg:
                dataset = load_dataset_shard(cfg, shard_idx, n_shards)
                if context_idx < len(dataset):
                    fen_data = dataset[context_idx]
                    if hasattr(fen_data, 'get') and callable(getattr(fen_data, 'get', None)):
                        fen = fen_data.get('fen')
                    elif isinstance(fen_data, dict):
                        fen = fen_data.get('fen')
                    else:
                        fen = getattr(fen_data, 'fen', None)
                    return fen
        else:
            if not os.path.exists(dataset_path):
                return None
            dataset = load_from_disk(dataset_path)
            if context_idx >= len(dataset):
                return None
            fen_data = dataset[context_idx]
            if hasattr(fen_data, 'get') and callable(getattr(fen_data, 'get', None)):
                fen = fen_data.get('fen')
            elif isinstance(fen_data, dict):
                fen = fen_data.get('fen')
            else:
                fen = getattr(fen_data, 'fen', None)
            if isinstance(fen, str):
                return fen
    except Exception:
        return None

def get_feature_top_activation(mongo_client, layer: int, feature_id: int, feature_type: str, sae_series: str = "BT4-exp128", analysis_name: str = "default") -> Optional[Dict]:
    """
    Ëé∑ÂèñÊåáÂÆö feature ÁöÑ top activationÔºàÊúÄÂ§ßÊøÄÊ¥ªÂÄºÔºâ‰ø°ÊÅØ„ÄÇ

    Args:
        mongo_client: MongoDBÂÆ¢Êà∑Á´Ø
        layer: Â±ÇÁ¥¢Âºï
        feature_id: featureÁ¥¢Âºï
        feature_type: featureÁ±ªÂûã ("transcoder" Êàñ "lorsa")
        sae_series: SAEÁ≥ªÂàóÔºåÈªòËÆ§‰∏∫ "BT4-exp128"
        analysis_name: ÂàÜÊûêÂêçÁß∞ÔºåÈªòËÆ§‰∏∫ "default"

    Returns:
        ÂåÖÂê´ top activation ‰ø°ÊÅØÁöÑÂ≠óÂÖ∏ÔºåÂ¶ÇÊûúÊú™ÊâæÂà∞ÂàôËøîÂõû None„ÄÇ
        Â≠óÂÖ∏ÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµÔºö
        - activation_value: ÊøÄÊ¥ªÂÄº
        - context_idx: contextÁ¥¢Âºï
        - position: ‰ΩçÁΩÆÁ¥¢ÂºïÔºàÂ¶ÇÊûúÊúâÔºâ
        - dataset_name: Êï∞ÊçÆÈõÜÂêçÁß∞
        - shard_idx: ÂàÜÁâáÁ¥¢ÂºïÔºàÂ¶ÇÊûúÊúâÔºâ
        - n_shards: ÂàÜÁâáÊÄªÊï∞ÔºàÂ¶ÇÊûúÊúâÔºâ
        - fen: FENÂ≠óÁ¨¶‰∏≤ÔºàÂ¶ÇÊûúÂèØËé∑ÂèñÔºâ
    """
    # ÊûÑÂª∫ SAE ÂêçÁß∞
    sae_name = (
        f"BT4_lorsa_L{layer}A_k30_e16" if feature_type == "lorsa"
        else f"BT4_tc_L{layer}M_k30_e16" if feature_type == "transcoder"
        else None
    )
    if sae_name is None:
        raise ValueError(f"Unknown feature_type: {feature_type}")

    # Ëé∑Âèñ feature record
    fr = mongo_client.get_feature(sae_name=sae_name, sae_series=sae_series, index=feature_id)
    if not fr or not fr.analyses:
        return None

    # Ëé∑ÂèñÊåáÂÆöÁöÑ analysis
    analysis = next((a for a in fr.analyses if a.name == analysis_name), fr.analyses[0])
    if not analysis.samplings:
        return None

    # ‰ΩøÁî®Á¨¨‰∏Ä‰∏™ samplingÔºàÈÄöÂ∏∏ÂåÖÂê´ top activationsÔºâ
    sampling = analysis.samplings[0]
    feature_values = np.asarray(sampling.feature_acts_values)
    
    if len(feature_values) == 0:
        return None

    # ÊâæÂà∞ÊúÄÂ§ßÊøÄÊ¥ªÂÄºÔºàtop activationÔºâ
    max_idx = np.argmax(np.abs(feature_values))
    activation_value = float(feature_values[max_idx])

    # Ëé∑ÂèñÂØπÂ∫îÁöÑ context_idx
    context_indices = np.asarray(sampling.context_idx)
    dataset_names = sampling.dataset_name
    shard_idx = getattr(sampling, 'shard_idx', None)
    n_shards = getattr(sampling, 'n_shards', None)
    positions = getattr(sampling, 'feature_acts_indices', None)

    # Ëß£Êûê‰ΩçÁΩÆ‰ø°ÊÅØ
    context_idx_idx = max_idx
    position = None
    if positions is not None:
        if isinstance(positions, np.ndarray) and positions.ndim == 2:
            context_idx_idx = int(positions[0, max_idx])
            position = int(positions[1, max_idx])
        elif isinstance(positions, (list, tuple)) and len(positions) >= 2:
            context_idx_idx = int(positions[0][max_idx])
            position = int(positions[1][max_idx])

    # Ëé∑ÂèñÊï∞ÊçÆÈõÜ‰ø°ÊÅØ
    context_idx = int(context_indices[context_idx_idx])
    dataset_name = str(dataset_names[context_idx_idx]) if context_idx_idx < len(dataset_names) else "master"
    shard_idx_val = (
        int(shard_idx[context_idx_idx])
        if shard_idx is not None
        and isinstance(shard_idx, (np.ndarray, list, tuple))
        and context_idx_idx < len(shard_idx)
        else None
    )
    n_shards_val = (
        int(n_shards[context_idx_idx])
        if n_shards is not None
        and isinstance(n_shards, (np.ndarray, list, tuple))
        and context_idx_idx < len(n_shards)
        else None
    )

    # Â∞ùËØïËé∑Âèñ FEN
    fen = None
    try:
        if shard_idx_val is not None and n_shards_val is not None:
            cfg = mongo_client.get_dataset_cfg(dataset_name)
            if cfg:
                dataset = load_dataset_shard(cfg, shard_idx_val, n_shards_val)
                if context_idx < len(dataset):
                    fen_data = dataset[context_idx]
                    if hasattr(fen_data, 'get') and callable(getattr(fen_data, 'get', None)):
                        fen = fen_data.get('fen')
                    elif isinstance(fen_data, dict):
                        fen = fen_data.get('fen')
                    else:
                        fen = getattr(fen_data, 'fen', None)
        else:
            fen = get_fen_from_context_idx(context_idx, DATASET_PATH, mongo_client=mongo_client)
    except Exception:
        pass

    return {
        "activation_value": activation_value,
        "context_idx": context_idx,
        "position": position,
        "dataset_name": dataset_name,
        "shard_idx": shard_idx_val,
        "n_shards": n_shards_val,
        "fen": fen,
    }

def get_feature_top_activation_value(mongo_client, layer: int, feature_id: int, feature_type: str, sae_series: str = "BT4-exp128", analysis_name: str = "default") -> Optional[float]:
    """
    Ëé∑ÂèñÊåáÂÆö feature ÁöÑ top activation ÂÄºÔºàÊúÄÂ§ßÊøÄÊ¥ªÂÄºÔºâ„ÄÇ

    Args:
        mongo_client: MongoDBÂÆ¢Êà∑Á´Ø
        layer: Â±ÇÁ¥¢Âºï
        feature_id: featureÁ¥¢Âºï
        feature_type: featureÁ±ªÂûã ("transcoder" Êàñ "lorsa")
        sae_series: SAEÁ≥ªÂàóÔºåÈªòËÆ§‰∏∫ "BT4-exp128"
        analysis_name: ÂàÜÊûêÂêçÁß∞ÔºåÈªòËÆ§‰∏∫ "default"

    Returns:
        ÊúÄÂ§ßÁöÑÊøÄÊ¥ªÂÄºÔºàfloatÔºâÔºåÂ¶ÇÊûúÊú™ÊâæÂà∞ÂàôËøîÂõû None„ÄÇ
    """
    # ÊûÑÂª∫ SAE ÂêçÁß∞
    sae_name = (
        f"BT4_lorsa_L{layer}A_k30_e16" if feature_type == "lorsa"
        else f"BT4_tc_L{layer}M_k30_e16" if feature_type == "transcoder"
        else None
    )
    if sae_name is None:
        raise ValueError(f"Unknown feature_type: {feature_type}")
    
    # Ëé∑Âèñ feature record
    fr = mongo_client.get_feature(sae_name=sae_name, sae_series=sae_series, index=feature_id)
    if not fr or not fr.analyses:
        return None
    
    # Ëé∑ÂèñÊåáÂÆöÁöÑ analysis
    analysis = next((a for a in fr.analyses if a.name == analysis_name), fr.analyses[0])
    if not analysis.samplings:
        return None
    
    # ‰ΩøÁî®Á¨¨‰∏Ä‰∏™ samplingÔºàÈÄöÂ∏∏ÂåÖÂê´ top activationsÔºâ
    sampling = analysis.samplings[0]
    feature_values = np.asarray(sampling.feature_acts_values)
    
    if len(feature_values) == 0:
        return None
    
    # ÊâæÂà∞ÊúÄÂ§ßÊøÄÊ¥ªÂÄºÔºàtop activationÔºâ
    max_idx = np.argmax(np.abs(feature_values))
    activation_value = float(feature_values[max_idx])
    
    return activation_value


def get_feature_top_activations(mongo_client, layer_or_name, feature_id_or_index, feature_type=None, sae_series: str = "BT4-exp128", analysis_name: str = "default") -> List[Dict]:
    pass


def get_feature_top_fen(mongo_client, layer: int, feature_id: int, feature_type: str, sae_series: str = "BT4-exp128", analysis_name: str = "default") -> List[str]:
    sae_name = f"BT4_lorsa_L{layer}A_k30_e16" if feature_type == "lorsa" else f"BT4_tc_L{layer}M_k30_e16" if feature_type == "transcoder" else None
    if sae_name is None:
        raise ValueError(f"Unknown feature_type: {feature_type}")

    fr = mongo_client.get_feature(sae_name=sae_name, sae_series=sae_series, index=feature_id)
    if not fr or not fr.analyses:
        return []
    analysis = next((a for a in fr.analyses if a.name == analysis_name), fr.analyses[0])
    if not analysis.samplings:
        return []

    unique_fens = set()
    context_idx_to_fen = {}

    for sampling in analysis.samplings:
        context_indices = sampling.context_idx
        shard_idx = getattr(sampling, 'shard_idx', None)
        n_shards = getattr(sampling, 'n_shards', None)
        dataset_names = getattr(sampling, 'dataset_name', None)

        use_sharding = (shard_idx is not None and n_shards is not None and
                       hasattr(shard_idx, '__len__') and len(shard_idx) > 0)

        if use_sharding:
            shard_groups = {}
            for i, context_idx in enumerate(context_indices):
                context_idx = int(context_idx)
                current_shard_idx = shard_idx[i] if i < len(shard_idx) else shard_idx[0]
                current_n_shards = n_shards[i] if i < len(n_shards) else n_shards[0]
                current_dataset_name = dataset_names[i] if dataset_names and i < len(dataset_names) else "master"

                key = (current_shard_idx, current_n_shards, current_dataset_name)
                if key not in shard_groups:
                    shard_groups[key] = []
                shard_groups[key].append(context_idx)

            for (shard_idx_val, n_shards_val, dataset_name_val), context_indices_group in shard_groups.items():
                try:
                    cfg = mongo_client.get_dataset_cfg(dataset_name_val)
                    if cfg:
                        dataset = load_dataset_shard(cfg, shard_idx_val, n_shards_val)
                        for context_idx in context_indices_group:
                            if context_idx not in context_idx_to_fen:
                                if context_idx < len(dataset):
                                    fen_data = dataset[context_idx]
                                    if hasattr(fen_data, 'get') and callable(getattr(fen_data, 'get', None)):
                                        fen = fen_data.get('fen')
                                    elif isinstance(fen_data, dict):
                                        fen = fen_data.get('fen')
                                    else:
                                        fen = getattr(fen_data, 'fen', None)
                                    context_idx_to_fen[context_idx] = fen
                                else:
                                    context_idx_to_fen[context_idx] = None
                except Exception:
                    for context_idx in context_indices_group:
                        if context_idx not in context_idx_to_fen:
                            context_idx_to_fen[context_idx] = None
        else:
            for context_idx in context_indices:
                context_idx = int(context_idx)
                if context_idx not in context_idx_to_fen:
                    fen = get_fen_from_context_idx(context_idx, DATASET_PATH, mongo_client=mongo_client)
                    context_idx_to_fen[context_idx] = fen

        for context_idx in context_indices:
            context_idx = int(context_idx)
            fen = context_idx_to_fen.get(context_idx)
            if fen is not None:
                unique_fens.add(fen)

    return list(unique_fens)


def get_feature_top_fen_batch(mongo_client, features_list, sae_series: str = "BT4-exp128", analysis_name: str = "default") -> Dict[Tuple[str, int, int], List[str]]:
    """
    ÊâπÈáèËé∑ÂèñÂ§ö‰∏™featuresÁöÑtop FENÂàóË°®

    Args:
        mongo_client: MongoDBÂÆ¢Êà∑Á´Ø
        features_list: [(feature_type, layer, feature_id), ...]
        sae_series: SAEÁ≥ªÂàó
        analysis_name: ÂàÜÊûêÂêçÁß∞

    Returns:
        Dict[(feature_type, layer, feature_id), List[str]]: featureÂà∞FENÂàóË°®ÁöÑÊò†Â∞Ñ
    """
    print(f"üîç get_feature_top_fen_batch ÂºÄÂßãÊâßË°å")
    print(f"   ËæìÂÖ•ÂèÇÊï∞: sae_series='{sae_series}', analysis_name='{analysis_name}'")
    print(f"   features_list ÈïøÂ∫¶: {len(features_list)}")
    if features_list:
        print(f"   features_list Á§∫‰æã: {features_list[:3]}")

    if not features_list:
        print("‚ö†Ô∏è features_list ‰∏∫Á©∫ÔºåÁõ¥Êé•ËøîÂõûÁ©∫Â≠óÂÖ∏")
        return {}

    result = {}

    # ÊåâlayerÂíåfeature_typeÂàÜÁªÑÔºå‰æø‰∫éÊâπÈáèÊü•ËØ¢
    features_by_sae = {}
    for feature_type, layer, feature_id in features_list:
        sae_name = f"BT4_lorsa_L{layer}A_k30_e16" if feature_type == "lorsa" else f"BT4_tc_L{layer}M_k30_e16" if feature_type == "transcoder" else None
        if sae_name is None:
            print(f"‚ö†Ô∏è Êú™Áü•ÁöÑfeature_type: {feature_type}ÔºåË∑≥Ëøá")
            continue
        if sae_name not in features_by_sae:
            features_by_sae[sae_name] = []
        features_by_sae[sae_name].append((feature_type, layer, feature_id, feature_id))

    print(f"üìä ÊåâSAEÂàÜÁªÑÂÆåÊàê:")
    for sae_name, feature_list in features_by_sae.items():
        print(f"   {sae_name}: {len(feature_list)} ‰∏™features")

    # ÊâπÈáèÊü•ËØ¢features
    print(f"üöÄ ÂºÄÂßãÊâπÈáèÂ§ÑÁêÜ {len(features_by_sae)} ‰∏™SAE...")
    for sae_name, feature_info_list in tqdm(features_by_sae.items(), desc="Â§ÑÁêÜSAE"):
        print(f"Â§ÑÁêÜSAE: {sae_name}, featuresÊï∞Èáè: {len(feature_info_list)}")
        try:
            # Âπ∂Ë°åÊü•ËØ¢SAEÁöÑÊâÄÊúâfeatures
            sae_features = {}
            print(f"  Âπ∂Ë°åÊü•ËØ¢ {len(feature_info_list)} ‰∏™features...")

            def query_single_feature(args):
                feature_type, layer, feature_id, index = args
                try:
                    fr = mongo_client.get_feature(sae_name=sae_name, sae_series=sae_series, index=index)
                    if fr and fr.analyses:
                        analysis = next((a for a in fr.analyses if a.name == analysis_name), fr.analyses[0])
                        if analysis.samplings:
                            return (feature_type, layer, feature_id), analysis
                except Exception:
                    pass
                return None

            # ‰ΩøÁî®Á∫øÁ®ãÊ±†Âπ∂Ë°åÊü•ËØ¢
            max_workers = min(20, len(feature_info_list))  # ÊúÄÂ§ö20‰∏™Âπ∂ÂèëÁ∫øÁ®ã
            try:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(query_single_feature, info) for info in feature_info_list]
                    for future in tqdm(as_completed(futures), total=len(futures), desc=f"Êü•ËØ¢{sae_name}"):
                        query_result = future.result()
                        if query_result:
                            key, analysis = query_result
                            sae_features[key] = analysis
            except Exception as e:
                print(f"Âπ∂Ë°åÊü•ËØ¢Â§±Ë¥•ÔºåÂõûÈÄÄÂà∞È°∫Â∫èÊü•ËØ¢: {e}")
                # ÂõûÈÄÄÂà∞È°∫Â∫èÊü•ËØ¢
                for info in tqdm(feature_info_list, desc=f"È°∫Â∫èÊü•ËØ¢{sae_name}"):
                    query_result = query_single_feature(info)
                    if query_result:
                        key, analysis = query_result
                        sae_features[key] = analysis

            print(f"  SAE {sae_name} ÊàêÂäüËé∑Âèñ {len(sae_features)} ‰∏™features")

            # Â§ÑÁêÜËøô‰∏™SAEÁöÑÊâÄÊúâfeatures
            if sae_features:
                _process_sae_features_batch(mongo_client, sae_features, result)

        except Exception as e:
            print(f"Â§ÑÁêÜSAE {sae_name} Â§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"üéâ get_feature_top_fen_batch ÊâßË°åÂÆåÊàê")
    print(f"   ËøîÂõûÁªìÊûúÂåÖÂê´ {len(result)} ‰∏™features")
    total_fens = sum(len(fen_list) for fen_list in result.values())
    print(f"   ÊÄªÂÖ±Êî∂ÈõÜ‰∫Ü {total_fens} ‰∏™ÂîØ‰∏ÄFEN")
    if result:
        avg_fens = total_fens / len(result)
        print(f"   Âπ≥ÂùáÊØè‰∏™feature {avg_fens:.1f} ‰∏™FEN")

    return result


def _process_sae_features_batch(mongo_client, sae_features, result):
    """Â§ÑÁêÜÂçï‰∏™SAEÁöÑÊâÄÊúâfeaturesÁöÑÊâπÈáèFENÊèêÂèñ"""
    print(f"üîß _process_sae_features_batch ÂºÄÂßãÊâßË°å")
    print(f"   sae_features Êï∞Èáè: {len(sae_features)}")
    print(f"   result ÂΩìÂâçÂ§ßÂ∞è: {len(result)}")

    if not sae_features:
        print("‚ö†Ô∏è sae_features ‰∏∫Á©∫ÔºåÁõ¥Êé•ËøîÂõû")
        return

    # Êî∂ÈõÜÊâÄÊúâÈúÄË¶ÅÁöÑcontext_idx
    all_context_indices = set()
    feature_to_contexts = {}

    print("üìã Êî∂ÈõÜcontext indices...")
    for (feature_type, layer, feature_id), analysis in sae_features.items():
        context_indices = set()
        for sampling in analysis.samplings:
            for context_idx in sampling.context_idx:
                context_indices.add(int(context_idx))

        feature_to_contexts[(feature_type, layer, feature_id)] = context_indices
        all_context_indices.update(context_indices)

    print(f"   Êî∂ÈõÜÂÆåÊàê: {len(feature_to_contexts)} ‰∏™features, {len(all_context_indices)} ‰∏™ÂîØ‰∏Äcontext indices")
    print(f"   context indices ËåÉÂõ¥: {min(all_context_indices)} - {max(all_context_indices)}")

    if not all_context_indices:
        print("‚ö†Ô∏è Ê≤°Êúâcontext indicesÔºåÁõ¥Êé•ËøîÂõû")
        return

    # ÊåâÂàÜÁâáÂàÜÁªÑcontext_idx
    shard_groups = {}
    context_to_shard_info = {}

    # ‰∏∫ÊØè‰∏™featureÈÅçÂéÜÂÖ∂ÊâÄÊúâsamplingsÔºåÊî∂ÈõÜÂàÜÁâá‰ø°ÊÅØ
    for (feature_type, layer, feature_id), analysis in sae_features.items():
        for sampling in analysis.samplings:
            context_indices = sampling.context_idx
            shard_idx = getattr(sampling, 'shard_idx', None)
            n_shards = getattr(sampling, 'n_shards', None)
            dataset_names = getattr(sampling, 'dataset_name', None)

            use_sharding = (shard_idx is not None and n_shards is not None and
                           hasattr(shard_idx, '__len__') and len(shard_idx) > 0)

            if use_sharding:
                for i, context_idx in enumerate(context_indices):
                    context_idx = int(context_idx)
                    if context_idx in all_context_indices:
                        current_shard_idx = shard_idx[i] if i < len(shard_idx) else shard_idx[0]
                        current_n_shards = n_shards[i] if i < len(n_shards) else n_shards[0]
                        current_dataset_name = dataset_names[i] if dataset_names and i < len(dataset_names) else "master"

                        key = (current_shard_idx, current_n_shards, current_dataset_name)
                        if key not in shard_groups:
                            shard_groups[key] = []
                        if context_idx not in shard_groups[key]:
                            shard_groups[key].append(context_idx)
                        context_to_shard_info[context_idx] = key

    print(f"üì¶ ÂàÜÁâáÂàÜÁªÑÂÆåÊàê: {len(shard_groups)} ‰∏™ÂàÜÁâágroups")
    for (shard_idx, n_shards, dataset_name), indices in list(shard_groups.items())[:3]:  # Âè™ÊòæÁ§∫Ââç3‰∏™
        print(f"   ÂàÜÁâá ({shard_idx}/{n_shards}, {dataset_name}): {len(indices)} ‰∏™context indices")

    # ÊâπÈáèÂä†ËΩΩFEN (Â§öÁ∫øÁ®ãÁâàÊú¨)
    context_idx_to_fen = {}

    def load_single_shard(shard_info):
        """Âä†ËΩΩÂçï‰∏™ÂàÜÁâáÁöÑFENÊï∞ÊçÆ"""
        (shard_idx_val, n_shards_val, dataset_name_val), context_indices_group = shard_info
        shard_results = {}

        try:
            print(f"  Âä†ËΩΩÂàÜÁâá {shard_idx_val}/{n_shards_val} ({dataset_name_val}), ÂåÖÂê´ {len(context_indices_group)} ‰∏™context indices...")
            cfg = mongo_client.get_dataset_cfg(dataset_name_val)
            if cfg:
                dataset = load_dataset_shard(cfg, shard_idx_val, n_shards_val)
                print(f"    Êï∞ÊçÆÈõÜÂ§ßÂ∞è: {len(dataset)}")
                for context_idx in context_indices_group:
                    if context_idx < len(dataset):
                        fen_data = dataset[context_idx]
                        if hasattr(fen_data, 'get') and callable(getattr(fen_data, 'get', None)):
                            fen = fen_data.get('fen')
                        elif isinstance(fen_data, dict):
                            fen = fen_data.get('fen')
                        else:
                            fen = getattr(fen_data, 'fen', None)
                        shard_results[context_idx] = fen
                    else:
                        shard_results[context_idx] = None
            else:
                print(f"    Êó†Ê≥ïËé∑ÂèñÊï∞ÊçÆÈõÜÈÖçÁΩÆ: {dataset_name_val}")
                for context_idx in context_indices_group:
                    shard_results[context_idx] = None
        except Exception as e:
            print(f"    Âä†ËΩΩÂàÜÁâáÂ§±Ë¥•: {e}")
            for context_idx in context_indices_group:
                shard_results[context_idx] = None
        return shard_results

    print(f"Â§öÁ∫øÁ®ãÂä†ËΩΩ {len(shard_groups)} ‰∏™ÂàÜÁâáÁöÑÊï∞ÊçÆ...")
    max_workers = min(8, len(shard_groups))  # ÊúÄÂ§ö8‰∏™Âπ∂ÂèëÁ∫øÁ®ãÂä†ËΩΩÂàÜÁâá
    try:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(load_single_shard, shard_info) for shard_info in shard_groups.items()]
            for future in tqdm(as_completed(futures), total=len(futures), desc="Âä†ËΩΩÂàÜÁâá"):
                shard_results = future.result()
                context_idx_to_fen.update(shard_results)
    except Exception as e:
        print(f"‚ùå Â§öÁ∫øÁ®ãÂä†ËΩΩÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞È°∫Â∫èÂä†ËΩΩ: {e}")
        # ÂõûÈÄÄÂà∞È°∫Â∫èÂä†ËΩΩ
        for shard_info in tqdm(shard_groups.items(), desc="È°∫Â∫èÂä†ËΩΩÂàÜÁâá"):
            shard_results = load_single_shard(shard_info)
            context_idx_to_fen.update(shard_results)

    print(f"‚úÖ ÂàÜÁâáÂä†ËΩΩÂÆåÊàê: Âä†ËΩΩ‰∫Ü {len(context_idx_to_fen)} ‰∏™context indices")

    # Â§ÑÁêÜÈùûÂàÜÁâáÁöÑÊÉÖÂÜµ (Â§öÁ∫øÁ®ãÁâàÊú¨)
    non_shard_contexts = [ctx for ctx in all_context_indices if ctx not in context_idx_to_fen]
    if non_shard_contexts:
        print(f"ÂèëÁé∞ {len(non_shard_contexts)} ‰∏™ÈùûÂàÜÁâácontext indices")
        print(f"  ÊÄªÂÖ±Êúâ {len(all_context_indices)} ‰∏™context indices")
        print(f"  ÂàÜÁâáÂä†ËΩΩ‰∫Ü {len(context_idx_to_fen)} ‰∏™context indices")
        print(f"  ÈùûÂàÜÁâáindicesÁ§∫‰æã: {sorted(non_shard_contexts)[:5]}")

        # Ê£ÄÊü•‰∏∫‰ªÄ‰πàËøô‰∫õcontext_idxÊ≤°ÊúâË¢´ÂàÜÁâáÂä†ËΩΩ
        print(f"  Ê£ÄÊü•ÂàÜÁâáË¶ÜÁõñÊÉÖÂÜµ...")
        covered_by_shards = set()
        for context_indices_group in shard_groups.values():
            covered_by_shards.update(context_indices_group)
        print(f"  ÂàÜÁâágroupsË¶ÜÁõñ‰∫Ü {len(covered_by_shards)} ‰∏™context indices")
        not_covered = all_context_indices - covered_by_shards
        if not_covered:
            print(f"  Êú™Ë¢´ÂàÜÁâáË¶ÜÁõñÁöÑcontext indices: {sorted(list(not_covered))[:10]}")
            print(f"  Ëøô‰∫õÂèØËÉΩÊù•Ëá™Ê≤°ÊúâÂàÜÁâá‰ø°ÊÅØÁöÑsamplings")

        def load_single_non_shard_context(context_idx):
            """Âä†ËΩΩÂçï‰∏™ÈùûÂàÜÁâácontextÁöÑFEN"""
            try:
                fen = get_fen_from_context_idx(context_idx, DATASET_PATH, mongo_client=mongo_client)
                return context_idx, fen
            except Exception as e:
                print(f"    Ëé∑Âèñcontext_idx {context_idx} Â§±Ë¥•: {e}")
                return context_idx, None

        max_workers = min(16, len(non_shard_contexts))  # ÊúÄÂ§ö16‰∏™Âπ∂ÂèëÁ∫øÁ®ã
        try:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(load_single_non_shard_context, ctx) for ctx in non_shard_contexts]
                for future in tqdm(as_completed(futures), total=len(futures), desc="Âä†ËΩΩÈùûÂàÜÁâáÊï∞ÊçÆ"):
                    context_idx, fen = future.result()
                    context_idx_to_fen[context_idx] = fen
        except Exception as e:
            print(f"Â§öÁ∫øÁ®ãÂä†ËΩΩÈùûÂàÜÁâáÊï∞ÊçÆÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞È°∫Â∫èÂä†ËΩΩ: {e}")
            # ÂõûÈÄÄÂà∞È°∫Â∫èÂä†ËΩΩ
            for context_idx in tqdm(non_shard_contexts, desc="È°∫Â∫èÂä†ËΩΩÈùûÂàÜÁâáÊï∞ÊçÆ"):
                try:
                    fen = get_fen_from_context_idx(context_idx, DATASET_PATH, mongo_client=mongo_client)
                    context_idx_to_fen[context_idx] = fen
                except Exception as e:
                    print(f"    Ëé∑Âèñcontext_idx {context_idx} Â§±Ë¥•: {e}")
                    context_idx_to_fen[context_idx] = None

    print(f"üìà ÊâÄÊúâcontext indicesÂä†ËΩΩÂÆåÊàê: ÊÄªÂÖ± {len(context_idx_to_fen)} ‰∏™")
    fen_count = sum(1 for fen in context_idx_to_fen.values() if fen is not None)
    print(f"   ÊàêÂäüÂä†ËΩΩFEN: {fen_count}/{len(context_idx_to_fen)} ({fen_count/len(context_idx_to_fen)*100:.1f}%)")

    # ‰∏∫ÊØè‰∏™featureÁîüÊàêFENÂàóË°® (Â§öÁ∫øÁ®ãÁâàÊú¨)
    print(f"‰∏∫ {len(feature_to_contexts)} ‰∏™featuresÁîüÊàêFENÂàóË°®...")

    def generate_fen_list(feature_info):
        """‰∏∫Âçï‰∏™featureÁîüÊàêFENÂàóË°®"""
        key, context_indices = feature_info
        unique_fens = set()
        for context_idx in context_indices:
            fen = context_idx_to_fen.get(context_idx)
            if fen is not None:
                unique_fens.add(fen)
        return key, list(unique_fens)

    max_workers = min(32, len(feature_to_contexts))  # ÊúÄÂ§ö32‰∏™Âπ∂ÂèëÁ∫øÁ®ã
    try:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(generate_fen_list, feature_info) for feature_info in feature_to_contexts.items()]
            for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc="ÁîüÊàêFENÂàóË°®")):
                key, fen_list = future.result()
                result[key] = fen_list

                # ÊØèÂ§ÑÁêÜ‰∏ÄÂÆöÊï∞ÈáèÂ∞±Êä•ÂëäËøõÂ∫¶
                if (i + 1) % 100 == 0:
                    print(f"    Â∑≤Â§ÑÁêÜ {i + 1}/{len(feature_to_contexts)} ‰∏™features")

    except Exception as e:
        print(f"Â§öÁ∫øÁ®ãÁîüÊàêFENÂàóË°®Â§±Ë¥•ÔºåÂõûÈÄÄÂà∞È°∫Â∫èÁîüÊàê: {e}")
        # ÂõûÈÄÄÂà∞È°∫Â∫èÁîüÊàê
        for i, (key, context_indices) in enumerate(tqdm(feature_to_contexts.items(), desc="È°∫Â∫èÁîüÊàêFENÂàóË°®")):
            unique_fens = set()
            for context_idx in context_indices:
                fen = context_idx_to_fen.get(context_idx)
                if fen is not None:
                    unique_fens.add(fen)
            result[key] = list(unique_fens)

            if (i + 1) % 50 == 0:
                print(f"    Â∑≤Â§ÑÁêÜ {i + 1}/{len(feature_to_contexts)} ‰∏™features")

    print(f"‚úÖ _process_sae_features_batch ÂÆåÊàê")
    print(f"   Â§ÑÁêÜ‰∫Ü {len(result)} ‰∏™features")
    total_fens = sum(len(fen_list) for fen_list in result.values())
    print(f"   ÊÄªÂÖ±ÁîüÊàê‰∫Ü {total_fens} ‰∏™ÂîØ‰∏ÄFEN")
    avg_fens_per_feature = total_fens / len(result) if result else 0
    print(f"   Âπ≥ÂùáÊØè‰∏™feature {avg_fens_per_feature:.1f} ‰∏™FEN")

