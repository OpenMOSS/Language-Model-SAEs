"""Activation functions for sparse autoencoders.

This module contains custom activation functions used in sparse autoencoder implementations,
including JumpReLU and its associated STEFunction.
"""

import torch
import torch.nn as nn
import torch.distributed.tensor
from torch.distributed.device_mesh import DeviceMesh
from typing_extensions import cast

from lm_saes.utils.distributed import DimMap


class STEFunction(torch.autograd.Function):
    """
    STE function for the jumprelu activation function.
    """

    @staticmethod
    def forward(ctx, input: torch.Tensor, jumprelu_threshold: torch.Tensor, jumprelu_threshold_window: float, dims_to_keep_in_bwd: tuple[int, ...]):
        ctx.save_for_backward(
            input,
            jumprelu_threshold,
            torch.tensor(jumprelu_threshold_window, dtype=input.dtype, device=input.device),
        )
        ctx.dims_to_keep_in_bwd = dims_to_keep_in_bwd
        return input * input.gt(jumprelu_threshold)

    @staticmethod
    def backward(ctx, *grad_outputs: torch.Tensor, **args):
        assert len(grad_outputs) == 1
        grad_output = grad_outputs[0]

        input, jumprelu_threshold, jumprelu_threshold_window = ctx.saved_tensors

        grad_jumprelu_threshold = torch.where(
            ((input - jumprelu_threshold).abs() < jumprelu_threshold_window * 0.5) * (input > 0.0),
            -jumprelu_threshold / jumprelu_threshold_window,
            0.0,
        )

        x_grad = grad_output * input.gt(jumprelu_threshold)

        grad_jumprelu_threshold = grad_jumprelu_threshold.sum(
            dim=tuple(
                i for i in range(grad_jumprelu_threshold.ndim)
                if i not in ctx.dims_to_keep_in_bwd
                    and i - grad_jumprelu_threshold.ndim not in ctx.dims_to_keep_in_bwd
            )
        )

        return x_grad, grad_jumprelu_threshold, None, None


class JumpReLU(torch.nn.Module):
    """
    JumpReLU activation function.
    """

    def __init__(
        self,
        jumprelu_threshold_window: float,
        *,
        shape: tuple[int, ...],
        dims_to_keep_in_bwd: tuple[int, ...] = (-1,),
        device: torch.device | str | None = None,
        dtype: torch.dtype,
        device_mesh: DeviceMesh | None = None,
    ):
        """
        Args:
            jumprelu_threshold_window: The window size for the jumprelu threshold.
            shape: The shape of the input tensor.
            dims_to_keep_in_bwd: The dimensions to keep in the backward pass.
                We want to keep gradients in the backward pass the same shape as JumpReLU threshold.
                For example in CLT, we have (n_layers, d_sae) as the shape of the threshold.
                For SAEs, we have (d_sae,).
            device: The device to use.
            dtype: The dtype to use.
            device_mesh: The device mesh to use.
        """
        super(JumpReLU, self).__init__()
        self.jumprelu_threshold_window = jumprelu_threshold_window
        self.shape = shape
        self.dims_to_keep_in_bwd = dims_to_keep_in_bwd
        self.device_mesh = device_mesh
        self.dtype = dtype
        if device_mesh is None:
            self.log_jumprelu_threshold = torch.nn.Parameter(torch.empty(shape, device=device, dtype=dtype))
        else:
            self.log_jumprelu_threshold = torch.nn.Parameter(
                torch.distributed.tensor.empty(
                    shape,
                    dtype=dtype,
                    device_mesh=device_mesh,
                    placements=self.dim_maps()["log_jumprelu_threshold"].placements(device_mesh),
                )
            )
        self._check_dims_to_keep_in_bwd()

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return cast(
            torch.Tensor,
            STEFunction.apply(
                input.to(self.dtype),
                self.log_jumprelu_threshold.exp(),
                self.jumprelu_threshold_window,
                self.dims_to_keep_in_bwd,
            ),
        ).to(input.dtype)
    
    def get_jumprelu_threshold(self) -> torch.Tensor:
        return self.log_jumprelu_threshold.exp()

    def load_distributed_state_dict(
        self, state_dict: dict[str, torch.Tensor], device_mesh: DeviceMesh, prefix: str = ""
    ) -> None:
        self.device_mesh = device_mesh
        self.register_parameter(
            "log_jumprelu_threshold",
            nn.Parameter(state_dict[f"{prefix}log_jumprelu_threshold"].to(self.log_jumprelu_threshold.dtype)),
        )

    def dim_maps(self) -> dict[str, DimMap]:
        return {
            "log_jumprelu_threshold": DimMap({"model": len(self.shape) - 1}),
        }

    def override_dtypes(self) -> dict[str, torch.dtype]:
        return {
            "log_jumprelu_threshold": self.dtype,
        } 

    def _check_dims_to_keep_in_bwd(self):
        assert len(self.dims_to_keep_in_bwd) == len(self.shape), f"dims_to_keep_in_bwd must have the same length as shape, got {self.dims_to_keep_in_bwd} and {self.shape}"