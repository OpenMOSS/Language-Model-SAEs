use_ddp = false
device = "cuda"
seed = 42
dtype = "torch.float32"

exp_name = "L3M"
exp_series = "default"
exp_result_dir = "results"

total_training_tokens = 10_000_000
train_batch_size = 4096

dead_feature_threshold = 1e-6
dead_feature_max_act_threshold = 1.0
decoder_norm_threshold = 0.99

[lm]
model_name = "gpt2"
d_model = 768
use_flash_attn = false

[dataset]
dataset_path = "openwebtext"
is_dataset_tokenized = false
is_dataset_on_disk = false
concat_tokens = false
context_size = 256
store_batch_size = 32

[act_store]
device = "cuda"
seed = 42
dtype = "torch.float32"
hook_points = [ "blocks.3.hook_mlp_out",]
use_cached_activations = false
n_tokens_in_buffer = 500000

[wandb]
log_to_wandb = true
wandb_project = "gpt2-sae"
wandb_entity = "fnlp-mechinterp"