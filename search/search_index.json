{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#language-model-saes","title":"Language Model SAEs","text":"<p>Welcome to the documentation for Language Model SAEs - a library for training and analyzing Sparse Autoencoders (SAEs) on language models.</p>"},{"location":"#overview","title":"Overview","text":"<p>Sparse Autoencoders (SAEs) are neural network models used to extract interpretable features from language models. They help address the superposition problem in neural networks by learning sparse, interpretable representations of activations.</p> <p>This library provides:</p> <ul> <li>Scalability: Our framework is fully distributed with arbitrary combinations of data, model, and head parallelism for both training and analysis. Enjoy training SAEs with millions of features!</li> <li>Flexibility: We support a wide range of SAE variants, including vanilla SAEs, Lorsa (Low-rank Sparse Attention), CLT (Cross-layer Transcoder), MoLT (Mixture of Linear Transforms), CrossCoder, and more. Each variant can be combined with different activation functions (e.g., ReLU, JumpReLU, TopK, BatchTopK) and sparsity penalties (e.g., L1, Tanh).</li> <li>Easy to Use: We provide high-level <code>runners</code> APIs to quickly launch experiments with simple configurations. Check our examples for verified hyperparameters.</li> <li>Visualization: We provide a unified web interface to visualize learned SAE variants and their features.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"Astral uvPip <p>We strongly recommend users to use uv for dependency management. uv is a modern drop-in replacement of poetry or pdm, with a lightning fast dependency resolution and package installation. See their instructions on how to initialize a Python project with uv.</p> <p>To add our library as a project dependency, run:</p> <pre><code>uv add lm-saes==2.0.0b19\n</code></pre> <p>We also support Ascend NPU as an accelerator backend. To add our library as a project dependency with NPU dependency constraints, run:</p> <pre><code>uv add lm-saes[npu]\n</code></pre> <p>Of course, you can also directly use pip to install our library. To install our library with pip, run:</p> <pre><code>pip install lm-saes==2.0.0b19\n</code></pre> <p>We also support Ascend NPU as an accelerator backend. To install our library with NPU dependency constraints, run:</p> <pre><code>pip install lm-saes[npu]\n</code></pre>"},{"location":"#load-a-trained-sparse-autoencoder-from-huggingface","title":"Load a trained Sparse Autoencoder from HuggingFace","text":"<p>Load any Sparse Autoencoder or other sparse dictionaries in <code>Language-Model-SAEs</code> or SAELens format.</p> Language-Model-SAEsSAELens <pre><code># Load Llama Scope 2 Transcoder\nsae = AbstractSparseAutoEncoder.from_pretrained(\n    \"OpenMOSS-Team/Llama-Scope-2-Qwen3-1.7B:transcoder/8x/k128/layer12_transcoder_8x_k128\",\n    fold_activation_scale=False\n)\n</code></pre> <pre><code># Load Gemma Scope 2 SAE\nsae = AbstractSparseAutoEncoder.from_pretrained(\n    \"gemma-scope-2-1b-pt-res-all:layer_12_width_16k_l0_small\",\n)\n</code></pre>"},{"location":"#training-a-sparse-autoencoder","title":"Training a Sparse Autoencoder","text":"<p>To train a simple Sparse Autoencoder on <code>blocks.5.hook_resid_post</code> of a Pythia-160M model with \\(768*8\\) features, you can use the following:</p> <pre><code>settings = TrainSAESettings(\n    sae=SAEConfig(\n        hook_point_in=\"blocks.6.hook_resid_post\",\n        hook_point_out=\"blocks.6.hook_resid_post\",\n        d_model=768,\n        expansion_factor=8,\n        act_fn=\"topk\",\n        top_k=50,\n        dtype=torch.float32,\n        device=\"cuda\",\n    ),\n    initializer=InitializerConfig(\n        grid_search_init_norm=True,\n    ),\n    trainer=TrainerConfig(\n        amp_dtype=torch.float32,\n        lr=1e-4,\n        initial_k=50,\n        k_warmup_steps=0.1,\n        k_schedule_type=\"linear\",\n        total_training_tokens=800_000_000,\n        log_frequency=1000,\n        eval_frequency=1000000,\n        n_checkpoints=0,\n        check_point_save_mode=\"linear\",\n        exp_result_path=\"results\",\n    ),\n    model=LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    ),\n    model_name=\"pythia-160m\",\n    datasets={\n        \"SlimPajama-3B\": DatasetConfig(\n            dataset_name_or_path=\"Hzfinfdu/SlimPajama-3B\",\n        )\n    },\n    wandb=WandbConfig(\n        wandb_project=\"lm-saes\",\n        exp_name=\"pythia-160m-sae\",\n    ),\n    activation_factory=ActivationFactoryConfig(\n        sources=[\n            ActivationFactoryDatasetSource(\n                name=\"SlimPajama-3B\",\n            )\n        ],\n        target=ActivationFactoryTarget.ACTIVATIONS_1D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=4096,\n        buffer_size=4096 * 4,\n        buffer_shuffle=BufferShuffleConfig(\n            perm_seed=42,\n            generator_device=\"cuda\",\n        ),\n    ),\n    sae_name=\"pythia-160m-sae\",\n    sae_series=\"pythia-sae\",\n)\ntrain_sae(settings)\n</code></pre>"},{"location":"#analyze-a-trained-sparse-autoencoder","title":"Analyze a trained Sparse Autoencoder","text":"<p>Requires setting up MongoDB. See analyze-saes for details.</p> <pre><code>settings = AnalyzeSAESettings(\n    sae=PretrainedSAE(pretrained_name_or_path=\"path/to/sae\", device=\"cuda\"),\n    sae_name=\"pythia-160m-sae\",\n    activation_factory=ActivationFactoryConfig(\n        sources=[ActivationFactoryDatasetSource(name=\"SlimPajama-3B\")],\n        target=ActivationFactoryTarget.ACTIVATIONS_2D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=16,\n        context_size=2048,\n    ),\n    model=LanguageModelConfig(model_name=\"EleutherAI/pythia-160m\", device=\"cuda\"),\n    model_name=\"pythia-160m\",\n    datasets={\"SlimPajama-3B\": DatasetConfig(dataset_name_or_path=\"Hzfinfdu/SlimPajama-3B\")},\n    analyzer=FeatureAnalyzerConfig(total_analyzing_tokens=100_000_000),\n    mongo=MongoDBConfig(),\n    device_type=\"cuda\",\n)\n\nanalyze_sae(settings)\n</code></pre>"},{"location":"#convert-trained-sparse-autoencoder-to-saelens-format","title":"Convert trained Sparse Autoencoder to SAELens format","text":"<p>Requires <code>sae_lens</code> package available. Supports ReLU, JumpReLU, and TopK SAEs.</p> <pre><code>from lm_saes import SparseAutoEncoder\n\nsae = SparseAutoEncoder.from_pretrained(\"path/to/sae\")\nsae_saelens = sae.to_saelens(model_name=\"pythia-160m\")\n</code></pre> <p>You can use the <code>sae_saelens</code> with any tools compatible to SAELens.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this library useful in your research, please cite:</p> <pre><code>@misc{Ge2024OpenMossSAEs,\n    title  = {OpenMoss Language Model Sparse Autoencoders},\n    author = {Xuyang Ge and Wentao Shu and Junxuan Wang and Guancheng Zhou and Jiaxing Wu and Fukang Zhu and Lingjie Chen and Zhengfu He},\n    url    = {https://github.com/OpenMOSS/Language-Model-SAEs},\n    year   = {2024}\n}\n</code></pre>"},{"location":"analyze-saes/","title":"Analyze SAEs","text":""},{"location":"analyze-saes/#analyze-sparse-autoencoders","title":"Analyze Sparse Autoencoders","text":"<p>What can a trained Sparse Autoencoder tell us? As an approach to Interpretability, we definitely want to see what each individual latent of a Sparse Autoencoder (i.e., feature) means.</p> <p><code>Language-Model-SAEs</code> incorporates a bunch of methods to explore the functionality of each individual feature, primarily on on what context a feature activates. If an SAE is trained well, you can naturally observe that there's a type of commonality among these contexts. The language model extracts information from these context and expresses it by the feature's activation. Other types of analytical methods are also supported, including Direct Logit Attribution and Automated Interpretation.</p>"},{"location":"analyze-saes/#setup-prerequisites","title":"Setup Prerequisites","text":"<p>A MongoDB instance is required to save all the analyses and speed up feature-level queries. To install MongoDB on your system and launch an instance, we refer you to read the official documentation of MongoDB.</p> <p>Alternatively, to launch MongoDB with Docker, run the following command:</p> <pre><code>docker run -d --name mongodb --restart always -p 27017:27017 mongo:latest\n</code></pre>"},{"location":"analyze-saes/#analyze-a-trained-sparse-autoencoder","title":"Analyze a trained Sparse Autoencoder","text":"<p>A main entrypoint of feaature analyzing is provided for basic feature statistical information, including the activation context at different magnitudes.</p> <p>To analyze a trained Sparse Autoencoder, you can run the following variants:</p> RunnerCLIFull Script <p>Create the <code>AnalyzeSAESettings</code> and call <code>analyze_sae</code> with it. </p> <pre><code>import torch\nfrom lm_saes import (\n    AnalyzeSAESettings,\n    analyze_sae,\n    PretrainedSAE,\n    DatasetConfig,\n    ActivationFactoryConfig,\n    ActivationFactoryDatasetSource,\n    ActivationFactoryTarget,\n    FeatureAnalyzerConfig,\n    LanguageModelConfig,\n)\n\nsettings = AnalyzeSAESettings(\n    sae=PretrainedSAE(\n        pretrained_name_or_path=\"results\",\n    ),\n    sae_name=\"pythia-160m-sae\",\n    sae_series=\"pythia-sae\",\n    model=LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    ),\n    model_name=\"pythia-160m\",\n    datasets={\n        \"SlimPajama-3B\": DatasetConfig(\n            dataset_name_or_path=\"Hzfinfdu/SlimPajama-3B\",\n        ) \n    },\n    activation_factory=ActivationFactoryConfig(\n        sources=[ActivationFactoryDatasetSource(name=\"SlimPajama-3B\")],\n        target=ActivationFactoryTarget.ACTIVATIONS_2D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=32,\n        context_size=1024,\n    ),\n    analyzer=FeatureAnalyzerConfig(\n        total_analyzing_tokens=100_000_000,\n    )\n    mongo=MongoDBConfig(),\n)\n\nanalyze_sae(settings)\n</code></pre> <p>CLI-based workflow requires a configuration file containing the settings consistent with <code>AnalyzeSAESettings</code>. </p> <p>Create a TOML configuration file (e.g., <code>analyze_config.toml</code>) with the following content:</p> <pre><code>sae_name = \"pythia-160m-sae\"\nsae_series = \"pythia-sae\"\nmodel_name = \"pythia-160m\"\noutput_dir = \"analysis_results\"\n\n[sae]\npretrained_name_or_path = \"results\"\n\n[model]\nmodel_name = \"EleutherAI/pythia-160m\"\ndevice = \"cuda\"\ndtype = \"torch.float16\"\n\n[datasets.\"SlimPajama-3B\"]\ndataset_name_or_path = \"Hzfinfdu/SlimPajama-3B\"\n\n[activation_factory]\ntarget = \"activations-2d\"\nhook_points = [\"blocks.6.hook_resid_post\"]\nbatch_size = 32\ncontext_size = 1024\n\n[[activation_factory.sources]]\ntype = \"dataset\"\nname = \"SlimPajama-3B\"\n\n[mongo]\nmongo_uri = \"localhost\"\n\n[analyzer]\ntotal_analyzing_tokens = 10_000_000\n</code></pre> <p>Then run the analysis with:</p> <pre><code>lm-saes analyze analyze_config.toml\n</code></pre> <p>For more granular control, you can use the <code>FeatureAnalyzer</code> directly.</p> <pre><code>import datasets\nimport torch\nfrom lm_saes import (\n    ActivationFactory,\n    ActivationFactoryConfig,\n    ActivationFactoryDatasetSource,\n    ActivationFactoryTarget,\n    LanguageModelConfig,\n    FeatureAnalyzer,\n    FeatureAnalyzerConfig,\n    TransformerLensLanguageModel,\n    AbstractSparseAutoEncoder,\n)\n\n# Load Model &amp; Dataset\nmodel = TransformerLensLanguageModel(\n    LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    )\n)\n\ndataset = datasets.load_dataset(\n    \"Hzfinfdu/SlimPajama-3B\",\n    split=\"train\",\n)\n\n# Generate Activations\nactivation_factory = ActivationFactory(\n    ActivationFactoryConfig(\n        sources=[ActivationFactoryDatasetSource(name=\"SlimPajama-3B\")],\n        target=ActivationFactoryTarget.ACTIVATIONS_2D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=32,\n        context_size=1024,\n    )\n)\n\n# Load trained SAE from disk\nsae = AbstractSparseAutoEncoder.from_pretrained(\"results\", device=\"cuda\")\n\n# Analyze it\nanalyzer = FeatureAnalyzer(\n    FeatureAnalyzerConfig(total_analyzing_tokens=100_000_000)\n)\n\nresult = analyzer.analyze_chunk(\n    activation_factory,\n    sae=sae,\n)\n</code></pre> <p>Note that a key difference of activation generation between training and analyzing is: we want activations with their complete contexts in analyzing. These tokens are only meaningful (to human) when the surrounding contexts are present. In comparison, SAEs are unaware of the contexts of activations in training, but just treat activations at different context positions as equal. Thus, we here generate activations with <code>ActivationFactoryTarget.ACTIVATIONS_2D</code> in <code>ActivationFactoryConfig</code>. This stops our generation process breaking down the with-context activations and shuffling them.</p>"},{"location":"analyze-saes/#visualize-feature-analysis","title":"Visualize Feature Analysis","text":"<p>We have successfully retrieved top activation contexts of each feature. But we definitely do not want to look at each token and each feature's activation value on it. Luckily, <code>Language-Model-SAEs</code> provide two methods to visualize the feature analyses.</p>"},{"location":"analyze-saes/#cli-feature-preview","title":"CLI Feature Preview","text":"<p>You can preview top activation contexts of a certain feature via the CLI. After analyzing an SAE, you can run:</p> <pre><code>lm-saes show feature &lt;sae-name&gt; &lt;feature-index&gt;\n</code></pre> <p>to preview the feature with its analyses. Here's an example output:</p> <pre>$ lm-saes show feature qwen3-1.7b-plt-8x-topk64-layer13 7893</pre> Feature Info Feature #7893 @ qwen3-1.7b-plt-8x-topk64-layer13 termination condition Statistics Activation Times: 1,207,198 Max Activation: 5.0000 Analyzed Tokens: 472,725,125 Top 5 Activation Samples\u00a0\u00a0(Weak | Medium | Strong)  Sample 1 (max act: 5.0000)  remove 1, 2, or 3 stones from the pile. The player who removes the last stone wins. If the first player goes first, what is their winning strategy?&lt;|im_end|&gt; &lt;|im_start|&gt; Sample 2 (max act: 4.7500) imax algorithm with alpha-beta pruning to search for the best move. It recursively explores the game tree to a specified depth and returns the evaluation score.  The `get_best_move` function iterates over all Sample 3 (max act: 4.7188)  returns a pagination token         // that matches the most recent token provided to the service.         StopOnDuplicateToken bool }  // DescribeFleetAttributesPaginator is a paginator for DescribeFleetAttributes Sample 4 (max act: 4.6875) ::wostream&amp; os, T const* str, size_t const max_size, bool const stop_at_null)     {         for(size_t i = 0; i &lt; max_size; ++ Sample 5 (max act: 4.4375)  mathematical expression. The valid characters are letters, numbers,                                               and                                                                  underscore. The first character must be a lowercase letter.                                            &lt;/p&gt;                                             &lt;p&gt;&lt;em <p>The highlighted tokens show where the feature activates, with colors indicating activation strength: weak, medium, and strong. In this example, Feature #7893 appears to detect \"termination condition\" patterns\u2014contexts related to stopping, ending, or terminal states in algorithms and data structures.</p>"},{"location":"analyze-saes/#web-ui","title":"Web UI","text":"<p>For a more comprehensive exploration experience, you can launch the web server to browse all features interactively. The server provides a visual interface for exploring feature analyses. To use the Web UI, you can either manually launch the Python backend and React frontend, or launch them through Docker Compose.</p> ManualDocker Compose <ol> <li> <p>Launch Backend: Start the FastAPI server using <code>uvicorn</code>. You may need to create a <code>.env</code> file in the <code>server</code> directory first (see <code>server/.env.example</code>).     </p><pre><code>uvicorn server.app:app --port 24577 --env-file server/.env\n</code></pre><p></p> </li> <li> <p>Launch Frontend: The frontend uses Bun for dependency management. Install dependencies and start the development server.     </p><pre><code>cd ui\nbun install\n# Copy .env.example and configure BACKEND_URL if necessary\ncp .env.example .env\nbun dev --port 24576\n</code></pre><p></p> </li> </ol> <p>After both are running, you can access the Web UI at <code>http://localhost:24576</code>.</p> <p>You can launch the entire stack (MongoDB, Backend, and Frontend) using Docker Compose. Create a <code>docker-compose.yml</code> file with the following content:</p> <pre><code>services:\n  mongodb:\n    image: mongo:latest\n    restart: always\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongodb_data:/data/db\n\n  backend:\n    image: ghcr.io/openmoss/language-model-saes-backend:latest\n    restart: always\n    ports:\n      - \"24577:24577\"\n    environment:\n      - MONGO_URI=mongodb://mongodb:27017/\n      - MONGO_DB=mechinterp\n    # volumes:\n    #   - ./models:/models\n    #   - ./datasets:/datasets\n    #   - ./saes:/saes\n    depends_on:\n      - mongodb\n\n  frontend:\n    image: ghcr.io/openmoss/language-model-saes-frontend:latest\n    restart: always\n    ports:\n      - \"24576:24576\"\n    environment:\n      - BACKEND_URL=http://backend:24577\n    depends_on:\n      - backend\n\nvolumes:\n  mongodb_data:\n</code></pre> <p>Note the above configuration contains a container for MongoDB. If you have launched your MongoDB instance/container elsewhere, configure it properly through the <code>MONGO_URI</code> environmental variable in <code>backend</code>.</p> <p>Then run:</p> <pre><code>docker compose up\n</code></pre> <p>The Web UI will be available at <code>http://localhost:24576</code>.</p>"},{"location":"analyze-saes/#direct-logit-attribution","title":"Direct Logit Attribution","text":"<p>Direct Logit Attribution (DLA) helps understand how each feature directly contributes to the model's output logits. It computes the projection of the feature's decoder weight onto the unembedding matrix.</p> <p>DLA is like an opposite of the top activation contexts: the top activation contexts are the most related inputs to a certain feature which makes it activate, while the DLA concerns about the most related output that the feature likely induces. Higher layer features are likely to have more direct effect on the output side and show clearer inclination in their DLA logits.</p> <p>To perform DLA, you can use the <code>direct_logit_attribute</code> runner:</p> <pre><code>from lm_saes import DirectLogitAttributeSettings, direct_logit_attribute, DirectLogitAttributorConfig, PretrainedSAE\n\nsettings = DirectLogitAttributeSettings(\n    sae=PretrainedSAE(pretrained_name_or_path=\"results\"),\n    sae_name=\"pythia-160m-sae\",\n    sae_series=\"pythia-sae\",\n    model_name=\"EleutherAI/pythia-160m\",\n    direct_logit_attributor=DirectLogitAttributorConfig(\n        top_k=10,\n    ),\n    mongo=MongoDBConfig(),\n)\n\ndirect_logit_attribute(settings)\n</code></pre>"},{"location":"analyze-saes/#automated-interpretation","title":"Automated Interpretation","text":"<p><code>Language-Model-SAEs</code> supports automated interpretation of features using LLMs. The interpretation are mostly generated through investigating the top activation context of each feature. While not perfect, it can help human to quickly gain a brief cognition of the feature.</p> <p>To run automated interpretation, you can use the <code>auto_interp</code> runner:</p> <pre><code>from lm_saes import AutoInterpSettings, auto_interp, AutoInterpConfig, LanguageModelConfig, MongoDBConfig\n\nsettings = AutoInterpSettings(\n    sae_name=\"pythia-160m-sae\",\n    sae_series=\"pythia-sae\",\n    model=LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    ),\n    model_name=\"pythia-160m\",\n    auto_interp=AutoInterpConfig(\n        openai_api_key=\"your-api-key\",\n        openai_model=\"gpt-4o\",\n    ),\n    mongo=MongoDBConfig(),\n)\n\nauto_interp(settings)\n</code></pre>"},{"location":"concepts/","title":"Key Concepts","text":""},{"location":"concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"distributed-guidelines/","title":"Distributed Guidelines","text":""},{"location":"distributed-guidelines/#distributed-guidelines","title":"Distributed Guidelines","text":"<p>A fundamental advantage of <code>Language-Model-SAEs</code> is its support of distributed setup, including data parallelism (DP), tensor parallelism (TP), some special parallelism strategy for some specific models, and their arbitrary combination. These strategies avoid OOM and accelerate model computation, making it possible for training arbitrarily large sparse dictionaries for frontier models.</p>"},{"location":"distributed-guidelines/#how-does-it-work","title":"How does it work","text":"<p>Note</p> <p>If you don't care about the under-the-hood implementation of our distributed settings, feel free to skip this section. You can still use the distributed settings to speed up everything with ease.</p> <p>We mainly take advantage of PyTorch DeviceMesh and DTensor to organize distributed storage/computation and collective communication.</p>"},{"location":"distributed-guidelines/#devicemesh","title":"DeviceMesh","text":"<p>DeviceMesh is a multi-dimensional mesh structure that manages the distribution of computation across your devices (typically GPUs). Each cell in the mesh represents a single device, and each dimension of the mesh corresponds to a specific parallelism strategy.</p> An illustration of DeviceMesh with 8 GPUs arranged in a 2\u00d74 grid. This configuration uses Data Parallelism (DP=2) along one dimension and Tensor Parallelism (TP=4) along the other. <p>DeviceMesh provides a standardized framework for implementing multi-dimensional parallelism. For each parallelism strategy, sharding and communication operations occur exclusively along the corresponding dimension of the mesh.</p> <p>In the example above, the input data is split into 2 shards. GPUs 0-3 process the first shard, while GPUs 4-7 process the second shard. And for the TP dimension, model parameters are partitioned into 4 shards, distributed across GPUs within each data-parallel group (e.g., GPUs 0, 1, 2, 3 each hold one shard of the model).</p> <p>This mesh abstraction allows you to compose different parallelism strategies cleanly, with each strategy operating independently along its designated dimension.</p> <p>A DeviceMesh can be created by:</p> <pre><code>device_mesh = init_device_mesh(\n    device_type=\"cuda\",\n    mesh_shape=(8,),\n    mesh_dim_names=(\"model\",),\n)\n</code></pre>"},{"location":"distributed-guidelines/#dtensor","title":"DTensor","text":"<p>Built on top of DeviceMesh, DTensor provides an abstraction layer that enables you to work with distributed tensors from a global perspective.</p> <p>DTensor requires your code to follow the SPMD (Single Program, Multiple Data) paradigm, meaning the same program executes across all processes. Under this model, any tensor created at a specific point in the program has a corresponding tensor in every other process at that same point.</p> <p>When using regular tensors in distributed settings, these per-process tensors exist independently with no explicit relationship or coordination between them. DTensor addresses this by providing a unified, global view: it logically represents a single large tensor containing all the data across all processes, which is then automatically sharded and distributed to each process according to its <code>Placement</code> specifications.</p> <p>A DTensor can be created by:</p> <pre><code>local_tensor = torch.randn(2, 4)\ndtensor = DTensor.from_local(\n    local_tensor,\n    device_mesh=device_mesh,\n    placements=(Shard(0)),\n) # This `dtensor` stores per-device data just as the `local_tensor`. \n# It just provides more information on how the tensor is organized across device globally.\n</code></pre> <p>More factory method of DTensor can be found at torch.distributed.tensor.</p>"},{"location":"distributed-guidelines/#determine-the-placements-of-dtensor","title":"Determine the Placements of DTensor","text":"<p>The term \"dimension\" is indeed overloaded. It can refer to a column or a row in a DeviceMesh, or in a tensor in its mathematical meaning. When it comes to \"sharding\" a DTensor, it actually relates to both of the interpretations. For each of the dimension of DeviceMesh we want the DTensor to shard across, we must select a tensor dimension to perform the sharding.</p> <p>However, we cannot randomly pick tensor dimensions to shard the tensors on. Suppose we are to perform distributed matrix multiplication with tensors \\(a \\in M \\times K\\) and \\(b \\in K \\times N\\). It's only possible to efficiently accelerate the computation when both the tensors are sharded on the middle dimension \\(K\\) or both are sharded in the outer dimension \\(M\\) and \\(N\\), in which case every local device has the required data to perform its block matrix multiplication.</p> <p>Thus, we need to carefully determine: for each mesh dimension, which tensor dimension, if any, should be sharded on? DTensor uses its <code>placements</code> to specify how shardings correspond to each mesh dimension:</p> <pre><code># Shard tensor dim 0 along the first mesh dimension, and replicate along the second mesh dimension.\nplacements = (Shard(0), Replicate())\n</code></pre> <p>However, this placement tuple are tightly coupling to mesh topology. If the mesh changes, e.g., from <code>(\"data\", \"model\")</code> to just <code>(\"model\",)</code> when running on a single node without data parallelism, the placement tuple should be changed correspondingly. </p> <p>To make every weight and activation in our codebase flexible to different mesh, we specify their <code>DimMap</code> to dynamically compute the placements, taking inspiration from JAX's PartitionSpec. <code>DimMap</code> maps mesh dimension names to tensor dimensions, and any mesh dimension absent from the map is implicitly replicated:</p> <pre><code>DimMap({\"data\": 0})            # Shard tensor dim 0 along mesh \"data\"; replicate elsewhere\nDimMap({\"model\": 1})           # Shard tensor dim 1 along mesh \"model\"; replicate elsewhere\nDimMap({\"data\": 0, \"model\": 1}) # Shard along both\nDimMap({})                      # Fully replicated\n</code></pre> <p>Given a concrete <code>DeviceMesh</code>, <code>DimMap.placements(device_mesh)</code> generates the correct positional <code>Placement</code> tuple dynamically.</p>"},{"location":"distributed-guidelines/#application-in-language-model-saes","title":"Application in Language-Model-SAEs","text":"<p>Ideally the above system can inherently solve multi-dimensional parallelism, including DP and TP -- we just need to provide a DeviceMesh, and specify the DimMap of each leaf node tensor (input and weight), and the tensor operations (matrix multiplications and others) will be automatically accelerated. </p> <p>But in practice, this cannot be the full story. Often a primitive of PyTorch does not know how it should deal with DTensor properly, or the implementation is not performant in distributed cases (run slowly or costs unnecessary extra GPU memory). So there're still a number of corner cases in which we need to convert the DTensors back to local tensor and operate on them manually.</p>"},{"location":"distributed-guidelines/#accelerate-your-traininganalyzing","title":"Accelerate Your Training/Analyzing","text":"<p>The use of distributed strategies is just as simple as other libraries: for the runners we've provided, just specify <code>data_parallel_size</code> and <code>model_parallel_size</code> in the settings, and launch your experiment via <code>torchrun</code>. The total number of processes must equal <code>data_parallel_size \u00d7 model_parallel_size</code>.</p> <p>Generate activations with 8 GPUs (8-way data parallelism):</p> <pre><code>uv run torchrun --nproc-per-node=8 examples/generate_pythia_activation_1d.py \\\n    --size 160m --layer 6 --activation_path /data/activations\n</code></pre> <pre><code>settings = GenerateActivationsSettings(\n    ...,\n    data_parallel_size=8,\n)\n</code></pre> <p>Train SAEs with 8 GPUs (2-way DP \u00d7 4-way TP):</p> <pre><code>uv run torchrun --nproc-per-node=8 examples/train_pythia_sae_topk.py\n</code></pre> <pre><code>settings = TrainSAESettings(\n    ...,\n    data_parallel_size=2,\n    model_parallel_size=4,\n)\n</code></pre> <p>Analyze SAEs with 4 GPUs (4-way TP):</p> <pre><code>uv run torchrun --nproc-per-node=4 examples/analyze_pythia_sae.py \\\n    --sae_path /path/to/sae\n</code></pre> <pre><code>settings = AnalyzeSAESettings(\n    ...,\n    model_parallel_size=4,\n)\n</code></pre> <p>For custom runners, you may create the <code>DeviceMesh</code> yourself and pass it to modules like <code>ActivationFactory</code>. Most modules in <code>Language-Model-SAEs</code> support <code>DeviceMesh</code> inherently.</p>"},{"location":"style-guide/","title":"Style Guide","text":""},{"location":"style-guide/#style-guide-for-language-model-saes","title":"Style Guide for Language-Model-SAEs","text":"<p>Language-Model-SAEs basically takes advantage of Python and TypeScript (React), respectively for the core library &amp; backend, and the frontend visualization. This style guide is a list of common dos and don'ts.</p>"},{"location":"style-guide/#python-style-guide","title":"Python Style Guide","text":"<p>The Python style guide mainly follows the best practices listed in Google Python Style Guide, but also contains instructions on writing tensor computation and distributed program.</p>"},{"location":"style-guide/#lint-and-format","title":"Lint and Format","text":"<p>Language-Model-SAEs uses ruff as the Python linter and formatter. <code>ruff</code> is a tool for detecting stylistic inconsistencies and potential bugs in Python source code. The formatter ensures consistent formatting throughout the codebase, including indentation, line width, trailing commas, and string quote style. The linter checks code quality, catching issues like unused variables and non-standard naming conventions. Make sure ruff is happy before committing, by running:</p> <pre><code>uv run ruff format # Run the Ruff formatter\nuv run ruff check --fix # Run the Ruff linter\n</code></pre> <p>These commands will check the formatting and linting issues in the Python codes based on the rules defined in <code>pyproject.toml</code>. It will also fix all formatting problems and some fixable linting problems. You should manually check the remaining linting problems (if exists) and fix them.</p> <p>We also have a pre-commit hook configured in <code>.pre-commit-config.yaml</code>. Install the pre-commit hook by running</p> <pre><code>uv run pre-commit install\n</code></pre> <p>This should automatically run the above <code>ruff</code> formatter and linter checks before committing.</p>"},{"location":"style-guide/#imports-and-exports","title":"Imports and Exports","text":"<ul> <li>Use <code>import x</code> for importing packages and modules.</li> <li>Use <code>from x import y</code> where <code>x</code> is the package prefix and <code>y</code> is the module name with no prefix.</li> <li>Use <code>from x import y as z</code> in any of the following circumstances:<ul> <li>Two modules named <code>y</code> are to be imported.</li> <li><code>y</code> conflicts with a top-level name defined in the current module.</li> <li><code>y</code> conflicts with a common parameter name that is part of the public API (e.g., <code>features</code>).</li> <li><code>y</code> is an inconveniently long name.</li> <li><code>y</code> is too generic in the context of your code (e.g., <code>from storage.file_system import options as fs_options</code>).</li> </ul> </li> <li>Use <code>import y as z</code> only when z is a standard abbreviation (e.g., <code>import numpy as np</code>).</li> <li>Always use complete absolute path to import first-party modules.</li> <li>For any functions or classes intended to be exposed to users, add them to <code>__all__</code> in <code>__init__.py</code>.</li> </ul>"},{"location":"style-guide/#exceptions","title":"Exceptions","text":"<p>WIP</p>"},{"location":"style-guide/#mutable-states","title":"Mutable States","text":"<p>Immutability produces code that's easier to reason about, easier to test, and easier to verify for correctness. Immutable data doesn't change, so we can safely reuse it without worrying that results will differ between calls. Immutable data can also be safely passed between threads without race conditions or other concurrency issues. We should avoid mutable states whenever possible.</p>"},{"location":"style-guide/#mutable-global-states","title":"Mutable Global States","text":"<p>Avoid mutable global states in the core library, as they significantly compromise the purity of core functionalities. Some global caches are permitted in the visualization server.</p>"},{"location":"style-guide/#mutable-local-states","title":"Mutable Local States","text":"<p>While a purely functional style (which avoids mutable states entirely) is preferred, some mutability is pragmatic or even necessary. Completely eliminating mutability may lead to overly complicated program structures and decreased readability. Thus, the preference for immutability follows a best effort principle. Below are some cases where mutable states are acceptable, though immutable alternatives should be considered first:</p> <ul> <li> <p>Use list/dictionary/set comprehension to create container types without resorting to procedural loops, <code>map</code>, <code>filter</code>, etc. The comprehension approach removes temporary mutable states and keeps codes concise.</p> <p>List Comprehension</p> <pre><code>arr = [x + 1 for x in range(5)]\n</code></pre> <p>Loops</p> <pre><code>arr = [] # Create an empty container\nfor x in range(5):\n    arr.append(x + 1) # Modify the container\n</code></pre> <p>Explicit Map</p> <pre><code>arr = list(map(lambda x: x + 1, range(5)))\n</code></pre> </li> <li> <p>The principle of avoiding \"empty first, then fill\" applies to other cases as well:</p> <p>Stack Tensors</p> <pre><code>full = torch.stack([part_a, part_b])\n</code></pre> <p>Fill Empty Tensor</p> <pre><code>full = torch.zeros(2, 5, 5)\nfull[0] = part_a\nfull[1] = part_b\n</code></pre> <p>Concatenate Strings</p> <pre><code>message = f\"Error: {error_type} at line {line_num}\"\n# or\nparts = [\"Processing\", filename, \"with\", str(num_items), \"items\"]\nmessage = \" \".join(parts)\n</code></pre> <p>Accumulate Strings</p> <pre><code>message = \"Error: \"\nmessage += error_type\nmessage += \" at line \"\nmessage += str(line_num)\n</code></pre> <p>Build Dictionary</p> <pre><code>config = {\n    \"model\": model_name,\n    \"layers\": [layer.name for layer in layers],\n    \"params\": {k: v for k, v in params.items() if v is not None}\n}\n</code></pre> <p>Incrementally Fill Dictionary</p> <pre><code>config = {}\nconfig[\"model\"] = model_name\nconfig[\"layers\"] = []\nfor layer in layers:\n    config[\"layers\"].append(layer.name)\nconfig[\"params\"] = {}\nfor k, v in params.items():\n    if v is not None:\n        config[\"params\"][k] = v\n</code></pre> </li> <li> <p>Avoid in-place modifications: create new containers instead of modifying old.</p> <p>Create New List on Modification</p> <pre><code>arr = list(range(5))\narr = [*arr, 5]\n# or\narr = arr + [5]\n</code></pre> <p>Modify Old List</p> <pre><code>arr = list(range(5))\narr.append(5)\n</code></pre> <p>Create New Dictionary on Modification</p> <pre><code>config = {\n    \"model\": model_name,\n    \"layer\": 1\n}\n\nconfig = {\n    **config,\n    \"layer\": 2\n}\n\n# or\n\nconfig = config | {\n    \"layer\": 2\n}\n</code></pre> <p>Modify Old Dictionary</p> <pre><code>config = {\n    \"model\": model_name,\n    \"layer\": 1\n}\nconfig[\"layer\"] = 2\n</code></pre> </li> <li> <p>When mutable state is inevitable, limit its scope and preserve the purity of the outer function. Ensure that only a minimal portion of the code has access to the mutable state.</p> <p>Localized Mutable State</p> <pre><code>def compute_statistics(data: list[float]) -&gt; dict[str, float]:\n    \"\"\"Pure function that returns statistics without side effects.\"\"\"\n    # Mutable state is confined within this function\n    stats = {}\n    total = 0.0\n\n    for value in data:\n        total += value\n\n    stats[\"mean\"] = total / len(data)\n    stats[\"sum\"] = total\n\n    return stats  # Return new object, no external mutation\n</code></pre> <p>Leaked Mutable State</p> <pre><code># Global mutable state\naccumulated_stats = {}\n\ndef compute_statistics(data: list[float]) -&gt; None:\n    \"\"\"Impure function that mutates global state.\"\"\"\n    total = 0.0\n    for value in data:\n        total += value\n\n    # Mutates external state - breaks purity\n    accumulated_stats[\"mean\"] = total / len(data)\n    accumulated_stats[\"sum\"] = total\n</code></pre> </li> </ul> <p>The function remains referentially transparent: given the same input, it always produces the same output without observable side effects. Internal mutability for performance is acceptable as long as it doesn't leak outside the function boundary.</p>"},{"location":"style-guide/#tensor-computation","title":"Tensor Computation","text":"<p>PyTorch provides a wide range of tensor operations. However, most can be decomposed into basic operators. As a library for interpretability, we encourage using einops for better readability, since it explicitly specifies input shapes, output shapes, and semantic dimensions instead of numeric indices.</p> <ul> <li> <p>Use <code>einops.einsum</code> to perform tensor products, including batch matrix multiplication and more complex operations. Use full names for dimensions, e.g., <code>batch</code> instead of <code>b</code>. Matrix multiplication can be written as <code>x @ y</code> for simplicity.</p> </li> <li> <p>Use <code>einops.rearrange</code> to perform reshape, transpose, permute, squeeze, and unsqueeze operations while explicitly showing how dimensions change.</p> <p>Rearrange</p> <pre><code>a = torch.randn(2, 3, 4)\nb = einops.rearrange(\n    a, \"batch n_context d_model -&gt; (batch n_context) d_model\"\n)\nc = einops.rearrange(b, \"batch d_model -&gt; d_model batch\")\n</code></pre> <p>Other Operators</p> <pre><code>a = torch.randn(2, 3, 4)\nb = a.reshape(6, 4)\nc = b.t()\n</code></pre> </li> <li> <p>Use <code>einops.reduce</code> to perform reductions over specific dimensions. Only use <code>.mean()</code> or <code>.sum()</code> for overall reductions that produce a scalar output.</p> <p>Reduce</p> <pre><code>a = torch.randn(2, 3, 4)\nb = einops.reduce(a, \"batch n_context d_model -&gt; d_model\", \"sum\")\n</code></pre> <p>Direct Reduction</p> <pre><code>a = torch.randn(2, 3, 4)\nb = a.sum(dim=[0, 1])  # Unclear what dimensions 0 and 1 represent\n</code></pre> </li> <li> <p>Use <code>einops.repeat</code> to broadcast or tile tensors along specific dimensions instead of manual reshaping and expanding.</p> <p>Repeat</p> <pre><code>a = torch.randn(3, 4)\nb = einops.repeat(a, \"n_context d_model -&gt; batch n_context d_model\", batch=2)\n</code></pre> <p>Manual Broadcasting</p> <pre><code>a = torch.randn(3, 4)\nb = a.unsqueeze(0).expand(2, -1, -1)  # Unclear dimension semantics\n</code></pre> </li> <li> <p>Avoid using complicated operators and modules from <code>torch.nn</code> and <code>torch.nn.functional</code>, unless there're significant performance gaps.</p> </li> </ul>"},{"location":"style-guide/#distributed-programming","title":"Distributed Programming","text":"<p>The distributed support in Language-Model-SAEs relies on DeviceMesh and DTensor. The design of <code>DeviceMesh</code> and <code>DTensor</code> is heavily inspired by JAX. <code>DeviceMesh</code> allows users to easily manage multi-dimensional parallelism by creating a \"mesh\" that controls all devices and specifies how different parallelism strategies are distributed across them. Built on <code>DeviceMesh</code>, <code>DTensor</code> provides a global view of how tensors are distributed across devices, following the SPMD (Single Program, Multiple Data) programming model. With <code>DTensor</code>, users can (ideally) work as if they have infinite logical device memory to accommodate large tensors and perform operations on them. <code>DTensor</code> automatically splits the data and computation across physical devices based on the <code>DeviceMesh</code> it operates on and the sharding strategy it uses.</p> <p>Below list some rules to better leverage <code>DTensor</code> for distributed programming in Language-Model-SAEs:</p> <ul> <li> <p>Avoid hardcoding <code>DTensor</code> placements. Use DimMap (which is designed to be similar to PartitionSpec in JAX) to dynamically generate placements based on current <code>DeviceMesh</code>. This allows absence of some specific dimensions in <code>DeviceMesh</code>.</p> <p>DimMap-generated Placements</p> <pre><code>device_mesh = init_device_mesh(\n    device_type=\"cuda\",\n    mesh_shape=(2, 4),\n    mesh_dim_names=(\"data\", \"model\"),\n)\nv = torch.randn(2, 4)\nv = DTensor.from_local(\n    v,\n    device_mesh=device_mesh,\n    placements=DimMap({\"data\": 0}).placements(device_mesh), # Generate placements from DimMap\n)\n</code></pre> <p>Hardcoded Placements</p> <pre><code>device_mesh = init_device_mesh(\n    device_type=\"cuda\",\n    mesh_shape=(2, 4),\n    mesh_dim_names=(\"data\", \"model\"),\n)\nv = torch.randn(2, 4)\nv = DTensor.from_local(\n    v,\n    device_mesh=device_mesh,\n    placements=(Shard(0), Replicate()),\n)\n</code></pre> </li> <li> <p>Avoid set</p> </li> </ul>"},{"location":"style-guide/#type-annotation","title":"Type Annotation","text":"<p>All codes should be annotated with type hints. Language-Model-SAEs relys on basedpyright to perform static type checking. Below list some extra rules:</p> <ul> <li> <p>Type hints of generic types should follow PEP 585. Use built-in types <code>list</code>, <code>dict</code>, <code>set</code>, etc. rather than types from the <code>typing</code> module.</p> <p>Built-in Types in Type Hints</p> <pre><code>def find(haystack: dict[str, list[int]]) -&gt; int:\n    ...\n</code></pre> <p>Types from <code>typing</code> module</p> <pre><code>def find(haystack: typing.Dict[str, typing.List[int]]) -&gt; int:\n    ...\n</code></pre> </li> <li> <p>Type hints of union types should follow PEP 604 syntax. Use <code>X | Y</code> rather than <code>Union[X, Y]</code>, and <code>X | None</code> rather than <code>Optional[X]</code>.</p> <p>PEP 604 Syntax</p> <pre><code>def f(param: int | None) -&gt; float | str:\n    ...\n</code></pre> <p>Old Syntax</p> <pre><code>def f(param: Optional[int]) -&gt; Union[float, str]:\n    ...\n</code></pre> </li> <li> <p>Tensors with known shapes should be annotated with <code>jaxtyping</code>.</p> <p>Tensors with Shapes Annotated</p> <pre><code>def encode(x: Float[torch.Tensor, \"batch n_context d_model\"]) -&gt; Float[torch.Tensor, \"batch n_context d_sae\"]:\n    ...\n</code></pre> <p>Bare Tensor</p> <pre><code>def encode(x: torch.Tensor) -&gt; torch.Tensor:\n    ...\n</code></pre> </li> </ul> <p>Some of the type hints in the current codebase may not follow the above rules since it's heavy work to fix them all. We expect new codes to follow these rules.</p>"},{"location":"style-guide/#typescript-style-guide","title":"TypeScript Style Guide","text":"<p>TBD</p>"},{"location":"train-saes/","title":"Train SAEs","text":""},{"location":"train-saes/#train-sparse-autoencoders","title":"Train Sparse Autoencoders","text":"<p><code>Language-Model-SAEs</code> provides a general way to train, analyze and visualize Sparse Autoencoders and their variants. To help you get started quickly, we've included example scripts that guide you through each stage of working with SAEs. This guide begins with a foundational example and progressively introduces the core features and capabilities of the library.</p>"},{"location":"train-saes/#training-basic-sparse-autoencoders","title":"Training Basic Sparse Autoencoders","text":"<p>A Sparse Autoencoder is trained to reconstruct model activations at specific position. We depend on TransformerLens to take activations out of model forward pass, specified by hook points. <code>Language-Model-SAEs</code> provides complete abstraction on the necessary components to train Sparse Autoencoders at ease.</p>"},{"location":"train-saes/#load-model-dataset","title":"Load Model &amp; Dataset","text":"<p>Generation of our training data, the model activations, requires the presence of both the language model and the dataset. First, we can load a pretrained language model by:</p> <pre><code>from lm_saes import LanguageModelConfig, TransformerLensLanguageModel\n\nmodel = TransformerLensLanguageModel(\n    LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    )\n)\n</code></pre> <p>where <code>TransformerLensLanguageModel</code> is a simple wrapper around the TransformerLens <code>HookedTransformer</code>, enhanced with:</p> <ol> <li>Unified interface for extracting activations (compatible with native HuggingFace transformers) and tracing token positions from original texts.</li> <li>Distributed training support with simple data parallelism integrated.</li> </ol> <p>See the section below to find how to use HuggingFace transformers directly for generating activation.</p> <p>Use Half Precision</p> <p>Activation generation constitutes the majority of training time. We strongly recommend using half precision (<code>float16</code> or <code>bfloat16</code>) to accelerate the forward pass and reduce GPU memory usage. Here we use FP16 since Pythia models are trained in FP16.</p> <p>Next, we load some text corpus from HuggingFace. Different pretraining text corpus often does not have so much effect on SAE training. Here we load <code>Hzfinfdu/SlimPajama-3B</code>, a 3B-token subset of the 627B SlimPajama dataset, which is typically sufficient for basic SAE training.</p> <pre><code>import datasets\n\ndataset = datasets.load_dataset(\n    \"Hzfinfdu/SlimPajama-3B\", \n    split=\"train\",\n)\n</code></pre>"},{"location":"train-saes/#generate-activations","title":"Generate Activations","text":"<p>Model activations often require some further transformation to ensure correct and efficient SAE training. We provide <code>ActivationFactory</code> as the core abstraction for producing activation streams. It provides a comprehensive interface to generate activations from model forward passes, filter unnecessary tokens, and reshape, re-batch, and shuffle activations.</p> <p>We can create an <code>ActivationFactory</code> as follow:</p> <pre><code>from lm_saes import (\n    ActivationFactory,\n    ActivationFactoryConfig,\n    ActivationFactoryDatasetSource,\n    ActivationFactoryTarget,\n    BufferShuffleConfig,\n)\n\nactivation_factory = ActivationFactory(\n    ActivationFactoryConfig(\n        sources=[ActivationFactoryDatasetSource(name=\"SlimPajama-3B\")],\n        target=ActivationFactoryTarget.ACTIVATIONS_1D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=4096,\n        buffer_size=4096 * 4, # Set to enable the online activation shuffling\n        buffer_shuffle=BufferShuffleConfig(\n            perm_seed=42,\n            generator_device=\"cuda\",\n        ),\n    )\n)\n</code></pre> <p>Then, we call the <code>.process</code> method, passing in our loaded model and dataset, to start the stream processing of the activations.</p> <pre><code>activations_stream = activation_factory.process(\n    model=model,\n    model_name=\"pythia-160m\",\n    datasets={\"SlimPajama-3B\": dataset},\n)\n</code></pre> <p>It returns a streaming iterator. Each item is a dictionary mainly containing:</p> Key Description <code>\"blocks.6.hook_resid_post\"</code> Activation tensor with shape <code>(batch_size, d_model)</code> \u2014 in this config, <code>(4096, 768)</code> <code>\"tokens\"</code> The corresponding token IDs <p>With target set to <code>ActivationFactoryTarget.ACTIVATIONS_1D</code>, the produced activations will have no sequence dimension. They are shuffled across both samples and context positions to ensure the SAE trains on randomly sampled activations from any position in any sample.</p>"},{"location":"train-saes/#create-and-initialize-sae","title":"Create and Initialize SAE","text":"<p>We've successfully prepared the data we need. It's time to turn to the SAE itself! But before training, we should first define the SAE architecture and initialize it. Create an <code>SAEConfig</code> to define the SAE architecture:</p> <pre><code>import torch\nfrom lm_saes import SAEConfig\n\nsae_cfg = SAEConfig(\n    hook_point_in=\"blocks.6.hook_resid_post\",\n    hook_point_out=\"blocks.6.hook_resid_post\",\n    d_model=768,\n    expansion_factor=8,\n    act_fn=\"relu\",\n    dtype=torch.float32,\n    device=\"cuda\",\n)\n</code></pre> <p>Here're some brief explanations of the config we set:</p> Parameter Description <code>hook_point_in</code> / <code>hook_point_out</code> When identical, this defines an SAE; when different, it becomes a transcoder <code>d_model</code> Must match the model's hidden size (768 for Pythia-160m) <code>expansion_factor</code> Multiplier for the latent dimension. Here, <code>d_sae = 768 \u00d7 8 = 6144</code> <code>act_fn</code> Activation function. Modern SAEs often use <code>\"jumprelu\"</code> or <code>\"batchtopk\"</code>, but we use <code>\"relu\"</code> for simplicity <p>More options of <code>SAEConfig</code> are introduced in the reference.</p> <p>With only SAEConfig defined, the created SAE will have nothing but empty tensors as parameters. We need to fill the empty parameters with proper initialization, which is often proved crucial for final SAE performance. The <code>Initializer</code> class handles parameter initialization. The <code>grid_search_init_norm</code> option (recommended) searches for the optimal encoder/decoder parameter scale to minimize initial MSE loss on the activation distribution.</p> <pre><code>from lm_saes import Initializer, InitializerConfig\n\ninitializer = Initializer(InitializerConfig(grid_search_init_norm=True))\nsae = initializer.initialize_sae_from_config(\n    sae_cfg,\n    activation_stream=activations_stream\n)\n</code></pre>"},{"location":"train-saes/#train-sae","title":"Train SAE","text":"<p>Finally, we can start training! A <code>Trainer</code> instance is responsible for holding optimizer &amp; scheduler states.</p> <pre><code>from lm_saes import Trainer, TrainerConfig\n\ntrainer = Trainer(\n    TrainerConfig(\n        amp_dtype=torch.float32,\n        lr=1e-4,\n        total_training_tokens=800_000_000,\n        log_frequency=1000,\n        exp_result_path=\"results\",\n    )\n)\n</code></pre> Parameter Description <code>amp_dtype</code> Mixed precision dtype. Also handles precision mismatches between SAE parameters and activations <code>lr</code> Learning rate <code>total_training_tokens</code> Total tokens for training. Training steps = total tokens / batch size <code>log_frequency</code> Logging interval (in steps) for console and W&amp;B <code>exp_result_path</code> Directory for saving results and checkpoints <p>More options on the optimizer/scheduler and other hyperparameters are available. See the reference for more detail.</p> <p>Just run <code>trainer.fit</code> and pass in the initialized SAE and the activation stream, and keep eyes on the console log to see whether the training goes well!</p> <pre><code>sae.cfg.save_hyperparameters(\"results\") # Save hyperparameter before training\n\ntrainer.fit(\n    sae=sae, \n    activation_stream=activations_stream,\n)\n\nsae.save_pretrained(save_path=\"results\") # Save the trained weight after training\n</code></pre> <p>Consistent Save Path</p> <p>The path in <code>sae.cfg.save_hyperparameters</code> and <code>sae.save_pretrained</code> should be the same as specified in <code>exp_result_path</code> in <code>TrainerConfig</code>. Otherwise, the trained SAE may not be able to be correctly loaded.</p>"},{"location":"train-saes/#using-the-high-level-runner-api","title":"Using the High-Level Runner API","text":"<p>For a more streamlined experience, <code>Language-Model-SAEs</code> also provides a high-level <code>train_sae</code> function that bundles all configuration into a single <code>TrainSAESettings</code> object. You can programmatically create the settings object and call the <code>train_sae</code>, or you can also use a configuration-file based settings and run it with our <code>lm-saes</code> CLI:</p> RunnerCLIFull Script <p>Create the <code>TrainSAESettings</code> in Python and call <code>train_sae</code> with it. </p> <pre><code>import torch\nfrom lm_saes import (\n    TrainSAESettings,\n    train_sae,\n    SAEConfig,\n    InitializerConfig,\n    TrainerConfig,\n    LanguageModelConfig,\n    DatasetConfig,\n    ActivationFactoryConfig,\n    ActivationFactoryDatasetSource,\n    ActivationFactoryTarget,\n    BufferShuffleConfig,\n)\n\nsettings = TrainSAESettings(\n    sae=SAEConfig(\n        hook_point_in=\"blocks.6.hook_resid_post\",\n        hook_point_out=\"blocks.6.hook_resid_post\",\n        d_model=768,\n        expansion_factor=8,\n        act_fn=\"relu\",\n        dtype=torch.float32,\n        device=\"cuda\",\n    ),\n    initializer=InitializerConfig(\n        grid_search_init_norm=True,\n    ),\n    trainer=TrainerConfig(\n        amp_dtype=torch.float32,\n        lr=1e-4,\n        total_training_tokens=800_000_000,\n        log_frequency=1000,\n        exp_result_path=\"results\",\n    ),\n    model=LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    ),\n    model_name=\"pythia-160m\",\n    datasets={\n        \"SlimPajama-3B\": DatasetConfig(\n            dataset_name_or_path=\"Hzfinfdu/SlimPajama-3B\",\n        )\n    },\n    activation_factory=ActivationFactoryConfig(\n        sources=[\n            ActivationFactoryDatasetSource(\n                name=\"SlimPajama-3B\",\n            )\n        ],\n        target=ActivationFactoryTarget.ACTIVATIONS_1D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=4096,\n        buffer_size=4096 * 4,\n        buffer_shuffle=BufferShuffleConfig(\n            perm_seed=42,\n            generator_device=\"cuda\",\n        ),\n    ),\n    sae_name=\"pythia-160m-sae\",\n    sae_series=\"pythia-sae\",\n)\n\ntrain_sae(settings)\n</code></pre> <p>CLI-based workflow requires a configuration file containing the settings consistent with <code>TrainSAESettings</code>. Common configuration file type like TOML, JSON and YAML are supported.</p> <p>Create a TOML configuration file (e.g., <code>train_config.toml</code>) with the following content:</p> <pre><code>sae_name = \"pythia-160m-sae\"\nsae_series = \"pythia-sae\"\nmodel_name = \"pythia-160m\"\ndevice_type = \"cuda\"\n\n[sae]\nsae_type = \"sae\"\nhook_point_in = \"blocks.6.hook_resid_post\"\nhook_point_out = \"blocks.6.hook_resid_post\"\nd_model = 768\nexpansion_factor = 8\nact_fn = \"relu\"\ndtype = \"torch.float32\"\ndevice = \"cuda\"\n\n[initializer]\ngrid_search_init_norm = true\n\n[trainer]\namp_dtype = \"torch.float32\"\nlr = 0.0001\ntotal_training_tokens = 800_000_000\nlog_frequency = 1000\nexp_result_path = \"results\"\n\n[model]\nmodel_name = \"EleutherAI/pythia-160m\"\ndevice = \"cuda\"\ndtype = \"torch.float16\"\n\n[datasets.\"SlimPajama-3B\"]\ndataset_name_or_path = \"Hzfinfdu/SlimPajama-3B\"\n\n[activation_factory]\ntarget = \"activations-1d\"\nhook_points = [\"blocks.6.hook_resid_post\"]\nbatch_size = 4096\nbuffer_size = 16384\n\n[[activation_factory.sources]]\ntype = \"dataset\"\nname = \"SlimPajama-3B\"\n\n[activation_factory.buffer_shuffle]\nperm_seed = 42\ngenerator_device = \"cuda\"\n</code></pre> <p>Then run the training with:</p> <pre><code>lm-saes train train_config.toml\n</code></pre> <p>We also recommend users to directly use the low level semantics for launching training, which allows more granular control and easier customizing:</p> <pre><code>import datasets\nimport torch\nfrom lm_saes import (\n    ActivationFactory,\n    ActivationFactoryConfig,\n    ActivationFactoryDatasetSource,\n    ActivationFactoryTarget,\n    BufferShuffleConfig,\n    Initializer,\n    InitializerConfig,\n    LanguageModelConfig,\n    SAEConfig,\n    Trainer,\n    TrainerConfig,\n    TransformerLensLanguageModel,\n)\n\n# 1. Load Model &amp; Dataset\nmodel = TransformerLensLanguageModel(\n    LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    )\n)\n\ndataset = datasets.load_dataset(\n    \"Hzfinfdu/SlimPajama-3B\",\n    split=\"train\",\n)\n\n# 2. Generate Activations\nactivation_factory = ActivationFactory(\n    ActivationFactoryConfig(\n        sources=[ActivationFactoryDatasetSource(name=\"SlimPajama-3B\")],\n        target=ActivationFactoryTarget.ACTIVATIONS_1D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=4096,\n        buffer_size=4096 * 4,\n        buffer_shuffle=BufferShuffleConfig(\n            perm_seed=42,\n            generator_device=\"cuda\",\n        ),\n    )\n)\n\nactivations_stream = activation_factory.process(\n    model=model,\n    model_name=\"pythia-160m\",\n    datasets={\"SlimPajama-3B\": dataset},\n)\n\n# 3. Create and Initialize SAE\nsae_cfg = SAEConfig(\n    hook_point_in=\"blocks.6.hook_resid_post\",\n    hook_point_out=\"blocks.6.hook_resid_post\",\n    d_model=768,\n    expansion_factor=8,\n    act_fn=\"relu\",\n    dtype=torch.float32,\n    device=\"cuda\",\n)\n\ninitializer = Initializer(InitializerConfig(grid_search_init_norm=True))\nsae = initializer.initialize_sae_from_config(\n    sae_cfg,\n    activation_stream=activations_stream,\n)\n\n# 4. Train SAE\ntrainer = Trainer(\n    TrainerConfig(\n        amp_dtype=torch.float32,\n        lr=1e-4,\n        total_training_tokens=800_000_000,\n        log_frequency=1000,\n        exp_result_path=\"results\",\n    )\n)\n\nsae.cfg.save_hyperparameters(\"results\")\n\ntrainer.fit(\n    sae=sae,\n    activation_stream=activations_stream,\n)\n\nsae.save_pretrained(save_path=\"results\")\n</code></pre>"},{"location":"train-saes/#logging-to-wb","title":"Logging to W&amp;B","text":"<p>Aside from the console logger, we support logging to Weights &amp; Biases for tracking loss and metric changes throughout the training. Training metrics including explained variance and \\(L^0\\) norm will be automatically recorded. Below is a screenshot of the W&amp;B logging:</p> <p></p> <p>Screenshot of W&amp;B logging in training our LlamaScope 2 Beta PLTs.</p> <p>To enable W&amp;B logging, add the <code>wandb</code> configuration to your training setup:</p> RunnerCLIFull Script <pre><code>from lm_saes import WandbConfig\n\nsettings = TrainSAESettings(\n    # ... other settings ...\n    wandb=WandbConfig(\n        wandb_project=\"my-sae-training\",\n        exp_name=\"pythia-160m-sae\",\n    ),\n)\n\ntrain_sae(settings)\n</code></pre> <pre><code># ... other configurations ... \n\n[wandb]\nwandb_project = \"my-sae-training\"\nexp_name = \"pythia-160m-sae\"\n</code></pre> <pre><code>import wandb\n\n# ... other training logics ...\n\n# Create a W&amp;B instance\nwandb_logger = wandb.init(\n    project=\"my-sae-training\",\n    name=\"pythia-160m-sae\",\n)\n\n# Pass it to `trainer.fit`\ntrainer.fit(\n    sae=sae,\n    activation_stream=activations_stream,\n    wandb_logger=wandb_logger,\n)\n\nwandb_logger.finish()\n</code></pre>"},{"location":"train-saes/#checkpoints-and-continue-training","title":"Checkpoints and Continue Training","text":"<p>WIP</p>"},{"location":"train-saes/#activation-functions","title":"Activation Functions","text":"<p>Activation functions are the direct architectural design to enforce a sparse feature activations in SAE and its variants. </p>"},{"location":"train-saes/#relu","title":"ReLU","text":"<p>ReLU is the most classical activation, proposed in initial works (Sparse Autoencoders Find Highly Interpretable Features in Language Models and Towards Monosemanticity: Decomposing Language Models With Dictionary Learning) using SAEs to disentangle superposition. Though its performance is found inferior to other activation functions in term of explained variance and \\(L^0\\) norms, it might be a good starting point to understand how SAE works due to its simplicity.</p> <p>To use ReLU activation function, just set <code>act_fn = \"relu\"</code> in <code>SAEConfig</code>.</p>"},{"location":"train-saes/#jumprelu","title":"JumpReLU","text":"<p>JumpReLU is a state-of-the-art activation function proposed in Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders, and adopted by both Google DeepMind GemmaScope and GemmaScope 2, and Anthropic Cross Layer Transcoder.</p> <p>JumpReLU modifies the ReLU activation function, allowing only elements that passing the corresponding element-wise thresholds to activate. Consider an input element \\(x\\) and a log-threshold \\(t\\), it computes:</p> \\[ \\operatorname{JumpReLU}(x;t) = H(x-e^t)x \\] <p>where \\(H(\\cdot)\\) is the Heaviside step function<sup>1</sup>. For comparison, ReLU can be written as \\(\\operatorname{ReLU}(x) = H(x)x\\).</p> <p>Since the Heaviside step function cannot be differentiated, JumpReLU uses a straight-through estimator of the gradient through the discontinuity of the nonlinearity. See Anthropic Circuit Update - January 2025 to learn how (log) JumpReLU thresholds are optimized.</p> <p>To use the JumpReLU activation function, set <code>act_fn = \"jumprelu\"</code> in <code>SAEConfig</code>. You may also adjust the <code>jumprelu_threshold_window</code> to control the sensitivity of how JumpReLU thresholds update.</p> <p>Dedicated Learning Rate for Log JumpReLU Thresholds</p> <p>In our crosscoder training experiments in Evolution of Concepts in Language Model Pre-Training, we find it better to apply a smaller learning rate (0.1x) to the log JumpReLU thresholds. Though this setting hardly affects the final performance on reconstruction and sparsity, it makes the training loss far more smooth. The mean feature activation becomes lower after the change.</p>"},{"location":"train-saes/#topk","title":"TopK","text":"<p>TopK is an activation function proposed in Scaling and evaluating sparse autoencoders. It keeps only the \\(k\\) largest elements, zeroing out the rest, thus directly enforcing strict sparsity on feature activation. To this end, it removes the need for additional sparsity penalties (which are typically basic requirements for ReLU &amp; JumpReLU activations), and enables direct control over the sparsity quantitative (\\(L^0\\)) of feature activation.</p> <p>To use TopK activation function, set <code>act_fn = \"topk\"</code> in <code>SAEConfig</code>, and set the <code>top_k</code> value to control the final sparsity of feature activation. We also provide some options in <code>TrainerConfig</code> to enable scheduling on \\(k\\) value during training:</p> Parameter Description Default <code>initial_k</code> The starting \\(k\\) value for scheduling. Must be greater than or equal to <code>top_k</code> set in <code>SAEConfig</code>. <code>None</code> <code>k_warmup_steps</code> Steps (int) or fraction of total steps (float) for \\(k\\) to decay from <code>initial_k</code> to <code>top_k</code>. <code>0.1</code> <code>k_cold_booting_steps</code> Steps (int) or fraction of total steps (float) to keep \\(k\\) at <code>initial_k</code> before starting the decay. <code>0</code> <code>k_schedule_type</code> Scheduling strategy: <code>\"linear\"</code> or <code>\"exponential\"</code>. <code>\"linear\"</code> <code>k_exponential_factor</code> Controls the curvature of the exponential decay. <code>3.0</code> <p>Use BatchTopK Activation</p> <p>TopK activation enforces unnecessary fixed allocation of active latents. For strict architectural sparsity control, we recommend using BatchTopK for better performance.</p>"},{"location":"train-saes/#batchtopk","title":"BatchTopK","text":"<p>BatchTopK is a state-of-the-art activation function proposed in BatchTopK Sparse Autoencoders. It follows the idea of TopK to directly enforce sparsity, but replaces the sample-level TopK operation with a batch-level BatchTopK operation. For pre-feature-activations of shape <code>(batch_size, d_sae)</code>, it selects the top <code>batch_size * top_k</code> activations across the entire batch of <code>batch_size</code> samples. This allows for more flexible allocation of active latents.</p> <p>To use TopK activation function, set <code>act_fn = \"batchtopk\"</code> in <code>SAEConfig</code>, and set the <code>top_k</code> value to control the final sparsity of feature activation. We also provide some options in <code>TrainerConfig</code> to enable scheduling on \\(k\\) value during training:</p> Parameter Description Default <code>initial_k</code> The starting \\(k\\) value for scheduling. Must be greater than or equal to <code>top_k</code> set in <code>SAEConfig</code>. <code>None</code> <code>k_warmup_steps</code> Steps (int) or fraction of total steps (float) for \\(k\\) to decay from <code>initial_k</code> to <code>top_k</code>. <code>0.1</code> <code>k_cold_booting_steps</code> Steps (int) or fraction of total steps (float) to keep \\(k\\) at <code>initial_k</code> before starting the decay. <code>0</code> <code>k_schedule_type</code> Scheduling strategy: <code>\"linear\"</code> or <code>\"exponential\"</code>. <code>\"linear\"</code> <code>k_exponential_factor</code> Controls the curvature of the exponential decay. <code>3.0</code>"},{"location":"train-saes/#convert-batchtopk-to-jumprelu","title":"Convert BatchTopK to JumpReLU","text":"<p>BatchTopK introduces a dependency between the activations for the samples in a batch. To eliminate the effect, it's better to estimate a threshold \\(\\theta\\) as average minimum positive activation values, and convert the activation function to JumpReLU with this threshold.</p>"},{"location":"train-saes/#sparsity-penalties","title":"Sparsity Penalties","text":"<p>Activation functions like ReLU and JumpReLU do not strictly enforce sparsity on feature activations. It's the responsibility of the regularization functions to provide dynamics pushing feature activations sparse. <code>Language-Model-SAEs</code> supports the following sparsity penalties on feature activations:</p>"},{"location":"train-saes/#lp-norm","title":"\\(L^p\\)-Norm","text":"<p>Sparsity referes to the number of active latents in feature activation. In principle, we may want to directly add \\(L^0\\) norm to the loss term. However, \\(L^0\\) norm is discontinuous and cannot be differentiated,</p> <p>In practice, \\(L^1\\) norm, as the best convex approximation to \\(L^0\\)<sup>2</sup>, is widely used in SAE training for controlling sparsity without lossing convexity of the optimization. <code>Language-Model-SAEs</code> implements a more general \\(L^p\\) norm as regularization, which is computed as:</p> \\[L_s = \\lambda \\| f(x) \\cdot \\| W_\\text{dec} \\|_2 \\|_p\\] <p>where \\(f(x)\\) is the feature activation, \\(\\| W_\\text{dec} \\|_2\\) is the decoder norm, \\(p\\) is the \\(L^p\\) power, and \\(\\lambda\\) is the coefficient for the sparsity loss term.</p> <p>To use the \\(L^p\\) norm, set <code>sparsity_loss_type=\"power\"</code> in <code>TrainerConfig</code>. Other parameters include:</p> Parameter Description Default <code>l1_coefficient</code> Coefficient \\(\\lambda\\) for the sparsity loss term. <code>0.00008</code> <code>l1_coefficient_warmup_steps</code> Steps (int) or fraction of total steps (float) to warm up the sparsity coefficient from 0. <code>0.1</code> <code>p</code> The power \\(p\\) for \\(L^p\\) norm. Set to \\(1\\) for \\(L^1\\) norm. <code>1</code>"},{"location":"train-saes/#tanh","title":"Tanh","text":"<p>One challenge with \\(L^1\\) penalty is shrinkage: in addition to encouraging sparsity, the penalty encourages activations to be smaller than they would be otherwise. This causes SAEs to recover a smaller fraction of the model loss than might be expected<sup>3</sup>.</p> <p>The \\(\\tanh\\) penalty addresses shrinkage by applying a bounded function to feature activations. For marginal cases where a feature is on the edge of activating, it provides the same gradient towards zero as \\(L^1\\), but for strongly-activating features it provides no penalty and hence no incentive to shrink the activation. The loss is computed as:</p> \\[L_s = \\lambda \\sum_i \\tanh(c \\cdot f_i(x) \\cdot \\| W_{\\text{dec},i} \\|_2)\\] <p>where \\(f_i(x)\\) is the \\(i\\)-th feature activation, \\(\\| W_{\\text{dec},i} \\|_2\\) is the decoder norm for feature \\(i\\), and \\(c\\) is the stretch coefficient. Since \\(\\tanh(x) \\to 1\\) as \\(x \\to \\infty\\), this loss approximates counting the number of active features (\\(L^0\\) norm).</p> <p>While the \\(\\tanh\\) penalty was found to be a Pareto improvement in the \\(L^0\\)/MSE tradeoff, Anthropic's experiments showed that features trained with tanh were much harder to interpret due to many more high-frequency features (some activating on over 10% of inputs). However, their following up experiments show that these high density features don\u2019t seem to be pathological as previous thought.</p> <p>To use the \\(\\tanh\\) penalty, set <code>sparsity_loss_type=\"tanh\"</code> in <code>TrainerConfig</code>. Other parameters include:</p> Parameter Description Default <code>l1_coefficient</code> Coefficient \\(\\lambda\\) for the sparsity loss term. <code>0.00008</code> <code>l1_coefficient_warmup_steps</code> Steps (int) or fraction of total steps (float) to warm up the sparsity coefficient from 0. <code>0.1</code> <code>tanh_stretch_coefficient</code> Stretch coefficient \\(c\\) controlling the steepness of the tanh function. <code>4.0</code>"},{"location":"train-saes/#tanh-quadratic","title":"Tanh-Quadratic","text":"<p>A key issue with standard sparsity penalties (\\(L^0\\), \\(L^1\\), or \\(\\tanh\\)) is that they only control the average number of active features, but are indifferent to the distribution of firing frequencies (See Removing High Frequency Latents from JumpReLU SAEs from the GDM Mech Interp Team for a detailed analysis). This allows some features to fire on a large fraction of inputs (&gt;10%), which often leads to uninterpretable high-frequency features.</p> <p>The tanh-quadratic loss addresses this by adding a quadratic term that specifically penalizes high-frequency features. First, an approximate frequency \\(\\hat{p}_i\\) is computed by averaging the tanh scores across samples:</p> \\[\\hat{p}_i = \\mathbb{E}_{x}\\left[\\tanh(c \\cdot f_i(x) \\cdot \\| W_{\\text{dec},i} \\|_2)\\right]\\] <p>Then the loss is:</p> \\[L_s = \\lambda \\sum_i \\hat{p}_i \\left(1 + \\frac{\\hat{p}_i}{s}\\right)\\] <p>where \\(s\\) is the frequency scale (controlled by <code>frequency_scale</code>). The first term \\(\\hat{p}_i\\) behaves like a standard sparsity penalty for low-frequency features (\\(\\hat{p}_i \\ll s\\)), while the quadratic term \\(\\hat{p}_i^2 / s\\) dominates for high-frequency features (\\(\\hat{p}_i \\gtrsim s\\)), making it increasingly expensive for features to activate on a large fraction of inputs.</p> <p>This formulation successfully eliminates high-frequency latents with only a modest impact on reconstruction loss, while improving frequency-weighted interpretability scores compared to standard JumpReLU SAEs.</p> <p>Note</p> <p>Our implementation of quadratic loss term uses \\(\\tanh\\) as differentiable \\(L^0\\) proxies, which is different to the original proposal by GDM which directly use \\(L^0\\) paired with straight-through estimators.</p> <p>To use tanh-quadratic, set <code>sparsity_loss_type=\"tanh-quad\"</code> in <code>TrainerConfig</code>. Other parameters include:</p> Parameter Description Default <code>l1_coefficient</code> Coefficient \\(\\lambda\\) for the sparsity loss term. <code>0.00008</code> <code>l1_coefficient_warmup_steps</code> Steps (int) or fraction of total steps (float) to warm up the sparsity coefficient from 0. <code>0.1</code> <code>tanh_stretch_coefficient</code> Stretch coefficient \\(c\\) controlling the steepness of the tanh function. <code>4.0</code> <code>frequency_scale</code> Scale factor \\(s\\) for the quadratic penalty. Smaller values penalize high-frequency features more aggressively. Typical values are <code>0.1</code> or <code>0.01</code> to suppress features firing on &gt;10% of tokens. <code>0.01</code>"},{"location":"train-saes/#auxiliary-losses","title":"Auxiliary Losses","text":""},{"location":"train-saes/#jumprelu-pre-act-loss","title":"JumpReLU Pre-act Loss","text":"<p>For JumpReLU SAEs, an additional \\(L_p\\) penalty (called the \"pre-act loss\") proposed by Anthropic applies a small penalty to features which don't fire:</p> \\[L_p = \\lambda_p \\sum_i \\text{ReLU}(e^{\\theta_i} - h_i) \\| W_{\\text{dec},i} \\|_2\\] <p>where \\(\\theta_i\\) is the log-threshold and \\(h_i\\) is the pre-activation. This loss has been found extremely helpful in reducing dead features by providing a gradient signal whenever a feature is inactive, pushing the threshold lower.</p> <p>Note</p> <p>Since this loss provides a gradient signal whenever a feature is inactive, the appropriate scale for <code>lp_coefficient</code> should be a factor of the typical feature activation density lower than other loss terms (e.g., <code>l1_coefficient</code>).</p> <p>To use the JumpReLU pre-act loss, set <code>lp_coefficient</code> to a positive value in <code>TrainerConfig</code>:</p> Parameter Description Default <code>lp_coefficient</code> Coefficient \\(\\lambda_p\\) for the JumpReLU pre-act loss. Recommended value is <code>3e-6</code>. Set to <code>None</code> to disable. <code>None</code>"},{"location":"train-saes/#aux-k-loss","title":"Aux-K Loss","text":"<p>For TopK activation function, an additional auxiliary loss (AuxK) proposed in Scaling and evaluating sparse autoencoders helps revive dead latents during training, similar to Ghost Grads.</p> <p>A latent is flagged as \"dead\" during training if it has not activated for a predetermined number of tokens (typically 10 million). Given the reconstruction error from the main model \\(e = x - \\hat{x}\\), the auxiliary loss models this error using the top-\\(k_{\\text{aux}}\\) dead latents:</p> \\[L_{\\text{aux}} = \\| e - \\hat{e} \\|_2^2\\] <p>where \\(\\hat{e} = W_{\\text{dec}} z_{\\text{dead}}\\) is the reconstruction using only the top-\\(k_{\\text{aux}}\\) dead latents. The full loss is then:</p> \\[L = L_{\\text{rec}} + \\alpha L_{\\text{aux}}\\] <p>where \\(\\alpha\\) is a small coefficient (typically \\(1/32\\)). Since the encoder forward pass can be shared (and dominates decoder cost and encoder backwards cost), adding this auxiliary loss only increases the computational cost by about 10%.</p> <p>To use the Aux-K loss, set <code>auxk_coefficient</code> to a positive value in <code>TrainerConfig</code>:</p> Parameter Description Default <code>auxk_coefficient</code> Coefficient \\(\\alpha\\) for the Aux-K loss. Set to <code>None</code> to disable. Typical value is <code>1/32</code> (~0.03125). <code>None</code> <code>k_aux</code> Number of top dead latents \\(k_{\\text{aux}}\\) to use for auxiliary reconstruction. <code>512</code> <code>dead_threshold</code> Number of tokens a latent must not activate for to be considered dead. <code>10_000_000</code> <p>Note</p> <p>Aux-K loss is specifically designed for TopK SAEs. It will not have effect on other activation functions like ReLU or JumpReLU.</p>"},{"location":"train-saes/#legacy-stategies","title":"Legacy Stategies","text":"<p>Early researches on SAEs employ strategies like Neuron Resampling and Ghost Grads to make dead neurons live again. However, modern initialization and sparsity losses have largely alleviated dead neurons. Thus, we remove the support for these strategies for simplicity of our internal code structure.</p>"},{"location":"train-saes/#caching-activations","title":"Caching Activations","text":"<p>Training with cached activations is a common workflow in practice. It enables efficient hyperparameter sweeping by reusing pre-generated activations and facilitates parallelized training and analysis (DP/TP). This approach significantly accelerates training; for example, training an 8x expansion SAE on Pythia 160M with 800M tokens typically drops from ~6 hours (on-the-fly) to ~30 minutes (cached). However, caching requires substantial disk space. For 800M tokens of activations from a single Pythia 160M layer (\\(d_{\\text{model}}=768\\)) stored in FP16, the storage requirement is:</p> \\[ 800 \\times 10^6 \\times 768 \\times 2 \\text{ bytes} \\approx 1.2 \\text{ TB} \\] <p>In this workflow, a separate task caches activations to disk at the output of <code>ActivationFactory</code>. When training, we re-configure the <code>ActivationFactory</code> to directly read from disk instead of generating activation from the language model on the fly.</p> <p>To cache activation on disk, you can:</p> RunnerCLIFull Script <p>Create the <code>GenerateActivationsSettings</code> in Python and call <code>generate_activations</code> with it. Configurations except <code>output_dir</code> and <code>total_tokens</code> should be consistent with on-the-fly settings above. <code>output_dir</code> is where you want to place your generated activations. Ensure you have enough space at this directory. <code>total_tokens</code> should be equal or greater than the <code>total_training_tokens</code> you want to train your SAE on. </p> <pre><code>from lm_saes import (\n    GenerateActivationsSettings,\n    generate_activations,\n    LanguageModelConfig,\n    DatasetConfig,\n    ActivationFactoryTarget,\n    BufferShuffleConfig,\n)\n\nsettings = GenerateActivationsSettings(\n    model=LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    ),\n    model_name=\"pythia-160m\",\n    dataset=DatasetConfig(dataset_name_or_path=\"Hzfinfdu/SlimPajama-3B\"),\n    dataset_name=\"SlimPajama-3B\",\n    hook_points=[\"blocks.6.hook_resid_post\"],\n    output_dir=\"path/to/activations\",\n    total_tokens=800_000_000,\n    context_size=1024,\n    target=ActivationFactoryTarget.ACTIVATIONS_1D,\n    model_batch_size=32,\n    batch_size=4096,\n    buffer_size=16384,\n    buffer_shuffle=BufferShuffleConfig(\n        perm_seed=42,\n        generator_device=\"cuda\",\n    ),\n    device_type=\"cuda\",\n)\n\ngenerate_activations(settings)\n</code></pre> <p>CLI-based workflow requires a configuration file containing the settings consistent with <code>GenerateActivationsSettings</code>. Common configuration file type like TOML, JSON and YAML are supported.</p> <p>Create a TOML configuration file (e.g., <code>generate_config.toml</code>) with the following content:</p> <pre><code>model_name = \"pythia-160m\"\ndataset_name = \"SlimPajama-3B\"\nhook_points = [\"blocks.6.hook_resid_post\"]\noutput_dir = \"path/to/activations\"\ntotal_tokens = 800_000_000\ncontext_size = 1024\ntarget = \"activations-1d\"\nmodel_batch_size = 32\nbatch_size = 4096\nbuffer_size = 16384\ndevice_type = \"cuda\"\n\n[model]\nmodel_name = \"EleutherAI/pythia-160m\"\ndevice = \"cuda\"\ndtype = \"torch.float16\"\n\n[dataset]\ndataset_name_or_path = \"Hzfinfdu/SlimPajama-3B\"\n\n[buffer_shuffle]\nperm_seed = 42\ngenerator_device = \"cuda\"\n</code></pre> <p>Then run the generation with:</p> <pre><code>lm-saes generate generate_config.toml\n</code></pre> <p>Also, you can directly create <code>ActivationFactory</code> and <code>ActivationWriter</code> instances to generate and write activations to disk.</p> <pre><code>import datasets\nfrom lm_saes import (\n    LanguageModelConfig,\n    TransformerLensLanguageModel,\n    ActivationFactory,\n    ActivationFactoryConfig,\n    ActivationFactoryDatasetSource,\n    ActivationFactoryTarget,\n    BufferShuffleConfig,\n    ActivationWriter,\n    ActivationWriterConfig,\n)\n\n# Use same way to generate activations\nmodel = TransformerLensLanguageModel(\n    LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    )\n)\n\ndataset = datasets.load_dataset(\n    \"Hzfinfdu/SlimPajama-3B\",\n    split=\"train\",\n)\n\nfactory_cfg = ActivationFactoryConfig(\n    sources=[ActivationFactoryDatasetSource(name=\"SlimPajama-3B\")],\n    target=ActivationFactoryTarget.ACTIVATIONS_1D,\n    hook_points=[\"blocks.6.hook_resid_post\"],\n    context_size=1024,\n    model_batch_size=32,\n    batch_size=4096,\n    buffer_size=16384,\n    buffer_shuffle=BufferShuffleConfig(\n        perm_seed=42,\n        generator_device=\"cuda\",\n    ),\n)\nfactory = ActivationFactory(factory_cfg)\n\nactivations = factory.process(\n    model=model,\n    model_name=\"pythia-160m\",\n    datasets={\"SlimPajama-3B\": (dataset, None)},\n)\n\n# Create an ActivationWriter to write the activation stream to disk\nwriter_cfg = ActivationWriterConfig(\n    hook_points=[\"blocks.6.hook_resid_post\"],\n    total_generating_tokens=800_000_000,\n    cache_dir=\"path/to/activations\",\n)\nwriter = ActivationWriter(writer_cfg)\n\nwriter.process(activations)\n</code></pre>"},{"location":"train-saes/#training-with-cached-activations","title":"Training with Cached Activations","text":"<p>Once you have generated and saved activations to disk, you can configure the <code>ActivationFactory</code> to read from these files instead of running the language model. This is done by replacing <code>ActivationFactoryDatasetSource</code> with <code>ActivationFactoryActivationsSource</code> in the configuration.</p> RunnerCLIFull Script <pre><code>import torch\nfrom lm_saes import (\n    TrainSAESettings,\n    train_sae,\n    SAEConfig,\n    TrainerConfig,\n    ActivationFactoryConfig,\n    ActivationFactoryActivationsSource,\n    ActivationFactoryTarget,\n)\n\nsettings = TrainSAESettings(\n    sae=SAEConfig(\n        hook_point_in=\"blocks.6.hook_resid_post\",\n        hook_point_out=\"blocks.6.hook_resid_post\",\n        d_model=768,\n        expansion_factor=8,\n        act_fn=\"relu\",\n        dtype=torch.float32,\n        device=\"cuda\",\n    ),\n    trainer=TrainerConfig(\n        amp_dtype=torch.float32,\n        lr=1e-4,\n        total_training_tokens=800_000_000,\n        exp_result_path=\"results\",\n    ),\n    activation_factory=ActivationFactoryConfig(\n        sources=[\n            ActivationFactoryActivationsSource(\n                path=\"path/to/activations\",\n                name=\"pythia-160m-cached\",\n                device=\"cuda\",\n            )\n        ],\n        target=ActivationFactoryTarget.ACTIVATIONS_1D,\n        hook_points=[\"blocks.6.hook_resid_post\"],\n        batch_size=4096,\n    ),\n    sae_name=\"pythia-160m-sae\",\n    sae_series=\"pythia-sae\",\n)\n\ntrain_sae(settings)\n</code></pre> <p>Update your training configuration to use the <code>activations</code> source type:</p> <pre><code># ... other configurations ...\n\n[activation_factory]\ntarget = \"activations-1d\"\nhook_points = [\"blocks.6.hook_resid_post\"]\nbatch_size = 4096\n\n[[activation_factory.sources]]\ntype = \"activations\"\nname = \"pythia-160m-cached\"\npath = \"path/to/activations\"\ndevice = \"cuda\"\n</code></pre> <p>In a full script, you can omit the language model and dataset loading, and directly use <code>ActivationFactory</code> with cached sources:</p> <pre><code>import torch\nfrom lm_saes import (\n    ActivationFactory,\n    ActivationFactoryConfig,\n    ActivationFactoryActivationsSource,\n    ActivationFactoryTarget,\n    SAEConfig,\n    Trainer,\n    TrainerConfig,\n    SparseAutoEncoder,\n)\n\n# 1. Configure Activation Factory with Cached Source\nfactory_cfg = ActivationFactoryConfig(\n    sources=[\n        ActivationFactoryActivationsSource(\n            path=\"path/to/activations\",\n            name=\"pythia-160m-cached\",\n            device=\"cuda\",\n        )\n    ],\n    target=ActivationFactoryTarget.ACTIVATIONS_1D,\n    hook_points=[\"blocks.6.hook_resid_post\"],\n    batch_size=4096,\n)\n\nfactory = ActivationFactory(factory_cfg)\nactivations_stream = factory.process()\n\n# 2. Initialize SAE and Trainer\nsae = SparseAutoEncoder(SAEConfig(\n    hook_point_in=\"blocks.6.hook_resid_post\",\n    hook_point_out=\"blocks.6.hook_resid_post\",\n    d_model=768,\n    expansion_factor=8,\n    act_fn=\"relu\",\n    device=\"cuda\",\n))\n\ntrainer = Trainer(TrainerConfig(\n    lr=1e-4,\n    total_training_tokens=800_000_000,\n    exp_result_path=\"results\",\n))\n\n# 3. Train\ntrainer.fit(sae=sae, activation_stream=activations_stream)\n</code></pre>"},{"location":"train-saes/#use-huggingface-backend","title":"Use HuggingFace Backend","text":"<p>While TransformerLens provides a unified set of hook points across numerous Transformer variants, there are compelling reasons to use the Hugging Face Transformers library directly for generating activations:</p> <ul> <li>Provides a wider range of model architectures. Almost every frontier model is released with official Hugging Face-compatible modeling code, ensuring immediate support for the latest architectures.</li> <li>Integrates GPU accelerate kernels, e.g. Flash Attention, for faster activation generation.</li> <li>Reduces numerical errors. TransformerLens re-implements model logic using more semantic linear algebraic operation (primarily Einsum), which will inevitably introduce subtle numerical discrepancies. Direct usage of Huggingface ensures the activation used for training sparse dictionaries is just the same as whatever you use for other purpose.</li> </ul> <p>To use the HuggingFace backend, you simply need to change the <code>backend</code> field in <code>LanguageModelConfig</code> to <code>\"huggingface\"</code>, or (in full script) load the model with <code>HuggingFaceLanguageModel</code> wrapper.</p> RunnerCLIFull Script <pre><code># ...\n\nsettings = TrainSAESettings(\n    # ...\n    model=LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n        backend=\"huggingface\",  # Set backend to huggingface\n    ),\n    # ...\n)\n\ntrain_sae(settings)\n</code></pre> <pre><code># ...\n\n[model]\nmodel_name = \"EleutherAI/pythia-160m\"\ndevice = \"cuda\"\ndtype = \"torch.float16\"\nbackend = \"huggingface\"\n\n# The rest of the configuration remains the same\n</code></pre> <pre><code>from lm_saes import HuggingFaceLanguageModel, LanguageModelConfig\n\nmodel = HuggingFaceLanguageModel(\n    LanguageModelConfig(\n        model_name=\"EleutherAI/pythia-160m\",\n        device=\"cuda\",\n        dtype=\"torch.float16\",\n    )\n)\n\n# The rest of the script remains the same\n</code></pre> <ol> <li> <p>The Heaviside step function \\(H(x)\\) is defined as:</p> \\[ H(x) = \\begin{cases}     1 &amp; \\text{if } x &gt; 0 \\\\     0 &amp; \\text{otherwise} \\end{cases} \\] <p>\u21a9</p> </li> <li> <p>See more discussion on \\(L^0\\) approximate functions in Comparing Measures of Sparsity and Why \\(\\ell_1\\) Is a Good Approximation to \\(\\ell_0\\): A Geometric Explanation.\u00a0\u21a9</p> </li> <li> <p>See Fixing Feature Suppression in SAEs for more discussion.\u00a0\u21a9</p> </li> </ol>"},{"location":"models/lorsa/","title":"Low-Rank Sparse Attention","text":""},{"location":"models/lorsa/#low-rank-sparse-attention-lorsa","title":"Low-Rank Sparse Attention (Lorsa)","text":"<p>Low-Rank Sparse Attention (Lorsa) is a specialized sparse dictionary architecture designed to decompose attention layers into interpretable sparse components. Unlike standard SAEs that treat attention as a black box, Lorsa explicitly models the query-key-value structure while maintaining sparsity and interpretability.</p> <p>Given an input sequence \\(X \\in \\mathbb{R}^{n \\times d}\\), Lorsa has:</p> <ul> <li>\\(n_{\\text{qk\\_heads}}\\) QK heads, each with projections \\(W_q^h, W_k^h \\in \\mathbb{R}^{d \\times d_{\\text{qk\\_head}}}\\)</li> <li>\\(n_{\\text{ov\\_heads}}\\) rank-1 OV heads, each with projections \\(\\mathbf{w}_v^i \\in \\mathbb{R}^{d \\times 1}\\), \\(\\mathbf{w}_o^i \\in \\mathbb{R}^{1 \\times d}\\)</li> </ul> <p>Every group of \\(n_{\\text{ov\\_heads}} / n_{\\text{qk\\_heads}}\\) consecutive OV heads shares the same QK head. Denote the QK head assigned to OV head \\(i\\) as \\(h(i)\\). The forward pass for each OV head \\(i\\) is:</p> \\[ \\begin{aligned} Q^{h(i)} &amp;= X W_q^{h(i)}, \\quad K^{h(i)} = X W_k^{h(i)} \\\\ A^{h(i)} &amp;= \\operatorname{softmax}\\!\\left(\\frac{Q^{h(i)} {(K^{h(i)})}^\\top}{\\sqrt{d_{\\text{qk\\_head}}}}\\right) \\in \\mathbb{R}^{n \\times n} \\\\ \\tilde{\\mathbf{z}}^i &amp;= A^{h(i)}\\, (X \\mathbf{w}_v^i) \\in \\mathbb{R}^{n \\times 1} \\end{aligned} \\] <p>The pre-activations across all OV heads are then passed through a sparsity-inducing activation function \\(\\sigma(\\cdot)\\):</p> \\[ [\\mathbf{z}^0, \\ldots, \\mathbf{z}^{n_{\\text{ov\\_heads}}-1}] = \\sigma([\\tilde{\\mathbf{z}}^0, \\ldots, \\tilde{\\mathbf{z}}^{n_{\\text{ov\\_heads}}-1}]) \\] <p>The final output sums the contributions of all OV heads weighted by their activations:</p> \\[ \\hat{Y} = \\sum_{i=0}^{n_{\\text{ov\\_heads}}-1} \\mathbf{z}^i\\, (\\mathbf{w}_o^i)^\\top \\in \\mathbb{R}^{n \\times d} \\] <p>The architecture was introduced in Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition (ICLR 2026), which proposes using sparse dictionary learning to address attention superposition\u2014the challenge of disentangling attention-mediated interactions between features at different token positions. For detailed architectural specifications and mathematical formulations, please refer to this paper.</p>"},{"location":"models/lorsa/#configuration","title":"Configuration","text":"<p>Lorsa is configured using the <code>LorsaConfig</code> class. All sparse dictionary models inherit common parameters from <code>BaseSAEConfig</code>. See the Common Configuration Parameters section for the full list of inherited parameters.</p>"},{"location":"models/lorsa/#lorsa-specific-parameters","title":"Lorsa-Specific Parameters","text":"<pre><code>from lm_saes import LorsaConfig\nimport torch\n\nlorsa_config = LorsaConfig(\n    # Hook points\n    hook_point_in=\"blocks.13.ln1.hook_normalized\",\n    hook_point_out=\"blocks.13.hook_attn_out\",\n\n    # Attention dimensions\n    n_qk_heads=16,\n    d_qk_head=128,\n    n_ctx=2048,\n\n    # Positional embeddings\n    positional_embedding_type=\"rotary\",\n    rotary_dim=128,\n    rotary_base=1000000,\n    rotary_adjacent_pairs=False,\n    rotary_scale=1,\n\n    # NTK-aware RoPE (optional)\n    use_NTK_by_parts_rope=False,\n    NTK_by_parts_factor=1.0,\n    NTK_by_parts_low_freq_factor=1.0,\n    NTK_by_parts_high_freq_factor=1.0,\n    old_context_len=2048,\n\n    # Attention settings\n    attn_scale=None,\n    use_post_qk_ln=True,\n    normalization_type=\"RMS\",\n    eps=1e-6,\n\n    # Common parameters\n    d_model=2048,\n    expansion_factor=32,\n    act_fn=\"topk\",\n    top_k=256,\n    dtype=torch.float32,\n    device=\"cuda\",\n)\n</code></pre>"},{"location":"models/lorsa/#hook-points","title":"Hook Points","text":"Parameter Type Description Default <code>hook_point_in</code> <code>str</code> Input hook point, typically the attention input (e.g., <code>blocks.L.ln1.hook_normalized</code>). Must differ from <code>hook_point_out</code> Required <code>hook_point_out</code> <code>str</code> Output hook point, typically the attention output (e.g., <code>blocks.L.hook_attn_out</code>). Must differ from <code>hook_point_in</code> Required"},{"location":"models/lorsa/#attention-dimensions","title":"Attention Dimensions","text":"<p>We recommend setting <code>d_qk_head</code> to match the target model's head dimension. <code>n_qk_heads</code> can be freely chosen: a natural starting point is <code>n_qk_heads = n_heads * expansion_factor</code> (n_heads is the num of attention heads of target attention layer), though a smaller value is also reasonable if you want to reduce Lorsa's parameter count(not less than <code>n_heads</code>).</p> Parameter Type Description Default <code>n_qk_heads</code> <code>int</code> Number of QK heads. Required <code>d_qk_head</code> <code>int</code> Dimension per QK head. Required <code>n_ctx</code> <code>int</code> Maximum context length. Required <p>Number of OV Heads</p> <p>The number of OV heads is automatically computed as: <code>n_ov_heads = expansion_factor * d_model</code> (same as <code>d_sae</code>).</p>"},{"location":"models/lorsa/#positional-embeddings","title":"Positional Embeddings","text":"<p>It is strongly recommended to copy the positional embedding parameters directly from the target model's implementation. Incorrect settings will make it harder for Lorsa to learn the target attention patterns.</p> Parameter Type Description Default <code>positional_embedding_type</code> <code>str</code> Type of positional embedding: <code>\"rotary\"</code> or <code>\"none\"</code> <code>\"rotary\"</code> <code>rotary_dim</code> <code>int</code> Dimension of rotary embeddings (typically <code>d_qk_head</code>) Required <code>rotary_base</code> <code>int</code> Base for rotary embeddings frequency <code>10000</code> <code>rotary_adjacent_pairs</code> <code>bool</code> Whether to apply RoPE on adjacent pairs <code>True</code> <code>rotary_scale</code> <code>int</code> Scaling factor of the head dimension for rotary embeddings <code>1</code>"},{"location":"models/lorsa/#ntk-aware-rope-only-for-llama-31-and-32-herd-models","title":"NTK-Aware RoPE (only for Llama 3.1 and 3.2 herd models)","text":"Parameter Type Description Default <code>use_NTK_by_parts_rope</code> <code>bool</code> Enable NTK-aware RoPE scaling for extended context <code>False</code> <code>NTK_by_parts_factor</code> <code>float</code> NTK scaling factor <code>1.0</code> <code>NTK_by_parts_low_freq_factor</code> <code>float</code> Low-frequency component scaling factor <code>1.0</code> <code>NTK_by_parts_high_freq_factor</code> <code>float</code> High-frequency component scaling factor <code>1.0</code> <code>old_context_len</code> <code>int</code> Original context length before scaling <code>2048</code>"},{"location":"models/lorsa/#attention-computation-details","title":"Attention Computation Details","text":"Parameter Type Description Default <code>attn_scale</code> <code>float | None</code> Attention scaling factor. If <code>None</code>, uses \\(\\frac{1}{\\sqrt{d_{\\text{qk\\_head}}}}\\) <code>None</code> <code>use_post_qk_ln</code> <code>bool</code> Apply LayerNorm/RMSNorm after computing Q and K projections <code>False</code> <code>normalization_type</code> <code>str | None</code> Normalization type: <code>\"LN\"</code> (LayerNorm) or <code>\"RMS\"</code> (RMSNorm). Only used when <code>use_post_qk_ln=True</code> <code>None</code> <code>eps</code> <code>float</code> Epsilon for numerical stability in normalization <code>1e-6</code>"},{"location":"models/lorsa/#initialization-strategy","title":"Initialization Strategy","text":"<p>For Lorsa, initialization from the original model's attention weights is highly recommended:</p> <pre><code>InitializerConfig(\n    grid_search_init_norm=True,\n    initialize_lorsa_with_mhsa=True,  # Initialize Q, K from attention weights\n    initialize_W_D_with_active_subspace=True,  # Initialize V, O from attention weights\n    model_layer=13,  # Specify layer to extract attention weights from\n)\n</code></pre> <p>This initialization helps Lorsa start from a good approximation of the attention computation.</p>"},{"location":"models/lorsa/#training","title":"Training","text":""},{"location":"models/lorsa/#basic-training-setup","title":"Basic Training Setup","text":"<p>Lorsa requires 2D activations with sequence dimension preserved (<code>ActivationFactoryTarget.ACTIVATIONS_2D</code>) since it models positional attention patterns:</p> <pre><code>from lm_saes import (\n    TrainLorsaSettings,\n    train_lorsa,\n    LorsaConfig,\n    InitializerConfig,\n    TrainerConfig,\n    ActivationFactoryConfig,\n    ActivationFactoryActivationsSource,\n    ActivationFactoryTarget,\n    LanguageModelConfig,\n)\nimport torch\n\nsettings = TrainLorsaSettings(\n    sae=LorsaConfig(\n        hook_point_in=\"blocks.13.ln1.hook_normalized\",\n        hook_point_out=\"blocks.13.hook_attn_out\",\n        # ... other settings ...\n    ),\n    initializer=InitializerConfig(\n        grid_search_init_norm=True,\n        initialize_lorsa_with_mhsa=True,  # Initialize with original attention weights\n        initialize_W_D_with_active_subspace=True,\n        model_layer=13,\n    ),\n    trainer=TrainerConfig(\n        lr=2e-4,\n        total_training_tokens=800_000_000,\n        initial_k=256,\n        k_warmup_steps=1500,\n        log_frequency=1000,\n        exp_result_path=\"results/lorsa\",\n    ),\n    activation_factory=ActivationFactoryConfig(\n        sources=[\n            ActivationFactoryActivationsSource(\n                path=\"path/to/cached/activations\",\n                name=\"lorsa-activations\",\n                device=\"cuda\",\n            )\n        ],\n        target=ActivationFactoryTarget.ACTIVATIONS_2D,  # Preserve sequence dimension\n        hook_points=[\n            \"blocks.13.ln1.hook_normalized\",\n            \"blocks.13.hook_attn_out\",\n        ],\n        batch_size=16,  # Batch size is per-sequence, not per-token\n    ),\n    sae_name=\"qwen-lorsa\",\n    sae_series=\"qwen-interpretability\",\n    model_name=\"Qwen/Qwen3-1.7B\",\n    model=LanguageModelConfig(\n        model_name=\"Qwen/Qwen3-1.7B\",\n        device=\"cuda\",\n        dtype=torch.float16,\n        model_from_pretrained_path=\"path/to/model\",\n    ),\n    data_parallel_size=1,\n    model_parallel_size=1,\n)\n\ntrain_lorsa(settings)\n</code></pre>"},{"location":"models/lorsa/#important-training-considerations","title":"Important Training Considerations","text":"<ol> <li> <p>Sequence batching: Since Lorsa operates on sequences, <code>batch_size</code> in <code>ActivationFactoryConfig</code> represents the number of sequences (not tokens). The effective token batch size is <code>batch_size * n_ctx</code>.</p> </li> <li> <p>Memory requirements: Lorsa stores attention patterns and requires more memory than standard SAEs. Consider using parallelism (see distributed-guidelines) reducing batch size.</p> </li> <li> <p>Context length: Ensure <code>n_ctx</code> in <code>LorsaConfig</code> matches the <code>context_size</code> in <code>ActivationFactoryConfig</code> during activation generation.</p> </li> </ol>"},{"location":"models/overview/","title":"Overview","text":""},{"location":"models/overview/#sparse-dictionary-models","title":"Sparse Dictionary Models","text":"<p>This section provides comprehensive documentation for the various sparse dictionary architectures supported by <code>Language-Model-SAEs</code>. While all models share the common goal of learning interpretable sparse representations of neural network activations, they differ in their architectural designs and the computational patterns they aim to capture.</p>"},{"location":"models/overview/#overview","title":"Overview","text":"<p><code>Language-Model-SAEs</code> supports multiple sparse dictionary variants:</p> <ul> <li> <p>Sparse Autoencoder (SAE): The foundational architecture that learns to decompose activations from a single layer into sparse, interpretable features.</p> </li> <li> <p>Transcoder: Also known as Per-Layer Transcoder (PLT), this variant reads from one hook point and writes to another, enabling the decomposition of specific computational units like MLP layers.</p> </li> <li> <p>Cross Layer Transcoder (CLT): An advanced architecture that captures cross-layer interactions by allowing features extracted at one layer to influence reconstructions at multiple downstream layers.</p> </li> <li> <p>Low-Rank Sparse Attention (Lorsa): A specialized architecture designed to decompose attention computations into interpretable sparse components.</p> </li> </ul>"},{"location":"models/overview/#common-configuration-parameters","title":"Common Configuration Parameters","text":"<p>All sparse dictionary variants inherit from <code>BaseSAEConfig</code>, which provides common configuration parameters. These parameters are available for all model types unless specifically overridden.</p>"},{"location":"models/overview/#core-architecture-parameters","title":"Core Architecture Parameters","text":"Parameter Type Description Default <code>d_model</code> <code>int</code> The dimension of the input/label activation space. In common settings where activations come from a transformer, this is the dimension of the model (also be known as <code>hidden_size</code>) Required <code>expansion_factor</code> <code>float</code> The expansion factor of the sparse dictionary. The hidden dimension of the sparse dictionary <code>d_sae</code> is <code>d_model * expansion_factor</code> Required <code>use_decoder_bias</code> <code>bool</code> Whether to use a bias term in the decoder. Including a bias term may make it easier to train a better sparse dictionary, in exchange for increased architectural complexity <code>True</code>"},{"location":"models/overview/#activation-function-parameters","title":"Activation Function Parameters","text":"Parameter Type Description Default <code>act_fn</code> <code>str</code> The activation function to use for the sparse dictionary. Options: <code>\"relu\"</code>, <code>\"jumprelu\"</code>, <code>\"topk\"</code>, <code>\"batchtopk\"</code>, <code>\"batchlayertopk\"</code>, <code>\"layertopk\"</code>. See Activation Functions for details <code>\"relu\"</code> <code>top_k</code> <code>int</code> The k value to use for the TopK family of activation functions. For vanilla TopK, the L0 norm of the feature activations is <code>top_k</code> <code>50</code> <p>Activation function descriptions:</p> <ul> <li><code>relu</code>: ReLU activation function. Used in the most vanilla SAE settings.</li> <li><code>jumprelu</code>: JumpReLU activation function, adding a trainable element-wise threshold that pre-activations must pass to be activated. Proposed in Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders.</li> <li><code>topk</code>: TopK activation function. Retains the top K activations per sample, zeroing out the rest. Proposed in Scaling and evaluating sparse autoencoders.</li> <li><code>batchtopk</code>: BatchTopK activation function. Relaxes TopK to batch-level, selecting the top <code>k * batch_size</code> activations per batch. Allows more adaptive allocation of latents on each sample. Proposed in BatchTopK Sparse Autoencoders.</li> <li><code>batchlayertopk</code>: (For CrossLayerTranscoder only) Extension of BatchTopK to layer-and-batch-aware, retaining the top <code>k * batch_size * n_layers</code> activations per batch and layer.</li> <li><code>layertopk</code>: (For CrossLayerTranscoder only) Extension of TopK to layer-aware, retaining the top <code>k * n_layers</code> activations per layer.</li> </ul>"},{"location":"models/overview/#jumprelu-specific-parameters","title":"JumpReLU-Specific Parameters","text":"Parameter Type Description Default <code>jumprelu_threshold_window</code> <code>float</code> The window size for the JumpReLU threshold. When pre-activations are element-wise in the window-neighborhood of the threshold, the threshold will begin to receive gradient. See Anthropic's Circuits Update - January 2025 for more details <code>2.0</code>"},{"location":"models/overview/#activation-normalization-parameters","title":"Activation Normalization Parameters","text":"Parameter Type Description Default <code>norm_activation</code> <code>str</code> The activation normalization strategy. Options: <code>\"token-wise\"</code>, <code>\"batch-wise\"</code>, <code>\"dataset-wise\"</code>, <code>\"inference\"</code>. During training, input/label activations are normalized to an average norm of \\(\\sqrt{d_{\\text{model}}}\\), allowing easier hyperparameter transfer between different model scales <code>\"dataset-wise\"</code> <p>Normalization strategies:</p> <ul> <li><code>token-wise</code>: Norm is directly computed for activation from each token. No averaging is performed.</li> <li><code>batch-wise</code>: Norm is computed for each batch, then averaged over the batch dimension.</li> <li><code>dataset-wise</code>: Norm is computed from several samples from the activation. Gives a fixed value of average norm for all activations, preserving the linearity of pre-activation encoding and decoding.</li> <li><code>inference</code>: No normalization is performed. Produced after calling <code>standardize_parameters_of_dataset_norm</code> method, which folds the dataset-wise average norm into the weights and biases.</li> </ul>"},{"location":"models/overview/#sparsity-parameters","title":"Sparsity Parameters","text":"Parameter Type Description Default <code>sparsity_include_decoder_norm</code> <code>bool</code> Whether to include the decoder norm term in feature activation gating. If true, pre-activation hidden states will be scaled by the decoder norm before applying the activation function, then scaled back after. This suppresses the training dynamics where the model tries to increase decoder norm in exchange for smaller feature activation magnitude <code>True</code>"},{"location":"models/overview/#performance-optimization-parameters","title":"Performance Optimization Parameters","text":"Parameter Type Description Default <code>use_triton_kernel</code> <code>bool</code> Whether to use the Triton SpMM kernel for sparse matrix multiplication. Currently only supported for vanilla SAE <code>False</code> <code>sparsity_threshold_for_triton_spmm_kernel</code> <code>float</code> The sparsity threshold for the Triton SpMM kernel. Only when feature activation sparsity reaches this threshold will the Triton SpMM kernel be used. Useful for JumpReLU or TopK with a k annealing schedule <code>0.996</code>"},{"location":"models/sae/","title":"Sparse Autoencoder","text":""},{"location":"models/sae/#sparse-autoencoder-sae","title":"Sparse Autoencoder (SAE)","text":"<p>Sparse Autoencoders (SAEs) are the foundational architecture for learning interpretable features from language model activations. They decompose neural network activations into sparse, interpretable features that help address the superposition problem.</p> <p>Given a model activation vector \\(\\mathbf{x} \\in \\mathbb{R}^{d_{\\text{model}}}\\), an SAE first encodes it into a high-dimensional sparse latent representation, then decodes it back to reconstruct the original activation:</p> \\[ \\begin{aligned} \\mathbf{z} &amp;= \\sigma(W_E \\mathbf{x} + \\mathbf{b}_E) \\in \\mathbb{R}^{d_{\\text{SAE}}} \\\\ \\hat{\\mathbf{x}} &amp;= W_D \\mathbf{z} + \\mathbf{b}_D \\in \\mathbb{R}^{d_{\\text{model}}} \\end{aligned} \\] <p>where \\(W_E \\in \\mathbb{R}^{d_{\\text{SAE}} \\times d_{\\text{model}}}\\) and \\(W_D \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{SAE}}}\\) are the encoder and decoder weight matrices, \\(\\mathbf{b}_E, \\mathbf{b}_D\\) are bias terms, and \\(\\sigma(\\cdot)\\) is a sparsity-inducing activation function (e.g., ReLU, TopK). The model is trained to minimize the reconstruction loss \\(\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2\\) while keeping \\(\\mathbf{z}\\) sparse, encouraging each dimension of \\(\\mathbf{z}\\) to correspond to a monosemantic feature.</p> <p>The architecture was introduced in foundational works including Sparse Autoencoders Find Highly Interpretable Features in Language Models and Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. For detailed architectural specifications and mathematical formulations, please refer to these papers.</p>"},{"location":"models/sae/#configuration","title":"Configuration","text":"<p>SAEs are configured using the <code>SAEConfig</code> class. All sparse dictionary models inherit common parameters from <code>BaseSAEConfig</code>. See the Common Configuration Parameters section for the full list of inherited parameters.</p>"},{"location":"models/sae/#sae-specific-parameters","title":"SAE-Specific Parameters","text":"<pre><code>from lm_saes import SAEConfig\nimport torch\n\nsae_config = SAEConfig(\n    # SAE-specific parameters\n    hook_point_in=\"blocks.6.hook_resid_post\",\n    hook_point_out=\"blocks.6.hook_resid_post\",  # Same as hook_point_in for SAE\n    use_glu_encoder=False,\n\n    # Common parameters (documented in Sparse Dictionaries overview)\n    d_model=768,\n    expansion_factor=8,\n    act_fn=\"topk\",\n    top_k=64,\n    dtype=torch.float32,\n    device=\"cuda\",\n)\n</code></pre> Parameter Type Description Default <code>hook_point_in</code> <code>str</code> Hook point to read activations from. For SAE, this is typically the same as <code>hook_point_out</code> Required <code>hook_point_out</code> <code>str</code> Hook point to write reconstructions to. For SAE, this is typically the same as <code>hook_point_in</code> Required <code>use_glu_encoder</code> <code>bool</code> Whether to use a Gated Linear Unit (GLU) in the encoder. GLU can improve expressiveness but increases parameter count <code>False</code> <p>SAE vs Transcoder</p> <p>For standard SAEs, <code>hook_point_in</code> and <code>hook_point_out</code> are identical, meaning the SAE reads from and reconstructs to the same point in the model. When these two hook points differ, the configuration defines a Transcoder instead.</p>"},{"location":"models/sae/#initialization-strategy","title":"Initialization Strategy","text":"<p>Proper initialization is crucial for training high-quality SAEs. We recommend the following configuration:</p> <pre><code>from lm_saes import InitializerConfig\n\ninitializer = InitializerConfig(\n    bias_init_method=\"geometric_median\",\n    grid_search_init_norm=True,\n    init_encoder_bias_with_mean_hidden_pre=True,\n    # ... (e.g. init_log_jumprelu_threshold_value if use)\n)\n</code></pre> Parameter Recommended Value Description <code>bias_init_method</code> <code>\"geometric_median\"</code> Initializes the decoder bias using the geometric median of the activation distribution, which is more robust to skewed/biased activations than <code>\"all_zero\"</code> <code>grid_search_init_norm</code> <code>True</code> Performs a grid search to find the optimal encoder/decoder weight scale that minimizes initial MSE loss <code>init_encoder_bias_with_mean_hidden_pre</code> <code>True</code> Initializes the encoder bias with the mean of the pre-activation distribution, which is more robust to skewed/biased activations and stabilizes early training"},{"location":"models/sae/#initialization-for-low-rank-activations","title":"Initialization for Low-Rank Activations","text":"<p>When training SAEs on low-rank activations (such as attention outputs), dead features become a prevalent problem due to the dimensional collapse in the activation space. As shown in Dimensional Collapse in Transformer Attention Outputs: A Challenge for Sparse Dictionary Learning, attention outputs are confined to a surprisingly low-dimensional subspace (only ~60% of the full space), creating a mismatch between randomly initialized features and the intrinsic geometry of the activation space.</p> <p>To address this issue, we recommend the following additional configuration:</p> <pre><code>initializer = InitializerConfig(\n    bias_init_method=\"geometric_median\",\n    grid_search_init_norm=True,\n    init_encoder_bias_with_mean_hidden_pre=True,\n    initialize_W_D_with_active_subspace=True,\n    d_active_subspace=384,  # Adjust based on effective rank (e.g., 0.5 * d_model)\n    # ... (e.g. init_log_jumprelu_threshold_value if use)\n)\n</code></pre> Parameter Recommended Value Description <code>initialize_W_D_with_active_subspace</code> <code>True</code> Constrains decoder features to the active subspace of the activations using PCA or SVD, ensuring features align with the intrinsic geometry <code>d_active_subspace</code> <code>~0.5 * d_model</code> Dimension of the active subspace. Should be adjusted based on the effective rank of your activations. For a model with <code>d_model=768</code>, starting with <code>384</code> is a good baseline <p>This subspace-constrained initialization dramatically reduces dead features in attention output SAEs. The appropriate value for <code>d_active_subspace</code> depends on the effective rank of your specific activations and may require some tuning.</p>"},{"location":"models/sae/#training","title":"Training","text":"<p>Training an SAE follows the same workflow as described in the Train SAEs guide. </p>"},{"location":"models/transcoder/","title":"Transcoder","text":""},{"location":"models/transcoder/#transcoder-per-layer-transcoder","title":"Transcoder (Per-Layer Transcoder)","text":"<p>Transcoders, also known as Per-Layer Transcoders (PLTs), are a variant of Sparse Autoencoders that read activations from one hook point and reconstruct at a different hook point within the same layer. This enables the decomposition of specific computational units, such as MLP sublayers, into interpretable sparse features. Unlike standard SAEs where <code>hook_point_in == hook_point_out</code>, transcoders have different input and output hook points. This allows them to faithfully approximate a computational unit (like an MLP layer) with a wider, sparsely-activating layer, making fine-grained circuit analysis more tractable.</p> <p>Transcoders were introduced in the following papers: Automatically Identifying Local and Global Circuits with Linear Computation Graphs (Ge et al., 2024) and Transcoders Find Interpretable LLM Feature Circuits (Dunefsky et al., 2024). These works demonstrate that transcoders can effectively decompose MLP computations into interpretable circuits while maintaining reconstruction fidelity. For detailed architectural specifications and mathematical formulations, please refer to these papers.</p>"},{"location":"models/transcoder/#configuration","title":"Configuration","text":"<p>Transcoders use the same <code>SAEConfig</code> and <code>InitializerConfig</code> as standard SAEs. See the SAE configuration guide for the full parameter reference.</p> <p>The only essential difference is that <code>hook_point_in</code> and <code>hook_point_out</code> must point to different locations\u2014typically the input and output of the MLP sublayer you want to decompose:</p> <pre><code>transcoder_config = SAEConfig(\n    hook_point_in=\"blocks.6.ln2.hook_normalized\",  # before MLP\n    hook_point_out=\"blocks.6.hook_mlp_out\",        # after MLP\n    ...\n)\n</code></pre>"},{"location":"models/transcoder/#initialization-strategy","title":"Initialization Strategy","text":"<p>Proper initialization is crucial for training high-quality transcoders. We recommend the following configuration:</p> <pre><code>from lm_saes import InitializerConfig\n\ninitializer = InitializerConfig(\n    bias_init_method=\"geometric_median\",\n    init_encoder_bias_with_mean_hidden_pre=True,\n    init_encoder_with_decoder_transpose=False,\n    grid_search_init_norm=True,\n    initialize_tc_with_mlp=True,\n    model_layer=6,  # Specify which layer to extract MLP weights from\n)\n</code></pre> Parameter Recommended Value Description <code>bias_init_method</code> <code>\"geometric_median\"</code> Initializes the decoder bias using the geometric median of the activation distribution, which is more robust to skewed/biased activations than <code>\"all_zero\"</code> <code>init_encoder_bias_with_mean_hidden_pre</code> <code>True</code> Initializes the encoder bias with the mean of the pre-activation distribution, which is more robust to skewed/biased activations and stabilizes early training <code>init_encoder_with_decoder_transpose</code> <code>False</code> Disables encoder initialization from decoder transpose. This is typically set to <code>False</code> when training transcoder <code>grid_search_init_norm</code> <code>True</code> Performs a grid search to find the optimal encoder/decoder weight scale that minimizes initial MSE loss <code>initialize_tc_with_mlp</code> <code>True</code> Initializes the transcoder decoder weights with the corresponding MLP layer weights. This helps the transcoder start from a good approximation of the MLP computation <code>model_layer</code> Layer index Specifies which layer to extract MLP weights from. Should match the layer number in your <code>hook_point_in</code>/<code>hook_point_out</code> configuration <p>This initialization strategy is particularly effective for transcoders decomposing MLP sublayers, as it allows the transcoder to start from a good approximation of the target computation and converge faster during training.</p>"},{"location":"models/transcoder/#training","title":"Training","text":"<p>Training a Transcoder follows the same workflow as described in the Train SAEs guide. </p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Models</li> <li>Training</li> <li>Evaluation</li> <li>Activation</li> <li>Analysis</li> <li>Runners</li> <li>Infrastructure</li> </ul>"},{"location":"reference/activation/","title":"Activation","text":""},{"location":"reference/activation/#activation","title":"Activation","text":"<p>Activation extraction, caching, and processing.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig","title":"ActivationFactoryConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>sources</code>                 (<code>list[ActivationFactoryDatasetSource | ActivationFactoryActivationsSource]</code>)             </li> <li> <code>target</code>                 (<code>ActivationFactoryTarget</code>)             </li> <li> <code>hook_points</code>                 (<code>list[str]</code>)             </li> <li> <code>batch_size</code>                 (<code>int</code>)             </li> <li> <code>num_workers</code>                 (<code>int</code>)             </li> <li> <code>context_size</code>                 (<code>int | None</code>)             </li> <li> <code>model_batch_size</code>                 (<code>int</code>)             </li> <li> <code>override_dtype</code>                 (<code>dtype | None</code>)             </li> <li> <code>buffer_size</code>                 (<code>int | None</code>)             </li> <li> <code>buffer_shuffle</code>                 (<code>BufferShuffleConfig | None</code>)             </li> <li> <code>ignore_token_ids</code>                 (<code>list[int] | None</code>)             </li> </ul>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.sources","title":"sources  <code>pydantic-field</code>","text":"<pre><code>sources: list[\n    ActivationFactoryDatasetSource\n    | ActivationFactoryActivationsSource\n]\n</code></pre> <p>List of sources to use for activations. Can be a dataset or a path to activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.target","title":"target  <code>pydantic-field</code>","text":"<pre><code>target: ActivationFactoryTarget\n</code></pre> <p>The target to produce.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.hook_points","title":"hook_points  <code>pydantic-field</code>","text":"<pre><code>hook_points: list[str]\n</code></pre> <p>The hook points to capture activations from.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.batch_size","title":"batch_size  <code>pydantic-field</code>","text":"<pre><code>batch_size: int\n</code></pre> <p>The batch size to use for outputting activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.num_workers","title":"num_workers  <code>pydantic-field</code>","text":"<pre><code>num_workers: int = 4\n</code></pre> <p>The number of workers to use for loading the dataset.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.context_size","title":"context_size  <code>pydantic-field</code>","text":"<pre><code>context_size: int | None = None\n</code></pre> <p>The context size to use for generating activations. All tokens will be padded or truncated to this size. If <code>None</code>, will not pad or truncate tokens. This may lead to some error when re-batching activations of different context sizes.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.model_batch_size","title":"model_batch_size  <code>pydantic-field</code>","text":"<pre><code>model_batch_size: int = 1\n</code></pre> <p>The batch size to use for model forward pass when generating activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.override_dtype","title":"override_dtype  <code>pydantic-field</code>","text":"<pre><code>override_dtype: dtype | None = None\n</code></pre> <p>The dtype to use for outputting activations. If <code>None</code>, will not override the dtype.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.buffer_size","title":"buffer_size  <code>pydantic-field</code>","text":"<pre><code>buffer_size: int | None = None\n</code></pre> <p>Buffer size for online shuffling. If <code>None</code>, no shuffling will be performed.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.buffer_shuffle","title":"buffer_shuffle  <code>pydantic-field</code>","text":"<pre><code>buffer_shuffle: BufferShuffleConfig | None = None\n</code></pre> <p>\" Manual seed and device of generator for generating randomperm in buffer.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryConfig.ignore_token_ids","title":"ignore_token_ids  <code>pydantic-field</code>","text":"<pre><code>ignore_token_ids: list[int] | None = None\n</code></pre> <p>Tokens to ignore in the activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactory","title":"ActivationFactory","text":"<pre><code>ActivationFactory(\n    cfg: ActivationFactoryConfig,\n    before_aggregation_interceptor: Callable[\n        [dict[str, Any], int], dict[str, Any]\n    ]\n    | None = None,\n    device_mesh: Any | None = None,\n)\n</code></pre> <p>Factory class for generating activation data from different sources.</p> <p>This class handles loading data from datasets or activation files, processing it through a pipeline of processors, and aggregating the results based on configured weights.</p> <p>The overall pipeline is like a tree, where multiple chains collect data from different sources, and then aggregated together, which in detail is: 1. Pre-aggregation processors: Process data from each source through a series of processors. 2. Aggregator: Aggregate the processed data streams. 3. Post-aggregation processor: Process the aggregated data through a final processor.</p> <p>Initialize the factory with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ActivationFactoryConfig</code> <p>Configuration object specifying data sources, processing pipeline and output format</p> required Source code in <code>src/lm_saes/activation/factory.py</code> <pre><code>def __init__(\n    self,\n    cfg: ActivationFactoryConfig,\n    before_aggregation_interceptor: Callable[[dict[str, Any], int], dict[str, Any]] | None = None,\n    device_mesh: Optional[Any] = None,\n):\n    \"\"\"Initialize the factory with the given configuration.\n\n    Args:\n        cfg: Configuration object specifying data sources, processing pipeline and output format\n    \"\"\"\n    self.cfg = cfg\n    self.device_mesh = device_mesh\n\n    self.pre_aggregation_processors = self.build_pre_aggregation_processors()\n    self.post_aggregation_processor = self.build_post_aggregation_processor()\n    self.aggregator = self.build_aggregator()\n    self.before_aggregation_interceptor = before_aggregation_interceptor\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationFactory.build_pre_aggregation_processors","title":"build_pre_aggregation_processors","text":"<pre><code>build_pre_aggregation_processors()\n</code></pre> <p>Build processors that run before aggregation for each data source.</p> <p>Returns:</p> Type Description <p>List of callables that process data from each source</p> Source code in <code>src/lm_saes/activation/factory.py</code> <pre><code>def build_pre_aggregation_processors(self):\n    \"\"\"Build processors that run before aggregation for each data source.\n\n    Returns:\n        List of callables that process data from each source\n    \"\"\"\n    # Split sources by type\n    dataset_sources = [source for source in self.cfg.sources if isinstance(source, ActivationFactoryDatasetSource)]\n    activations_sources = [\n        source for source in self.cfg.sources if isinstance(source, ActivationFactoryActivationsSource)\n    ]\n\n    pre_aggregation_processors = [\n        self._build_pre_aggregation_dataset_source_processors(source, i) for i, source in enumerate(dataset_sources)\n    ] + [\n        self._build_pre_aggregation_activations_source_processors(source, i + len(dataset_sources))\n        for i, source in enumerate(activations_sources)\n    ]\n    return pre_aggregation_processors\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationFactory.build_post_aggregation_processor","title":"build_post_aggregation_processor","text":"<pre><code>build_post_aggregation_processor()\n</code></pre> <p>Build processor that runs after aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Factory configuration object</p> required <p>Returns:</p> Type Description <p>Callable that processes aggregated data</p> Source code in <code>src/lm_saes/activation/factory.py</code> <pre><code>def build_post_aggregation_processor(self):\n    \"\"\"Build processor that runs after aggregation.\n\n    Args:\n        cfg: Factory configuration object\n\n    Returns:\n        Callable that processes aggregated data\n    \"\"\"\n\n    def build_batchler():\n        \"\"\"Create batchler for batched activations.\"\"\"\n        assert self.cfg.batch_size is not None, \"Batch size must be provided for outputting batched activations\"\n        return ActivationBatchler(\n            batch_size=self.cfg.batch_size,\n            buffer_size=self.cfg.buffer_size,\n            buffer_shuffle_config=self.cfg.buffer_shuffle,\n            device_mesh=self.device_mesh,\n        )\n\n    def build_override_dtype_processor():\n        \"\"\"Create processor that overrides the dtype of the activations.\"\"\"\n        assert self.cfg.override_dtype is not None, (\n            \"Override dtype must be provided for outputting activations with different dtype\"\n        )\n        return OverrideDtypeProcessor(dtype=self.cfg.override_dtype)\n\n    processors = []\n    if self.cfg.batch_size is not None:\n        processors.append(build_batchler())\n    if self.cfg.override_dtype is not None:\n        processors.append(build_override_dtype_processor())\n\n    def process_activations(activations: Iterable[dict[str, Any]], **kwargs: Any):\n        \"\"\"Process aggregated activations through post-processors.\n\n        Args:\n            activations: Stream of aggregated activation data\n            **kwargs: Additional arguments passed to processors\n\n        Returns:\n            Processed activation stream\n        \"\"\"\n        for processor in processors:\n            activations = processor.process(activations, **kwargs)\n        return activations\n\n    return process_activations\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationFactory.build_aggregator","title":"build_aggregator","text":"<pre><code>build_aggregator()\n</code></pre> <p>Build function to aggregate data from multiple sources.</p> <p>Returns:</p> Type Description <p>Callable that aggregates data streams. Currently is a simple weighted random sampler.</p> Source code in <code>src/lm_saes/activation/factory.py</code> <pre><code>def build_aggregator(self):\n    \"\"\"Build function to aggregate data from multiple sources.\n\n    Returns:\n        Callable that aggregates data streams. Currently is a simple weighted random sampler.\n    \"\"\"\n    source_sample_weights = np.array([source.sample_weights for source in self.cfg.sources])\n\n    def aggregate(activations: list[Iterable[dict[str, Any]]], **kwargs: Any) -&gt; Iterable[dict[str, Any]]:\n        \"\"\"Aggregate multiple activation streams by sampling based on weights.\n\n        Args:\n            activations: List of activation streams from different sources\n            **kwargs: Additional arguments (unused)\n\n        Yields:\n            Sampled activation data with source info\n        \"\"\"\n        ran_out_of_samples = np.zeros(len(self.cfg.sources), dtype=bool)\n        activations: list[Iterator[dict[str, Any]]] = [iter(activation) for activation in activations]\n        # Mask out sources run out of samples\n        weights = source_sample_weights[~ran_out_of_samples]\n        weights = weights / weights.sum()\n\n        while not all(ran_out_of_samples):\n            sampled_sources = np.random.choice(len(activations), replace=True, p=weights)\n            try:\n                result = next(activations[sampled_sources])\n            except StopIteration:\n                ran_out_of_samples[sampled_sources] = True\n                continue\n            yield result\n\n    return aggregate\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationFactory.process","title":"process","text":"<pre><code>process(**kwargs: Any)\n</code></pre> <p>Process data through the full pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arguments passed to processors (must include required args)</p> <code>{}</code> <p>Returns:</p> Type Description <p>Iterable of processed activation data</p> Source code in <code>src/lm_saes/activation/factory.py</code> <pre><code>def process(self, **kwargs: Any):\n    \"\"\"Process data through the full pipeline.\n\n    Args:\n        **kwargs: Arguments passed to processors (must include required args)\n\n    Returns:\n        Iterable of processed activation data\n    \"\"\"\n    streams = [processor(**kwargs) for processor in self.pre_aggregation_processors]\n    stream = self.aggregator(streams)\n    return self.post_aggregation_processor(stream, **kwargs)\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationFactoryTarget","title":"ActivationFactoryTarget","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryTarget.TOKENS","title":"TOKENS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TOKENS = 'tokens'\n</code></pre> <p>Output non-padded and non-truncated tokens.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryTarget.ACTIVATIONS_2D","title":"ACTIVATIONS_2D  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ACTIVATIONS_2D = 'activations-2d'\n</code></pre> <p>Output activations in <code>(batch_size, seq_len, d_model)</code> shape. Tokens are padded and truncated to the same length.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryTarget.ACTIVATIONS_1D","title":"ACTIVATIONS_1D  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ACTIVATIONS_1D = 'activations-1d'\n</code></pre> <p>Output activations in <code>(n_filtered_tokens, d_model)</code> shape. Tokens are filtered in this stage.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryDatasetSource","title":"ActivationFactoryDatasetSource  <code>pydantic-model</code>","text":"<p>               Bases: <code>ActivationFactorySource</code></p> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>sample_weights</code>                 (<code>float</code>)             </li> <li> <code>type</code>                 (<code>str</code>)             </li> <li> <code>is_dataset_tokenized</code>                 (<code>bool</code>)             </li> <li> <code>prepend_bos</code>                 (<code>bool</code>)             </li> </ul>"},{"location":"reference/activation/#lm_saes.ActivationFactoryDatasetSource.is_dataset_tokenized","title":"is_dataset_tokenized  <code>pydantic-field</code>","text":"<pre><code>is_dataset_tokenized: bool = False\n</code></pre> <p>Whether the dataset is tokenized. Non-tokenized datasets should have records with fields <code>text</code>, <code>images</code>, etc. Tokenized datasets should have records with fields <code>tokens</code>, which could contain either padded or non-padded tokens.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryDatasetSource.prepend_bos","title":"prepend_bos  <code>pydantic-field</code>","text":"<pre><code>prepend_bos: bool = True\n</code></pre> <p>Whether to prepend the BOS token to each record when tokenizing.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryActivationsSource","title":"ActivationFactoryActivationsSource  <code>pydantic-model</code>","text":"<p>               Bases: <code>ActivationFactorySource</code></p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>sample_weights</code>                 (<code>float</code>)             </li> <li> <code>type</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str | dict[str, str]</code>)             </li> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype | None</code>)             </li> <li> <code>num_workers</code>                 (<code>int</code>)             </li> <li> <code>prefetch</code>                 (<code>int | None</code>)             </li> </ul>"},{"location":"reference/activation/#lm_saes.ActivationFactoryActivationsSource.path","title":"path  <code>pydantic-field</code>","text":"<pre><code>path: str | dict[str, str]\n</code></pre> <p>The path to the cached activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryActivationsSource.device","title":"device  <code>pydantic-field</code>","text":"<pre><code>device: str = 'cpu'\n</code></pre> <p>The device to load the activations on.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryActivationsSource.dtype","title":"dtype  <code>pydantic-field</code>","text":"<pre><code>dtype: dtype | None = None\n</code></pre> <p>We might want to convert presaved bf16 activations to fp32</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryActivationsSource.num_workers","title":"num_workers  <code>pydantic-field</code>","text":"<pre><code>num_workers: int = 4\n</code></pre> <p>The number of workers to use for loading the activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationFactoryActivationsSource.prefetch","title":"prefetch  <code>pydantic-field</code>","text":"<pre><code>prefetch: int | None = 8\n</code></pre> <p>The number of chunks to prefetch.</p>"},{"location":"reference/activation/#lm_saes.BufferShuffleConfig","title":"BufferShuffleConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>perm_seed</code>                 (<code>int</code>)             </li> <li> <code>generator_device</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"reference/activation/#lm_saes.BufferShuffleConfig.perm_seed","title":"perm_seed  <code>pydantic-field</code>","text":"<pre><code>perm_seed: int = 42\n</code></pre> <p>Perm seed for aligned permutation for generating activations. If <code>None</code>, will not use manual seed for Generator.</p>"},{"location":"reference/activation/#lm_saes.BufferShuffleConfig.generator_device","title":"generator_device  <code>pydantic-field</code>","text":"<pre><code>generator_device: str | None = None\n</code></pre> <p>The device to be assigned for the torch.Generator. If 'None', generator will be initialized on cpu as pytorch default.</p>"},{"location":"reference/activation/#lm_saes.ActivationWriterConfig","title":"ActivationWriterConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>hook_points</code>                 (<code>list[str]</code>)             </li> <li> <code>total_generating_tokens</code>                 (<code>int | None</code>)             </li> <li> <code>n_samples_per_chunk</code>                 (<code>int | None</code>)             </li> <li> <code>cache_dir</code>                 (<code>str</code>)             </li> <li> <code>format</code>                 (<code>Literal['pt', 'safetensors']</code>)             </li> <li> <code>num_workers</code>                 (<code>int | None</code>)             </li> </ul>"},{"location":"reference/activation/#lm_saes.ActivationWriterConfig.hook_points","title":"hook_points  <code>pydantic-field</code>","text":"<pre><code>hook_points: list[str]\n</code></pre> <p>The hook points to capture activations from.</p>"},{"location":"reference/activation/#lm_saes.ActivationWriterConfig.total_generating_tokens","title":"total_generating_tokens  <code>pydantic-field</code>","text":"<pre><code>total_generating_tokens: int | None = None\n</code></pre> <p>The total number of tokens to generate. If <code>None</code>, will write all activations to disk.</p>"},{"location":"reference/activation/#lm_saes.ActivationWriterConfig.n_samples_per_chunk","title":"n_samples_per_chunk  <code>pydantic-field</code>","text":"<pre><code>n_samples_per_chunk: int | None = None\n</code></pre> <p>The number of samples to write to disk per chunk. If <code>None</code>, will not further batch the activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationWriterConfig.cache_dir","title":"cache_dir  <code>pydantic-field</code>","text":"<pre><code>cache_dir: str = 'activations'\n</code></pre> <p>The directory to save the activations.</p>"},{"location":"reference/activation/#lm_saes.ActivationWriterConfig.num_workers","title":"num_workers  <code>pydantic-field</code>","text":"<pre><code>num_workers: int | None = None\n</code></pre> <p>The number of workers to use for writing the activations. If <code>None</code>, will not use multi-threaded writing.</p>"},{"location":"reference/activation/#lm_saes.ActivationWriter","title":"ActivationWriter","text":"<pre><code>ActivationWriter(\n    cfg: ActivationWriterConfig,\n    executor: ThreadPoolExecutor | None = None,\n)\n</code></pre> <p>Writes activations to disk in a format compatible with CachedActivationLoader.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ActivationWriterConfig</code> <p>Configuration for writing activations</p> required <code>executor</code> <code>ThreadPoolExecutor | None</code> <p>Optional ThreadPoolExecutor for parallel writing. If None, a new executor will be created with max_workers=2.</p> <code>None</code> Source code in <code>src/lm_saes/activation/writer.py</code> <pre><code>def __init__(\n    self,\n    cfg: ActivationWriterConfig,\n    executor: Optional[ThreadPoolExecutor] = None,\n):\n    self.cache_dir = Path(cfg.cache_dir)\n    self.cfg = cfg\n    if cfg.num_workers is None:\n        self.executor = None\n    else:\n        self.executor = executor or ThreadPoolExecutor(max_workers=cfg.num_workers)\n    self._owned_executor = cfg.num_workers is not None and executor is None\n\n    # Create directories for each hook point\n    for hook_point in self.cfg.hook_points:\n        hook_dir = self.cache_dir / hook_point\n        hook_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationWriter.process","title":"process","text":"<pre><code>process(\n    data: Iterable[dict[str, Any]],\n    *,\n    device_mesh: DeviceMesh | None = None,\n    start_shard: int = 0,\n) -&gt; None\n</code></pre> <p>Write activation data to disk in chunks.</p> <p>Processes a stream of activation dictionaries, accumulating samples until reaching the configured chunk size, then writes each chunk to disk. Files are organized by hook point with names following the pattern 'chunk-{N}.pt'.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable[dict[str, Any]]</code> <p>Stream of activation dictionaries containing: - Activations for each hook point - Original tokens - Meta information</p> required <code>device_mesh</code> <code>DeviceMesh | None</code> <p>The device mesh to use for distributed writing. If None, will write to disk on the current rank.</p> <code>None</code> <code>start_shard</code> <code>int</code> <p>The shard to start writing from.</p> <code>0</code> Source code in <code>src/lm_saes/activation/writer.py</code> <pre><code>def process(\n    self,\n    data: Iterable[dict[str, Any]],\n    *,\n    device_mesh: Optional[DeviceMesh] = None,\n    start_shard: int = 0,\n) -&gt; None:\n    \"\"\"Write activation data to disk in chunks.\n\n    Processes a stream of activation dictionaries, accumulating samples until reaching\n    the configured chunk size, then writes each chunk to disk. Files are organized by\n    hook point with names following the pattern 'chunk-{N}.pt'.\n\n    Args:\n        data: Stream of activation dictionaries containing:\n            - Activations for each hook point\n            - Original tokens\n            - Meta information\n        device_mesh: The device mesh to use for distributed writing. If None, will write to disk on the current rank.\n        start_shard: The shard to start writing from.\n    \"\"\"\n    total = (\n        self.cfg.total_generating_tokens // device_mesh.get_group(\"data\").size()\n        if device_mesh is not None and self.cfg.total_generating_tokens is not None\n        else self.cfg.total_generating_tokens\n    )\n    pbar = tqdm(desc=\"Writing activations to disk\", total=total)\n    n_tokens_written = 0\n\n    futures = set() if self.cfg.num_workers is not None else None\n\n    if self.cfg.n_samples_per_chunk is not None:\n\n        def collate_batch(batch: Sequence[dict[str, Any]]) -&gt; dict[str, Any]:\n            # Assert that all samples have the same keys\n            assert all(k in d for k in batch[0] for d in batch), (\n                f\"All samples must have the same keys: {batch[0].keys()}\"\n            )\n            return {\n                k: torch.stack([d[k] for d in batch])\n                if isinstance(batch[0][k], torch.Tensor)\n                else [d[k] for d in batch]\n                for k in batch[0].keys()\n            }\n\n        data = map(collate_batch, more_itertools.batched(data, self.cfg.n_samples_per_chunk))\n\n    for chunk_id, chunk in enumerate(data):\n        assert all(k in chunk for k in self.cfg.hook_points), (\n            f\"All samples must have the hook points: {self.cfg.hook_points}\"\n        )\n\n        chunk_name = (\n            f\"chunk-{chunk_id:08d}\"\n            if device_mesh is None\n            else f\"shard-{device_mesh.get_group('data').rank() + start_shard}-chunk-{chunk_id:08d}\"\n        )\n\n        # Submit writing tasks for each hook point\n        with timer.time(\"write_chunk\"):\n            for hook_point in self.cfg.hook_points:\n                chunk_data = {\"activation\": chunk[hook_point]} | {\n                    k: v for k, v in chunk.items() if k not in [\"meta\", *self.cfg.hook_points]\n                }\n                if futures is None:\n                    self._write_chunk(\n                        hook_point, chunk_data, chunk_name, chunk[\"meta\"] if \"meta\" in chunk else None\n                    )\n                else:\n                    assert self.executor is not None, \"Executor is not initialized\"\n                    future = self.executor.submit(\n                        self._write_chunk,\n                        hook_point,\n                        chunk_data,\n                        chunk_name,\n                        chunk[\"meta\"] if \"meta\" in chunk else None,\n                    )\n                    futures.add(future)\n\n            if futures is not None:\n                assert self.cfg.num_workers is not None, \"num_workers must be set to use parallel writing\"\n                # Wait for some futures to complete if we have too many pending\n                while len(futures) &gt;= self.cfg.num_workers * 2:\n                    done, futures = wait(futures, return_when=\"FIRST_COMPLETED\")\n                    for future in done:\n                        future.result()  # Raise any exceptions that occurred\n\n        if timer.enabled:\n            logger.info(f\"\\nTimer Summary:\\n{timer.summary()}\\n\")\n\n        n_tokens_written += chunk[\"tokens\"].numel()\n        pbar.update(chunk[\"tokens\"].numel())\n\n        if total is not None and n_tokens_written &gt;= total:\n            break\n\n    if futures is not None:\n        # Wait for remaining futures to complete\n        for future in as_completed(futures):\n            future.result()\n\n    pbar.close()\n</code></pre>"},{"location":"reference/activation/#lm_saes.ActivationWriter.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> <p>Cleanup the executor if we own it.</p> Source code in <code>src/lm_saes/activation/writer.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Cleanup the executor if we own it.\"\"\"\n    if self._owned_executor and self.executor is not None:\n        self.executor.shutdown(wait=True)\n</code></pre>"},{"location":"reference/analysis/","title":"Analysis","text":""},{"location":"reference/analysis/#analysis","title":"Analysis","text":"<p>Post-training feature analysis and interpretability tools.</p>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzerConfig","title":"FeatureAnalyzerConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>total_analyzing_tokens</code>                 (<code>int</code>)             </li> <li> <code>ignore_token_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>subsamples</code>                 (<code>dict[str, dict[str, int | float]]</code>)             </li> <li> <code>clt_layer</code>                 (<code>int | None</code>)             </li> </ul>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzerConfig.total_analyzing_tokens","title":"total_analyzing_tokens  <code>pydantic-field</code>","text":"<pre><code>total_analyzing_tokens: int\n</code></pre> <p>Total number of tokens to analyze</p>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzerConfig.ignore_token_ids","title":"ignore_token_ids  <code>pydantic-field</code>","text":"<pre><code>ignore_token_ids: list[int] | None = None\n</code></pre> <p>Tokens to ignore in the activations.</p>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzerConfig.subsamples","title":"subsamples  <code>pydantic-field</code>","text":"<pre><code>subsamples: dict[str, dict[str, int | float]]\n</code></pre> <p>Dictionary mapping subsample names to their parameters: - <code>proportion</code>: Proportion of max activation to consider - <code>n_samples</code>: Number of samples to keep - <code>max_length</code>: Maximum length of the sample</p>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzerConfig.clt_layer","title":"clt_layer  <code>pydantic-field</code>","text":"<pre><code>clt_layer: int | None = None\n</code></pre> <p>Layer to analyze for CLT. Provided iff analyzing CLT.</p>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzer","title":"FeatureAnalyzer","text":"<pre><code>FeatureAnalyzer(cfg: FeatureAnalyzerConfig)\n</code></pre> <p>Analyzes feature activations from a sparse autoencoder.</p> <p>This class processes activation data from a sparse autoencoder to: 1. Track activation statistics like frequency and magnitude 2. Sample and store representative activations 3. Organize results by feature for analysis</p> <p>Initialize the feature analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>FeatureAnalyzerConfig</code> <p>Analysis configuration specifying parameters like sample sizes and thresholds</p> required Source code in <code>src/lm_saes/analysis/feature_analyzer.py</code> <pre><code>def __init__(\n    self,\n    cfg: FeatureAnalyzerConfig,\n):\n    \"\"\"Initialize the feature analyzer.\n\n    Args:\n        cfg: Analysis configuration specifying parameters like sample sizes and thresholds\n    \"\"\"\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzer.compute_ignore_token_masks","title":"compute_ignore_token_masks","text":"<pre><code>compute_ignore_token_masks(\n    tokens: Tensor,\n    ignore_token_ids: list[int] | None = None,\n) -&gt; Tensor\n</code></pre> <p>Compute ignore token masks for the given tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Tensor</code> <p>The tokens to compute the ignore token masks for</p> required <code>ignore_token_ids</code> <code>list[int] | None</code> <p>The token IDs to ignore</p> <code>None</code> Source code in <code>src/lm_saes/analysis/feature_analyzer.py</code> <pre><code>def compute_ignore_token_masks(\n    self, tokens: torch.Tensor, ignore_token_ids: Optional[list[int]] = None\n) -&gt; torch.Tensor:\n    \"\"\"Compute ignore token masks for the given tokens.\n\n    Args:\n        tokens: The tokens to compute the ignore token masks for\n        ignore_token_ids: The token IDs to ignore\n    \"\"\"\n    if ignore_token_ids is None:\n        warnings.warn(\n            \"ignore_token_ids are not provided. No tokens (including pad tokens) will be filtered out. If this is intentional, set ignore_token_ids explicitly to an empty list to avoid this warning.\",\n            UserWarning,\n            stacklevel=2,\n        )\n        ignore_token_ids = []\n    mask = torch.ones_like(tokens, dtype=torch.bool)\n    for token_id in ignore_token_ids:\n        mask &amp;= tokens != token_id\n    return mask\n</code></pre>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzer.get_post_analysis_func","title":"get_post_analysis_func","text":"<pre><code>get_post_analysis_func(sae_type: str)\n</code></pre> <p>Get the post-analysis processor for the given SAE type.</p> <p>Parameters:</p> Name Type Description Default <code>sae_type</code> <code>str</code> <p>The SAE type identifier</p> required <p>Returns:</p> Type Description <p>The post-analysis processor instance</p> Source code in <code>src/lm_saes/analysis/feature_analyzer.py</code> <pre><code>def get_post_analysis_func(self, sae_type: str):\n    \"\"\"Get the post-analysis processor for the given SAE type.\n\n    Args:\n        sae_type: The SAE type identifier\n\n    Returns:\n        The post-analysis processor instance\n    \"\"\"\n    try:\n        return get_post_analysis_processor(sae_type)\n    except KeyError:\n        # Fallback to generic processor if no specific processor is registered\n        return get_post_analysis_processor(\"generic\")\n</code></pre>"},{"location":"reference/analysis/#lm_saes.FeatureAnalyzer.analyze_chunk","title":"analyze_chunk","text":"<pre><code>analyze_chunk(\n    activation_factory: ActivationFactory,\n    sae: AbstractSparseAutoEncoder,\n    device_mesh: DeviceMesh | None = None,\n    activation_factory_process_kwargs: dict[str, Any] = {},\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Analyze feature activations for a chunk of the SAE.</p> <p>Processes activation data to: 1. Track activation statistics 2. Sample representative activations 3. Organize results by feature</p> <p>Parameters:</p> Name Type Description Default <code>activation_factory</code> <code>ActivationFactory</code> <p>The activation factory to use</p> required <code>sae</code> <code>AbstractSparseAutoEncoder</code> <p>The sparse autoencoder model</p> required <code>device_mesh</code> <code>DeviceMesh | None</code> <p>The device mesh to use</p> <code>None</code> <code>activation_factory_process_kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments to pass to the activation factory's process method</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dictionaries containing per-feature analysis results:</p> <code>list[dict[str, Any]]</code> <ul> <li>Activation counts and maximums</li> </ul> <code>list[dict[str, Any]]</code> <ul> <li>Sampled activations with metadata</li> </ul> Source code in <code>src/lm_saes/analysis/feature_analyzer.py</code> <pre><code>@torch.no_grad()\ndef analyze_chunk(\n    self,\n    activation_factory: ActivationFactory,\n    sae: AbstractSparseAutoEncoder,\n    device_mesh: DeviceMesh | None = None,\n    activation_factory_process_kwargs: dict[str, Any] = {},\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Analyze feature activations for a chunk of the SAE.\n\n    Processes activation data to:\n    1. Track activation statistics\n    2. Sample representative activations\n    3. Organize results by feature\n\n    Args:\n        activation_factory: The activation factory to use\n        sae: The sparse autoencoder model\n        device_mesh: The device mesh to use\n        activation_factory_process_kwargs: Keyword arguments to pass to the activation factory's process method\n\n    Returns:\n        List of dictionaries containing per-feature analysis results:\n        - Activation counts and maximums\n        - Sampled activations with metadata\n    \"\"\"\n    activation_stream = activation_factory.process(**activation_factory_process_kwargs)\n    n_tokens = n_analyzed_tokens = 0\n\n    # Progress tracking\n    pbar = tqdm(\n        total=self.cfg.total_analyzing_tokens,\n        desc=\"Analyzing SAE\",\n        smoothing=0.01,\n        disable=not is_primary_rank(device_mesh),\n    )\n\n    if device_mesh is not None and device_mesh.mesh_dim_names is not None and \"model\" in device_mesh.mesh_dim_names:\n        d_sae_local = sae.cfg.d_sae // device_mesh[\"model\"].size()\n    else:\n        d_sae_local = sae.cfg.d_sae\n\n    # Initialize tracking variables\n    sample_result = {k: None for k in self.cfg.subsamples.keys()}\n    if device_mesh is not None:\n        act_times = torch.distributed.tensor.zeros(\n            (sae.cfg.d_sae,),\n            dtype=torch.long,\n            device_mesh=device_mesh,\n            placements=DimMap({\"model\": 0}).placements(device_mesh),\n        )\n        max_feature_acts = torch.distributed.tensor.zeros(\n            (sae.cfg.d_sae,),\n            dtype=sae.cfg.dtype,\n            device_mesh=device_mesh,\n            placements=DimMap({\"model\": 0}).placements(device_mesh),\n        )\n    else:\n        act_times = torch.zeros((d_sae_local,), dtype=torch.long, device=sae.cfg.device)\n        max_feature_acts = torch.zeros((d_sae_local,), dtype=sae.cfg.dtype, device=sae.cfg.device)\n    mapper = KeyedDiscreteMapper()\n\n    # TODO: Make a wrapper for CLT\n    if isinstance(sae, CrossLayerTranscoder):\n        sae.encode = partial(sae.encode_single_layer, layer=self.cfg.clt_layer)  # type: ignore\n        sae.prepare_input = partial(sae.prepare_input_single_layer, layer=self.cfg.clt_layer)  # type: ignore\n        sae.decoder_norm_per_feature = partial(sae.decoder_norm_per_feature, layer=self.cfg.clt_layer)  # type: ignore\n        sae.keep_only_decoders_for_layer_from(self.cfg.clt_layer)  # type: ignore\n        torch.cuda.empty_cache()\n\n    # Process activation batches\n    for batch in activation_stream:\n        # Reshape meta to zip outer dimensions to inner\n        meta = {k: [m[k] for m in batch[\"meta\"]] for k in batch[\"meta\"][0].keys()}\n\n        # Get feature activations from SAE\n        x, encoder_kwargs, _ = sae.prepare_input(batch)\n        tokens = batch[\"tokens\"]\n        feature_acts: torch.Tensor = sae.encode(x, **encoder_kwargs)\n        if isinstance(feature_acts, DTensor):\n            assert device_mesh is not None, \"Device mesh is required for DTensor feature activations\"\n            if device_mesh is not feature_acts.device_mesh:\n                feature_acts = DTensor.from_local(\n                    feature_acts.redistribute(\n                        placements=DimMap({\"head\": -1, \"model\": -1}).placements(feature_acts.device_mesh)\n                    ).to_local(),\n                    device_mesh,\n                    placements=DimMap({\"model\": -1}).placements(device_mesh),\n                )\n                # TODO: Remove this once redistributing across device meshes is supported\n            feature_acts = feature_acts.redistribute(placements=DimMap({\"model\": -1}).placements(device_mesh))\n            if not isinstance(tokens, DTensor):\n                tokens = DTensor.from_local(tokens, device_mesh, placements=DimMap({}).placements(device_mesh))\n        if isinstance(sae, CrossCoder):\n            feature_acts = feature_acts.amax(dim=-2)\n        assert feature_acts.shape == (tokens.shape[0], tokens.shape[1], sae.cfg.d_sae), (\n            f\"feature_acts.shape: {feature_acts.shape}, expected: {(tokens.shape[0], tokens.shape[1], sae.cfg.d_sae)}\"\n        )\n\n        # Compute and apply ignore token masks\n        if self.cfg.ignore_token_ids is None and batch.get(\"mask\") is not None:\n            ignore_token_masks = batch[\"mask\"]\n            if device_mesh is not None and not isinstance(ignore_token_masks, DTensor):\n                ignore_token_masks = DTensor.from_local(\n                    ignore_token_masks, device_mesh, placements=DimMap({}).placements(device_mesh)\n                )\n        else:\n            ignore_token_masks = self.compute_ignore_token_masks(tokens, self.cfg.ignore_token_ids)\n        feature_acts *= rearrange(ignore_token_masks, \"batch_size n_ctx -&gt; batch_size n_ctx 1\")\n\n        # Update activation statistics\n        active_feature_count = feature_acts.gt(0.0).sum(dim=[0, 1])\n        act_times += active_feature_count\n        max_feature_acts = torch.max(max_feature_acts, feature_acts.max(dim=0).values.max(dim=0).values)\n\n        # Apply discrete mapper encoding only to string metadata, keep others as-is\n        discrete_meta = {}\n        for k, v in meta.items():\n            if all(isinstance(item, str) for item in v):\n                # Apply discrete mapper encoding to string metadata\n                discrete_meta[k] = torch.tensor(mapper.encode(k, v), device=sae.cfg.device, dtype=torch.int32)\n            else:\n                # Keep non-string metadata as-is (assuming they are already tensors or can be converted)\n                discrete_meta[k] = torch.tensor(v, device=sae.cfg.device)\n        if device_mesh is not None:\n            discrete_meta = {\n                k: DTensor.from_local(\n                    local_tensor=repeat(v, \"batch_size -&gt; batch_size d_sae\", d_sae=d_sae_local),\n                    device_mesh=device_mesh,\n                    placements=DimMap({\"model\": 1}).placements(device_mesh),\n                )\n                for k, v in discrete_meta.items()\n            }\n        else:\n            discrete_meta = {\n                k: repeat(v, \"batch_size -&gt; batch_size d_sae\", d_sae=d_sae_local) for k, v in discrete_meta.items()\n            }\n        sample_result = self._process_batch(\n            feature_acts, discrete_meta, sample_result, max_feature_acts, device_mesh\n        )\n\n        # Update progress\n        n_tokens_current = tokens.numel()\n        n_tokens += n_tokens_current\n        n_analyzed_tokens += cast(int, item(ignore_token_masks.int().sum()))\n        pbar.update(n_tokens_current)\n        if n_tokens &gt;= self.cfg.total_analyzing_tokens:\n            break\n\n    pbar.close()\n\n    # Filter out None values and format final per-feature results\n    sample_result = {k: v for k, v in sample_result.items() if v is not None}\n    sample_result = {\n        name: {k: to_local(v) for k, v in subsample.items()} for name, subsample in sample_result.items()\n    }\n\n    return self.get_post_analysis_func(sae.cfg.sae_type).process(\n        sae=sae,\n        act_times=to_local(act_times),\n        n_analyzed_tokens=n_analyzed_tokens,\n        max_feature_acts=to_local(max_feature_acts),\n        sample_result=sample_result,\n        mapper=mapper,\n        device_mesh=device_mesh,\n        activation_factory=activation_factory,\n        activation_factory_process_kwargs=activation_factory_process_kwargs,\n    )\n</code></pre>"},{"location":"reference/analysis/#lm_saes.DirectLogitAttributorConfig","title":"DirectLogitAttributorConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>clt_layer</code>                 (<code>int | None</code>)             </li> </ul>"},{"location":"reference/analysis/#lm_saes.DirectLogitAttributorConfig.top_k","title":"top_k  <code>pydantic-field</code>","text":"<pre><code>top_k: int = 10\n</code></pre> <p>The number of top tokens to attribute to.</p>"},{"location":"reference/analysis/#lm_saes.DirectLogitAttributorConfig.clt_layer","title":"clt_layer  <code>pydantic-field</code>","text":"<pre><code>clt_layer: int | None = None\n</code></pre> <p>Layer to analyze for CLT. Provided iff analyzing CLT.</p>"},{"location":"reference/analysis/#lm_saes.DirectLogitAttributor","title":"DirectLogitAttributor","text":"<pre><code>DirectLogitAttributor(cfg: DirectLogitAttributorConfig)\n</code></pre> Source code in <code>src/lm_saes/analysis/direct_logit_attributor.py</code> <pre><code>def __init__(self, cfg: DirectLogitAttributorConfig):\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/analysis/#lm_saes.DirectLogitAttributor.direct_logit_attribute","title":"direct_logit_attribute","text":"<pre><code>direct_logit_attribute(\n    sae, model: LanguageModel, layer_idx: int | None = None\n)\n</code></pre> <p>Compute direct logit attribution for the given SAE.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <p>The SAE model to attribute.</p> required <code>model</code> <code>LanguageModel</code> <p>The language model backend.</p> required <code>layer_idx</code> <code>int | None</code> <p>The layer index (required for some SAE types like CrossLayerTranscoder).</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of dictionaries containing top positive and negative logits for each feature.</p> Source code in <code>src/lm_saes/analysis/direct_logit_attributor.py</code> <pre><code>@torch.no_grad()\ndef direct_logit_attribute(self, sae, model: LanguageModel, layer_idx: int | None = None):\n    \"\"\"Compute direct logit attribution for the given SAE.\n\n    Args:\n        sae: The SAE model to attribute.\n        model: The language model backend.\n        layer_idx: The layer index (required for some SAE types like CrossLayerTranscoder).\n\n    Returns:\n        A list of dictionaries containing top positive and negative logits for each feature.\n    \"\"\"\n    assert isinstance(model, TransformerLensLanguageModel), (\n        \"DirectLogitAttributor only supports TransformerLensLanguageModel as the model backend\"\n    )\n    hooked_model: HookedTransformer | None = model.model\n    assert hooked_model is not None, \"Model ckpt must be loaded for direct logit attribution\"\n\n    # Use singledispatch to compute logits and d_sae based on SAE type\n    logits, d_sae = compute_logits_and_d_sae(sae, hooked_model, layer_idx)\n\n    # Select the top k tokens\n    top_k_logits, top_k_indices = torch.topk(logits, self.cfg.top_k, dim=-1)\n    top_k_tokens = [hooked_model.to_str_tokens(top_k_indices[i]) for i in range(d_sae)]\n\n    assert top_k_logits.shape == top_k_indices.shape == (d_sae, self.cfg.top_k), (\n        f\"Top k logits and indices should have shape (d_sae, top_k), but got {top_k_logits.shape} and {top_k_indices.shape}\"\n    )\n    assert (len(top_k_tokens), len(top_k_tokens[0])) == (d_sae, self.cfg.top_k), (\n        f\"Top k tokens should have shape (d_sae, top_k), but got {len(top_k_tokens)} and {len(top_k_tokens[0])}\"\n    )\n\n    # Select the bottom k tokens\n    bottom_k_logits, bottom_k_indices = torch.topk(logits, self.cfg.top_k, dim=-1, largest=False)\n    bottom_k_tokens = [hooked_model.to_str_tokens(bottom_k_indices[i]) for i in range(d_sae)]\n\n    assert bottom_k_logits.shape == bottom_k_indices.shape == (d_sae, self.cfg.top_k), (\n        f\"Bottom k logits and indices should have shape (d_sae, top_k), but got {bottom_k_logits.shape} and {bottom_k_indices.shape}\"\n    )\n    assert (len(bottom_k_tokens), len(bottom_k_tokens[0])) == (d_sae, self.cfg.top_k), (\n        f\"Bottom k tokens should have shape (d_sae, top_k), but got {len(bottom_k_tokens)} and {len(bottom_k_tokens[0])}\"\n    )\n\n    result = [\n        {\n            \"top_positive\": [\n                {\"token\": token, \"logit\": logit} for token, logit in zip(top_k_tokens[i], top_k_logits[i].tolist())\n            ],\n            \"top_negative\": [\n                {\"token\": token, \"logit\": logit}\n                for token, logit in zip(bottom_k_tokens[i], bottom_k_logits[i].tolist())\n            ],\n        }\n        for i in range(d_sae)\n    ]\n    return result\n</code></pre>"},{"location":"reference/evaluation/","title":"Evaluation","text":""},{"location":"reference/evaluation/#evaluation","title":"Evaluation","text":"<p>Evaluation pipeline for trained sparse dictionaries.</p>"},{"location":"reference/evaluation/#lm_saes.EvalConfig","title":"EvalConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>total_eval_tokens</code>                 (<code>int</code>)             </li> </ul>"},{"location":"reference/evaluation/#lm_saes.EvalConfig.total_eval_tokens","title":"total_eval_tokens  <code>pydantic-field</code>","text":"<pre><code>total_eval_tokens: int = 1000000\n</code></pre> <p>Total number of tokens to evaluate on</p>"},{"location":"reference/evaluation/#lm_saes.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(cfg: EvalConfig)\n</code></pre> Source code in <code>src/lm_saes/evaluator.py</code> <pre><code>def __init__(self, cfg: EvalConfig):\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/infrastructure/","title":"Infrastructure","text":""},{"location":"reference/infrastructure/#infrastructure","title":"Infrastructure","text":"<p>Language model backend, dataset, and database configuration.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig","title":"LanguageModelConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModelConfig</code></p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>model_name</code>                 (<code>str</code>)             </li> <li> <code>model_from_pretrained_path</code>                 (<code>str | None</code>)             </li> <li> <code>use_flash_attn</code>                 (<code>bool</code>)             </li> <li> <code>cache_dir</code>                 (<code>str | None</code>)             </li> <li> <code>local_files_only</code>                 (<code>bool</code>)             </li> <li> <code>max_length</code>                 (<code>int</code>)             </li> <li> <code>backend</code>                 (<code>Literal['huggingface', 'transformer_lens', 'auto']</code>)             </li> <li> <code>load_ckpt</code>                 (<code>bool</code>)             </li> <li> <code>tokenizer_only</code>                 (<code>bool</code>)             </li> <li> <code>prepend_bos</code>                 (<code>bool</code>)             </li> <li> <code>bos_token_id</code>                 (<code>int | None</code>)             </li> <li> <code>eos_token_id</code>                 (<code>int | None</code>)             </li> <li> <code>pad_token_id</code>                 (<code>int | None</code>)             </li> </ul>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.model_name","title":"model_name  <code>pydantic-field</code>","text":"<pre><code>model_name: str = 'gpt2'\n</code></pre> <p>The name of the model to use.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.model_from_pretrained_path","title":"model_from_pretrained_path  <code>pydantic-field</code>","text":"<pre><code>model_from_pretrained_path: str | None = None\n</code></pre> <p>The path to the pretrained model. If <code>None</code>, will use the model from HuggingFace.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.use_flash_attn","title":"use_flash_attn  <code>pydantic-field</code>","text":"<pre><code>use_flash_attn: bool = False\n</code></pre> <p>Whether to use Flash Attention.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.cache_dir","title":"cache_dir  <code>pydantic-field</code>","text":"<pre><code>cache_dir: str | None = None\n</code></pre> <p>The directory of the HuggingFace cache. Should have the same effect as <code>HF_HOME</code>.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.local_files_only","title":"local_files_only  <code>pydantic-field</code>","text":"<pre><code>local_files_only: bool = False\n</code></pre> <p>Whether to only load the model from the local files. Should have the same effect as <code>HF_HUB_OFFLINE=1</code>.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.max_length","title":"max_length  <code>pydantic-field</code>","text":"<pre><code>max_length: int = 2048\n</code></pre> <p>The maximum length of the input.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.backend","title":"backend  <code>pydantic-field</code>","text":"<pre><code>backend: Literal[\n    \"huggingface\", \"transformer_lens\", \"auto\"\n] = \"auto\"\n</code></pre> <p>The backend to use for the language model.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.tokenizer_only","title":"tokenizer_only  <code>pydantic-field</code>","text":"<pre><code>tokenizer_only: bool = False\n</code></pre> <p>Whether to only load the tokenizer.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.prepend_bos","title":"prepend_bos  <code>pydantic-field</code>","text":"<pre><code>prepend_bos: bool = True\n</code></pre> <p>Whether to prepend the BOS token to the input.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.bos_token_id","title":"bos_token_id  <code>pydantic-field</code>","text":"<pre><code>bos_token_id: int | None = None\n</code></pre> <p>The ID of the BOS token. If <code>None</code>, will use the default BOS token.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.eos_token_id","title":"eos_token_id  <code>pydantic-field</code>","text":"<pre><code>eos_token_id: int | None = None\n</code></pre> <p>The ID of the EOS token. If <code>None</code>, will use the default EOS token.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.pad_token_id","title":"pad_token_id  <code>pydantic-field</code>","text":"<pre><code>pad_token_id: int | None = None\n</code></pre> <p>The ID of the padding token. If <code>None</code>, will use the default padding token.</p>"},{"location":"reference/infrastructure/#lm_saes.LanguageModelConfig.from_pretrained_sae","title":"from_pretrained_sae  <code>staticmethod</code>","text":"<pre><code>from_pretrained_sae(pretrained_name_or_path: str, **kwargs)\n</code></pre> <p>Load the LanguageModelConfig from a pretrained SAE name or path. Config is read from /lm_config.json (for local storage), //lm_config.json (for HuggingFace Hub), or constructed from model name (for SAELens).</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_name_or_path</code> <code>str</code> <p>The path to the pretrained SAE.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the LanguageModelConfig constructor.</p> <code>{}</code> Source code in <code>src/lm_saes/backend/language_model.py</code> <pre><code>@staticmethod\ndef from_pretrained_sae(pretrained_name_or_path: str, **kwargs):\n    \"\"\"Load the LanguageModelConfig from a pretrained SAE name or path. Config is read from &lt;pretrained_name_or_path&gt;/lm_config.json (for local storage), &lt;repo_id&gt;/&lt;name&gt;/lm_config.json (for HuggingFace Hub), or constructed from model name (for SAELens).\n\n    Args:\n        pretrained_name_or_path (str): The path to the pretrained SAE.\n        **kwargs: Additional keyword arguments to pass to the LanguageModelConfig constructor.\n    \"\"\"\n    sae_type = auto_infer_pretrained_sae_type(pretrained_name_or_path.split(\":\")[0])\n    if sae_type == PretrainedSAEType.LOCAL:\n        path = os.path.join(os.path.dirname(pretrained_name_or_path), \"lm_config.json\")\n    elif sae_type == PretrainedSAEType.HUGGINGFACE:\n        repo_id, name = pretrained_name_or_path.split(\":\")\n        path = hf_hub_download(repo_id=repo_id, filename=f\"{name}/lm_config.json\")\n    elif sae_type == PretrainedSAEType.SAELENS:\n        from sae_lens.loading.pretrained_saes_directory import get_pretrained_saes_directory\n\n        repo_id, name = pretrained_name_or_path.split(\":\")\n        lookups = get_pretrained_saes_directory()\n        assert lookups.get(repo_id) is not None and lookups[repo_id].saes_map.get(name) is not None, (\n            f\"Pretrained SAE {pretrained_name_or_path} not found in SAELens. This might indicate bugs in `auto_infer_pretrained_sae_type`.\"\n        )\n        model_name = lookups[repo_id].model\n        return LanguageModelConfig(model_name=model_name, **kwargs)\n    else:\n        raise ValueError(f\"Unsupported pretrained type: {sae_type}\")\n    with open(os.path.join(path, \"lm_config.json\"), \"r\") as f:\n        lm_config = json.load(f)\n    return LanguageModelConfig.model_validate(lm_config, **kwargs)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.TransformerLensLanguageModel","title":"TransformerLensLanguageModel","text":"<pre><code>TransformerLensLanguageModel(\n    cfg: LanguageModelConfig,\n    device_mesh: DeviceMesh | None = None,\n)\n</code></pre> <p>               Bases: <code>LanguageModel</code></p> Source code in <code>src/lm_saes/backend/language_model.py</code> <pre><code>def __init__(self, cfg: LanguageModelConfig, device_mesh: DeviceMesh | None = None):\n    self.cfg = cfg\n    self.device_mesh = device_mesh\n    if cfg.device == \"cuda\":\n        self.device = torch.device(f\"cuda:{torch.cuda.current_device()}\")\n    elif cfg.device == \"npu\":\n        self.device = torch.device(f\"npu:{torch.npu.current_device()}\")  # type: ignore[reportAttributeAccessIssue]\n    else:\n        self.device = torch.device(cfg.device)\n\n    hf_model = (\n        AutoModelForCausalLM.from_pretrained(\n            (cfg.model_name if cfg.model_from_pretrained_path is None else cfg.model_from_pretrained_path),\n            cache_dir=cfg.cache_dir,\n            local_files_only=cfg.local_files_only,\n            dtype=cfg.dtype,\n            trust_remote_code=True,\n        )\n        if cfg.load_ckpt and not cfg.tokenizer_only\n        else None\n    )\n    hf_tokenizer = AutoTokenizer.from_pretrained(\n        (cfg.model_name if cfg.model_from_pretrained_path is None else cfg.model_from_pretrained_path),\n        cache_dir=cfg.cache_dir,\n        trust_remote_code=True,\n        use_fast=True,\n        add_bos_token=True,\n        local_files_only=cfg.local_files_only,\n    )\n    self.tokenizer = set_tokens(\n        hf_tokenizer,\n        cfg.bos_token_id,\n        cfg.eos_token_id,\n        cfg.pad_token_id,\n    )\n    self.model = (\n        HookedTransformer.from_pretrained_no_processing(\n            cfg.model_name,\n            use_flash_attn=cfg.use_flash_attn,\n            device=self.device,\n            cache_dir=cfg.cache_dir,\n            hf_model=hf_model,\n            hf_config=hf_model.config,\n            tokenizer=hf_tokenizer,\n            dtype=cfg.dtype,  # type: ignore ; issue with transformer_lens\n        )\n        if hf_model and not cfg.tokenizer_only\n        else None\n    )\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.TransformerLensLanguageModel.run_with_cache_until","title":"run_with_cache_until","text":"<pre><code>run_with_cache_until(*args, **kwargs) -&gt; Any\n</code></pre> <p>Run with activation caching, stopping at a given hook for efficiency.</p> Source code in <code>src/lm_saes/backend/language_model.py</code> <pre><code>def run_with_cache_until(self, *args, **kwargs) -&gt; Any:\n    \"\"\"Run with activation caching, stopping at a given hook for efficiency.\"\"\"\n    assert self.model is not None, \"model must be initialized\"\n    if self.device_mesh is None:\n        return run_with_cache_until(self.model, *args, **kwargs)\n\n    args = pytree.tree_map(self._to_tensor, args)\n    kwargs = pytree.tree_map(self._to_tensor, kwargs)\n    return pytree.tree_map(self._to_dtensor, run_with_cache_until(self.model, *args, **kwargs))\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.DatasetConfig","title":"DatasetConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>dataset_name_or_path</code>                 (<code>str</code>)             </li> <li> <code>cache_dir</code>                 (<code>str | None</code>)             </li> <li> <code>is_dataset_on_disk</code>                 (<code>bool</code>)             </li> </ul>"},{"location":"reference/infrastructure/#lm_saes.DatasetConfig.dataset_name_or_path","title":"dataset_name_or_path  <code>pydantic-field</code>","text":"<pre><code>dataset_name_or_path: str = 'openwebtext'\n</code></pre> <p>The name or path to the dataset. Should be a valid dataset name or path for <code>datasets.load_dataset</code> or <code>datasets.load_from_disk</code>, depending on <code>is_dataset_on_disk</code>.</p>"},{"location":"reference/infrastructure/#lm_saes.DatasetConfig.cache_dir","title":"cache_dir  <code>pydantic-field</code>","text":"<pre><code>cache_dir: str | None = None\n</code></pre> <p>The directory to cache the dataset. Will be passed to <code>datasets.load_dataset</code>.</p>"},{"location":"reference/infrastructure/#lm_saes.DatasetConfig.is_dataset_on_disk","title":"is_dataset_on_disk  <code>pydantic-field</code>","text":"<pre><code>is_dataset_on_disk: bool = False\n</code></pre> <p>Whether the dataset is saved through <code>datasets.save_to_disk</code>. If True, the dataset will be loaded through <code>datasets.load_from_disk</code>. Otherwise, it will be loaded through <code>datasets.load_dataset</code>.</p>"},{"location":"reference/infrastructure/#lm_saes.MongoDBConfig","title":"MongoDBConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Fields:</p> <ul> <li> <code>mongo_uri</code>                 (<code>str</code>)             </li> <li> <code>mongo_db</code>                 (<code>str</code>)             </li> </ul>"},{"location":"reference/infrastructure/#lm_saes.MongoClient","title":"MongoClient","text":"<pre><code>MongoClient(cfg: MongoDBConfig)\n</code></pre> Source code in <code>src/lm_saes/database.py</code> <pre><code>def __init__(self, cfg: MongoDBConfig):\n    self.client: pymongo.MongoClient = pymongo.MongoClient(cfg.mongo_uri)\n    self.db = self.client[cfg.mongo_db]\n    self.fs: gridfs.GridFS | None = None\n    self.feature_collection = self.db[\"features\"]\n    self.sae_collection = self.db[\"saes\"]\n    self.analysis_collection = self.db[\"analyses\"]\n    self.dataset_collection = self.db[\"datasets\"]\n    self.model_collection = self.db[\"models\"]\n    self.bookmark_collection = self.db[\"bookmarks\"]\n    self.sae_set_collection = self.db[\"sae_sets\"]\n    self.circuit_collection = self.db[\"circuits\"]\n    self.sae_collection.create_index([(\"name\", pymongo.ASCENDING), (\"series\", pymongo.ASCENDING)], unique=True)\n    self.sae_collection.create_index([(\"series\", pymongo.ASCENDING)])\n    self.analysis_collection.create_index(\n        [(\"name\", pymongo.ASCENDING), (\"sae_name\", pymongo.ASCENDING), (\"sae_series\", pymongo.ASCENDING)],\n        unique=True,\n    )\n    self.feature_collection.create_index(\n        [(\"sae_name\", pymongo.ASCENDING), (\"sae_series\", pymongo.ASCENDING), (\"index\", pymongo.ASCENDING)],\n        unique=True,\n    )\n    self.dataset_collection.create_index([(\"name\", pymongo.ASCENDING)], unique=True)\n    self.model_collection.create_index([(\"name\", pymongo.ASCENDING)], unique=True)\n    self.sae_set_collection.create_index([(\"name\", pymongo.ASCENDING)], unique=True)\n    self.sae_set_collection.create_index([(\"sae_series\", pymongo.ASCENDING)])\n    self.bookmark_collection.create_index(\n        [(\"sae_name\", pymongo.ASCENDING), (\"sae_series\", pymongo.ASCENDING), (\"feature_index\", pymongo.ASCENDING)],\n        unique=True,\n    )\n    self.bookmark_collection.create_index([(\"created_at\", pymongo.DESCENDING)])\n    self.circuit_collection.create_index([(\"sae_series\", pymongo.ASCENDING)])\n    self.circuit_collection.create_index([(\"created_at\", pymongo.DESCENDING)])\n\n    # Initialize GridFS by default\n    self._init_fs()\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.enable_gridfs","title":"enable_gridfs","text":"<pre><code>enable_gridfs() -&gt; None\n</code></pre> <p>Enable GridFS for storing large binary data.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def enable_gridfs(self) -&gt; None:\n    \"\"\"Enable GridFS for storing large binary data.\"\"\"\n    if self.fs is None:\n        self._init_fs()\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.disable_gridfs","title":"disable_gridfs","text":"<pre><code>disable_gridfs() -&gt; None\n</code></pre> <p>Disable GridFS usage.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def disable_gridfs(self) -&gt; None:\n    \"\"\"Disable GridFS usage.\"\"\"\n    self.fs = None\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.is_gridfs_enabled","title":"is_gridfs_enabled","text":"<pre><code>is_gridfs_enabled() -&gt; bool\n</code></pre> <p>Check if GridFS is enabled.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def is_gridfs_enabled(self) -&gt; bool:\n    \"\"\"Check if GridFS is enabled.\"\"\"\n    return self.fs is not None\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_sae","title":"update_sae","text":"<pre><code>update_sae(\n    name: str, series: str, update_data: dict[str, Any]\n)\n</code></pre> <p>Update an SAE and all its references.</p> <p>If the name is updated, all references in other collections are also updated within a transaction.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_sae(self, name: str, series: str, update_data: dict[str, Any]):\n    \"\"\"Update an SAE and all its references.\n\n    If the name is updated, all references in other collections are also updated within a transaction.\n    \"\"\"\n    new_name = update_data.get(\"name\")\n    if new_name and new_name != name:\n        with self.client.start_session() as session:\n            with session.start_transaction():\n                self.feature_collection.update_many(\n                    {\"sae_name\": name, \"sae_series\": series}, {\"$set\": {\"sae_name\": new_name}}, session=session\n                )\n                self.analysis_collection.update_many(\n                    {\"sae_name\": name, \"sae_series\": series}, {\"$set\": {\"sae_name\": new_name}}, session=session\n                )\n                self.bookmark_collection.update_many(\n                    {\"sae_name\": name, \"sae_series\": series}, {\"$set\": {\"sae_name\": new_name}}, session=session\n                )\n                self.sae_set_collection.update_many(\n                    {\"sae_names\": name, \"sae_series\": series},\n                    {\"$set\": {\"sae_names.$[elem]\": new_name}},\n                    array_filters=[{\"elem\": name}],\n                    session=session,\n                )\n                self.circuit_collection.update_many(\n                    {\"clt_names\": name, \"sae_series\": series},\n                    {\"$set\": {\"clt_names.$[elem]\": new_name}},\n                    array_filters=[{\"elem\": name}],\n                    session=session,\n                )\n                self.circuit_collection.update_many(\n                    {\"lorsa_names\": name, \"sae_series\": series},\n                    {\"$set\": {\"lorsa_names.$[elem]\": new_name}},\n                    array_filters=[{\"elem\": name}],\n                    session=session,\n                )\n                self.sae_collection.update_one(\n                    {\"name\": name, \"series\": series}, {\"$set\": update_data}, session=session\n                )\n    else:\n        self.sae_collection.update_one({\"name\": name, \"series\": series}, {\"$set\": update_data})\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_sae_set","title":"update_sae_set","text":"<pre><code>update_sae_set(name: str, update_data: dict[str, Any])\n</code></pre> <p>Update an SAE set and all its references.</p> <p>If the name is updated, all references in other collections are also updated within a transaction.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_sae_set(self, name: str, update_data: dict[str, Any]):\n    \"\"\"Update an SAE set and all its references.\n\n    If the name is updated, all references in other collections are also updated within a transaction.\n    \"\"\"\n    new_name = update_data.get(\"name\")\n    if new_name and new_name != name:\n        with self.client.start_session() as session:\n            with session.start_transaction():\n                self.circuit_collection.update_many(\n                    {\"sae_set_name\": name}, {\"$set\": {\"sae_set_name\": new_name}}, session=session\n                )\n                self.sae_set_collection.update_one({\"name\": name}, {\"$set\": update_data}, session=session)\n    else:\n        self.sae_set_collection.update_one({\"name\": name}, {\"$set\": update_data})\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.get_random_alive_feature","title":"get_random_alive_feature","text":"<pre><code>get_random_alive_feature(\n    sae_name: str,\n    sae_series: str,\n    name: str | None = None,\n    metric_filters: dict[str, dict[str, float]]\n    | None = None,\n) -&gt; FeatureRecord | None\n</code></pre> <p>Get a random feature that has non-zero activation.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE model</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE model</p> required <code>name</code> <code>str | None</code> <p>Name of the analysis</p> <code>None</code> <code>metric_filters</code> <code>dict[str, dict[str, float]] | None</code> <p>Optional dict of metric filters in the format {\"metric_name\": {\"\\(gte\": value, \"\\)lte\": value}}</p> <code>None</code> <p>Returns:</p> Type Description <code>FeatureRecord | None</code> <p>A random feature record with non-zero activation, or None if no such feature exists</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def get_random_alive_feature(\n    self,\n    sae_name: str,\n    sae_series: str,\n    name: str | None = None,\n    metric_filters: Optional[dict[str, dict[str, float]]] = None,\n) -&gt; Optional[FeatureRecord]:\n    \"\"\"Get a random feature that has non-zero activation.\n\n    Args:\n        sae_name: Name of the SAE model\n        sae_series: Series of the SAE model\n        name: Name of the analysis\n        metric_filters: Optional dict of metric filters in the format {\"metric_name\": {\"$gte\": value, \"$lte\": value}}\n\n    Returns:\n        A random feature record with non-zero activation, or None if no such feature exists\n    \"\"\"\n    elem_match: dict[str, Any] = {\"max_feature_acts\": {\"$gt\": 0}}\n    if name is not None:\n        elem_match[\"name\"] = name\n\n    match_filter: dict[str, Any] = {\n        \"sae_name\": sae_name,\n        \"sae_series\": sae_series,\n        \"analyses\": {\"$elemMatch\": elem_match},\n    }\n\n    # Add metric filters if provided\n    if metric_filters:\n        for metric_name, filters in metric_filters.items():\n            match_filter[f\"metric.{metric_name}\"] = filters\n\n    pipeline = [\n        {\"$match\": match_filter},\n        {\"$sample\": {\"size\": 1}},\n    ]\n    feature = next(self.feature_collection.aggregate(pipeline), None)\n    if feature is None:\n        return None\n\n    # Convert GridFS references back to numpy arrays\n    if self.is_gridfs_enabled():\n        feature = self._from_gridfs(feature)\n\n    return FeatureRecord.model_validate(feature)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_feature","title":"update_feature","text":"<pre><code>update_feature(\n    sae_name: str,\n    feature_index: int,\n    update_data: dict,\n    sae_series: str | None = None,\n)\n</code></pre> <p>Update a feature with additional data.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE</p> required <code>feature_index</code> <code>int</code> <p>Index of the feature to update</p> required <code>update_data</code> <code>dict</code> <p>Dictionary with data to update</p> required <code>sae_series</code> <code>str | None</code> <p>Optional series of the SAE</p> <code>None</code> <p>Returns:</p> Type Description <p>Result of the update operation</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the feature doesn't exist</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_feature(self, sae_name: str, feature_index: int, update_data: dict, sae_series: str | None = None):\n    \"\"\"Update a feature with additional data.\n\n    Args:\n        sae_name: Name of the SAE\n        feature_index: Index of the feature to update\n        update_data: Dictionary with data to update\n        sae_series: Optional series of the SAE\n\n    Returns:\n        Result of the update operation\n\n    Raises:\n        ValueError: If the feature doesn't exist\n    \"\"\"\n    # Ensure we have a non-None sae_series\n    if sae_series is None:\n        raise ValueError(\"sae_series cannot be None\")\n\n    feature = self.get_feature(sae_name, sae_series, feature_index)\n    if feature is None:\n        raise ValueError(f\"Feature {feature_index} not found for SAE {sae_name}/{sae_series}\")\n\n    # Initialize GridFS if not already done\n    if not self.is_gridfs_enabled():\n        self.enable_gridfs()\n\n    # Convert numpy arrays to GridFS references\n    processed_update_data = self._to_gridfs(update_data)\n\n    result = self.feature_collection.update_one(\n        {\"sae_name\": sae_name, \"sae_series\": sae_series, \"index\": feature_index}, {\"$set\": processed_update_data}\n    )\n\n    return result\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.add_bookmark","title":"add_bookmark","text":"<pre><code>add_bookmark(\n    sae_name: str,\n    sae_series: str,\n    feature_index: int,\n    tags: list[str] | None = None,\n    notes: str | None = None,\n) -&gt; bool\n</code></pre> <p>Add a bookmark for a feature.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE</p> required <code>feature_index</code> <code>int</code> <p>Index of the feature to bookmark</p> required <code>tags</code> <code>list[str] | None</code> <p>Optional list of tags for the bookmark</p> <code>None</code> <code>notes</code> <code>str | None</code> <p>Optional notes for the bookmark</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if bookmark was added, False if it already exists</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the feature doesn't exist</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def add_bookmark(\n    self,\n    sae_name: str,\n    sae_series: str,\n    feature_index: int,\n    tags: Optional[list[str]] = None,\n    notes: Optional[str] = None,\n) -&gt; bool:\n    \"\"\"Add a bookmark for a feature.\n\n    Args:\n        sae_name: Name of the SAE\n        sae_series: Series of the SAE\n        feature_index: Index of the feature to bookmark\n        tags: Optional list of tags for the bookmark\n        notes: Optional notes for the bookmark\n\n    Returns:\n        bool: True if bookmark was added, False if it already exists\n\n    Raises:\n        ValueError: If the feature doesn't exist\n    \"\"\"\n    # Check if feature exists\n    feature = self.get_feature(sae_name, sae_series, feature_index)\n    if feature is None:\n        raise ValueError(f\"Feature {feature_index} not found for SAE {sae_name}/{sae_series}\")\n\n    bookmark_data = {\n        \"sae_name\": sae_name,\n        \"sae_series\": sae_series,\n        \"feature_index\": feature_index,\n        \"created_at\": datetime.utcnow(),\n        \"tags\": tags or [],\n        \"notes\": notes,\n    }\n\n    try:\n        result = self.bookmark_collection.insert_one(bookmark_data)\n        return result.inserted_id is not None\n    except pymongo.errors.DuplicateKeyError:\n        return False\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.remove_bookmark","title":"remove_bookmark","text":"<pre><code>remove_bookmark(\n    sae_name: str, sae_series: str, feature_index: int\n) -&gt; bool\n</code></pre> <p>Remove a bookmark for a feature.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE</p> required <code>feature_index</code> <code>int</code> <p>Index of the feature to remove bookmark from</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if bookmark was removed, False if it didn't exist</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def remove_bookmark(self, sae_name: str, sae_series: str, feature_index: int) -&gt; bool:\n    \"\"\"Remove a bookmark for a feature.\n\n    Args:\n        sae_name: Name of the SAE\n        sae_series: Series of the SAE\n        feature_index: Index of the feature to remove bookmark from\n\n    Returns:\n        bool: True if bookmark was removed, False if it didn't exist\n    \"\"\"\n    result = self.bookmark_collection.delete_one(\n        {\n            \"sae_name\": sae_name,\n            \"sae_series\": sae_series,\n            \"feature_index\": feature_index,\n        }\n    )\n    return result.deleted_count &gt; 0\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.is_bookmarked","title":"is_bookmarked","text":"<pre><code>is_bookmarked(\n    sae_name: str, sae_series: str, feature_index: int\n) -&gt; bool\n</code></pre> <p>Check if a feature is bookmarked.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE</p> required <code>feature_index</code> <code>int</code> <p>Index of the feature</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the feature is bookmarked, False otherwise</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def is_bookmarked(self, sae_name: str, sae_series: str, feature_index: int) -&gt; bool:\n    \"\"\"Check if a feature is bookmarked.\n\n    Args:\n        sae_name: Name of the SAE\n        sae_series: Series of the SAE\n        feature_index: Index of the feature\n\n    Returns:\n        bool: True if the feature is bookmarked, False otherwise\n    \"\"\"\n    bookmark = self.bookmark_collection.find_one(\n        {\n            \"sae_name\": sae_name,\n            \"sae_series\": sae_series,\n            \"feature_index\": feature_index,\n        }\n    )\n    return bookmark is not None\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.get_bookmark","title":"get_bookmark","text":"<pre><code>get_bookmark(\n    sae_name: str, sae_series: str, feature_index: int\n) -&gt; BookmarkRecord | None\n</code></pre> <p>Get a specific bookmark.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE</p> required <code>feature_index</code> <code>int</code> <p>Index of the feature</p> required <p>Returns:</p> Name Type Description <code>BookmarkRecord</code> <code>BookmarkRecord | None</code> <p>The bookmark record if it exists, None otherwise</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def get_bookmark(self, sae_name: str, sae_series: str, feature_index: int) -&gt; Optional[BookmarkRecord]:\n    \"\"\"Get a specific bookmark.\n\n    Args:\n        sae_name: Name of the SAE\n        sae_series: Series of the SAE\n        feature_index: Index of the feature\n\n    Returns:\n        BookmarkRecord: The bookmark record if it exists, None otherwise\n    \"\"\"\n    bookmark = self.bookmark_collection.find_one(\n        {\n            \"sae_name\": sae_name,\n            \"sae_series\": sae_series,\n            \"feature_index\": feature_index,\n        }\n    )\n    if bookmark is None:\n        return None\n    return BookmarkRecord.model_validate(bookmark)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.list_bookmarks","title":"list_bookmarks","text":"<pre><code>list_bookmarks(\n    sae_name: str | None = None,\n    sae_series: str | None = None,\n    tags: list[str] | None = None,\n    limit: int | None = None,\n    skip: int = 0,\n) -&gt; list[BookmarkRecord]\n</code></pre> <p>List bookmarks with optional filtering.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def list_bookmarks(\n    self,\n    sae_name: Optional[str] = None,\n    sae_series: Optional[str] = None,\n    tags: Optional[list[str]] = None,\n    limit: Optional[int] = None,\n    skip: int = 0,\n) -&gt; list[BookmarkRecord]:\n    \"\"\"List bookmarks with optional filtering.\"\"\"\n    query = {}\n\n    if sae_name is not None:\n        query[\"sae_name\"] = sae_name\n    if sae_series is not None:\n        query[\"sae_series\"] = sae_series\n    if tags:\n        query[\"tags\"] = {\"$in\": tags}\n\n    cursor = self.bookmark_collection.find(query).sort(\"created_at\", pymongo.DESCENDING)\n\n    if skip &gt; 0:\n        cursor = cursor.skip(skip)\n    if limit is not None:\n        cursor = cursor.limit(limit)\n\n    return [BookmarkRecord.model_validate(bookmark) for bookmark in cursor]\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_bookmark","title":"update_bookmark","text":"<pre><code>update_bookmark(\n    sae_name: str,\n    sae_series: str,\n    feature_index: int,\n    tags: list[str] | None = None,\n    notes: str | None = None,\n) -&gt; bool\n</code></pre> <p>Update an existing bookmark.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_bookmark(\n    self,\n    sae_name: str,\n    sae_series: str,\n    feature_index: int,\n    tags: Optional[list[str]] = None,\n    notes: Optional[str] = None,\n) -&gt; bool:\n    \"\"\"Update an existing bookmark.\"\"\"\n    update_data = {}\n    if tags is not None:\n        update_data[\"tags\"] = tags\n    if notes is not None:\n        update_data[\"notes\"] = notes\n\n    if not update_data:\n        return True  # Nothing to update\n\n    result = self.bookmark_collection.update_one(\n        {\n            \"sae_name\": sae_name,\n            \"sae_series\": sae_series,\n            \"feature_index\": feature_index,\n        },\n        {\"$set\": update_data},\n    )\n    return result.modified_count &gt; 0\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.get_bookmark_count","title":"get_bookmark_count","text":"<pre><code>get_bookmark_count(\n    sae_name: str | None = None,\n    sae_series: str | None = None,\n) -&gt; int\n</code></pre> <p>Get the total count of bookmarks with optional filtering.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def get_bookmark_count(self, sae_name: Optional[str] = None, sae_series: Optional[str] = None) -&gt; int:\n    \"\"\"Get the total count of bookmarks with optional filtering.\"\"\"\n    query = {}\n    if sae_name is not None:\n        query[\"sae_name\"] = sae_name\n    if sae_series is not None:\n        query[\"sae_series\"] = sae_series\n\n    return self.bookmark_collection.count_documents(query)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.get_available_metrics","title":"get_available_metrics","text":"<pre><code>get_available_metrics(\n    sae_name: str, sae_series: str\n) -&gt; list[str]\n</code></pre> <p>Get available metrics for an SAE by checking the first feature.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE model</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE model</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of available metric names</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def get_available_metrics(self, sae_name: str, sae_series: str) -&gt; list[str]:\n    \"\"\"Get available metrics for an SAE by checking the first feature.\n\n    Args:\n        sae_name: Name of the SAE model\n        sae_series: Series of the SAE model\n\n    Returns:\n        List of available metric names\n    \"\"\"\n    # Use projection to avoid loading large arrays from analyses[0].samplings\n    projection = {\n        \"metric\": 1,\n    }\n\n    first_feature = self.feature_collection.find_one({\"sae_name\": sae_name, \"sae_series\": sae_series}, projection)\n\n    if first_feature is None or first_feature.get(\"metric\") is None:\n        return []\n\n    return list(first_feature[\"metric\"].keys())\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.count_features_with_filters","title":"count_features_with_filters","text":"<pre><code>count_features_with_filters(\n    sae_name: str,\n    sae_series: str,\n    name: str | None = None,\n    metric_filters: dict[str, dict[str, float]]\n    | None = None,\n) -&gt; int\n</code></pre> <p>Count features that match the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>sae_name</code> <code>str</code> <p>Name of the SAE model</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE model</p> required <code>name</code> <code>str | None</code> <p>Name of the analysis</p> <code>None</code> <code>metric_filters</code> <code>dict[str, dict[str, float]] | None</code> <p>Optional dict of metric filters in the format {\"metric_name\": {\"\\(gte\": value, \"\\)lte\": value}}</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of features matching the filters</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def count_features_with_filters(\n    self,\n    sae_name: str,\n    sae_series: str,\n    name: str | None = None,\n    metric_filters: Optional[dict[str, dict[str, float]]] = None,\n) -&gt; int:\n    \"\"\"Count features that match the given filters.\n\n    Args:\n        sae_name: Name of the SAE model\n        sae_series: Series of the SAE model\n        name: Name of the analysis\n        metric_filters: Optional dict of metric filters in the format {\"metric_name\": {\"$gte\": value, \"$lte\": value}}\n\n    Returns:\n        Number of features matching the filters\n    \"\"\"\n    elem_match: dict[str, Any] = {\"max_feature_acts\": {\"$gt\": 0}}\n    if name is not None:\n        elem_match[\"name\"] = name\n\n    match_filter: dict[str, Any] = {\n        \"sae_name\": sae_name,\n        \"sae_series\": sae_series,\n        \"analyses\": {\"$elemMatch\": elem_match},\n    }\n\n    # Add metric filters if provided\n    if metric_filters:\n        for metric_name, filters in metric_filters.items():\n            match_filter[f\"metric.{metric_name}\"] = filters\n\n    return self.feature_collection.count_documents(match_filter)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.create_circuit","title":"create_circuit","text":"<pre><code>create_circuit(\n    sae_set_name: str,\n    sae_series: str,\n    prompt: str,\n    input: CircuitInput,\n    config: CircuitConfig,\n    name: str | None = None,\n    group: str | None = None,\n    parent_id: str | None = None,\n    clt_names: list[str] | None = None,\n    lorsa_names: list[str] | None = None,\n    use_lorsa: bool = True,\n) -&gt; str\n</code></pre> <p>Create a new circuit graph record with pending status.</p> <p>Parameters:</p> Name Type Description Default <code>sae_set_name</code> <code>str</code> <p>Name of the SAE set used.</p> required <code>sae_series</code> <code>str</code> <p>Series of the SAE.</p> required <code>prompt</code> <code>str</code> <p>The prompt used for generation.</p> required <code>input</code> <code>CircuitInput</code> <p>The circuit input configuration.</p> required <code>config</code> <code>CircuitConfig</code> <p>The circuit configuration.</p> required <code>name</code> <code>str | None</code> <p>Optional custom name for the circuit.</p> <code>None</code> <code>group</code> <code>str | None</code> <p>Optional group name.</p> <code>None</code> <code>parent_id</code> <code>str | None</code> <p>Optional parent circuit ID.</p> <code>None</code> <code>clt_names</code> <code>list[str] | None</code> <p>Names of CLT SAEs used.</p> <code>None</code> <code>lorsa_names</code> <code>list[str] | None</code> <p>Names of LORSA SAEs used.</p> <code>None</code> <code>use_lorsa</code> <code>bool</code> <p>Whether LORSA was used.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the created circuit.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def create_circuit(\n    self,\n    sae_set_name: str,\n    sae_series: str,\n    prompt: str,\n    input: CircuitInput,\n    config: CircuitConfig,\n    name: Optional[str] = None,\n    group: Optional[str] = None,\n    parent_id: Optional[str] = None,\n    clt_names: Optional[list[str]] = None,\n    lorsa_names: Optional[list[str]] = None,\n    use_lorsa: bool = True,\n) -&gt; str:\n    \"\"\"Create a new circuit graph record with pending status.\n\n    Args:\n        sae_set_name: Name of the SAE set used.\n        sae_series: Series of the SAE.\n        prompt: The prompt used for generation.\n        input: The circuit input configuration.\n        config: The circuit configuration.\n        name: Optional custom name for the circuit.\n        group: Optional group name.\n        parent_id: Optional parent circuit ID.\n        clt_names: Names of CLT SAEs used.\n        lorsa_names: Names of LORSA SAEs used.\n        use_lorsa: Whether LORSA was used.\n\n    Returns:\n        The ID of the created circuit.\n    \"\"\"\n    circuit_data = {\n        \"name\": name,\n        \"group\": group,\n        \"parent_id\": parent_id,\n        \"sae_set_name\": sae_set_name,\n        \"sae_series\": sae_series,\n        \"prompt\": prompt,\n        \"input\": input.model_dump(),\n        \"config\": config.model_dump(),\n        \"created_at\": datetime.utcnow(),\n        \"status\": CircuitStatus.PENDING,\n        \"progress\": 0.0,\n        \"progress_phase\": None,\n        \"error_message\": None,\n        \"raw_graph_id\": None,\n        \"clt_names\": clt_names,\n        \"lorsa_names\": lorsa_names,\n        \"use_lorsa\": use_lorsa,\n    }\n    result = self.circuit_collection.insert_one(circuit_data)\n    return str(result.inserted_id)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.get_circuit","title":"get_circuit","text":"<pre><code>get_circuit(circuit_id: str) -&gt; CircuitRecord | None\n</code></pre> <p>Get a circuit by its ID.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def get_circuit(self, circuit_id: str) -&gt; Optional[CircuitRecord]:\n    \"\"\"Get a circuit by its ID.\"\"\"\n    try:\n        circuit = self.circuit_collection.find_one({\"_id\": ObjectId(circuit_id)})\n    except Exception:\n        return None\n    if circuit is None:\n        return None\n    circuit[\"id\"] = str(circuit.pop(\"_id\"))\n    return CircuitRecord.model_validate(circuit)\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.list_circuits","title":"list_circuits","text":"<pre><code>list_circuits(\n    sae_series: str | None = None,\n    group: str | None = None,\n    limit: int | None = None,\n    skip: int = 0,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>List circuits with optional filtering.</p> <p>Note: raw_graph_id is excluded from the listing for efficiency.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def list_circuits(\n    self,\n    sae_series: Optional[str] = None,\n    group: Optional[str] = None,\n    limit: Optional[int] = None,\n    skip: int = 0,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"List circuits with optional filtering.\n\n    Note: raw_graph_id is excluded from the listing for efficiency.\n    \"\"\"\n    query: dict[str, Any] = {}\n    if sae_series is not None:\n        query[\"sae_series\"] = sae_series\n    if group is not None:\n        query[\"group\"] = group\n\n    # Exclude raw_graph_id from listing\n    projection = {\"raw_graph_id\": 0}\n\n    cursor = self.circuit_collection.find(query, projection=projection).sort(\"created_at\", pymongo.DESCENDING)\n\n    if skip &gt; 0:\n        cursor = cursor.skip(skip)\n    if limit is not None:\n        cursor = cursor.limit(limit)\n\n    circuits = []\n    for circuit in cursor:\n        circuit[\"id\"] = str(circuit.pop(\"_id\"))\n        circuits.append(circuit)\n    return circuits\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_circuits_group","title":"update_circuits_group","text":"<pre><code>update_circuits_group(\n    circuit_ids: list[str], group: str | None\n) -&gt; int\n</code></pre> <p>Update the group for multiple circuits.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_circuits_group(self, circuit_ids: list[str], group: Optional[str]) -&gt; int:\n    \"\"\"Update the group for multiple circuits.\"\"\"\n    try:\n        object_ids = [ObjectId(cid) for cid in circuit_ids]\n    except Exception:\n        return 0\n    result = self.circuit_collection.update_many({\"_id\": {\"$in\": object_ids}}, {\"$set\": {\"group\": group}})\n    return result.modified_count\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_circuit","title":"update_circuit","text":"<pre><code>update_circuit(\n    circuit_id: str, update_data: dict[str, Any]\n) -&gt; bool\n</code></pre> <p>Update a circuit by its ID.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_circuit(self, circuit_id: str, update_data: dict[str, Any]) -&gt; bool:\n    \"\"\"Update a circuit by its ID.\"\"\"\n    try:\n        result = self.circuit_collection.update_one({\"_id\": ObjectId(circuit_id)}, {\"$set\": update_data})\n    except Exception:\n        return False\n    return result.modified_count &gt; 0\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.delete_circuit","title":"delete_circuit","text":"<pre><code>delete_circuit(circuit_id: str) -&gt; bool\n</code></pre> <p>Delete a circuit by its ID.</p> <p>Also deletes the associated raw graph from GridFS if it exists.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def delete_circuit(self, circuit_id: str) -&gt; bool:\n    \"\"\"Delete a circuit by its ID.\n\n    Also deletes the associated raw graph from GridFS if it exists.\n    \"\"\"\n    try:\n        # First get the circuit to find raw_graph_id\n        circuit = self.circuit_collection.find_one({\"_id\": ObjectId(circuit_id)})\n        if circuit and circuit.get(\"raw_graph_id\") and self.fs:\n            try:\n                self.fs.delete(ObjectId(circuit[\"raw_graph_id\"]))\n            except Exception:\n                pass  # Ignore errors when deleting GridFS file\n\n        result = self.circuit_collection.delete_one({\"_id\": ObjectId(circuit_id)})\n    except Exception:\n        return False\n    return result.deleted_count &gt; 0\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_circuit_progress","title":"update_circuit_progress","text":"<pre><code>update_circuit_progress(\n    circuit_id: str,\n    progress: float,\n    progress_phase: str | None = None,\n) -&gt; bool\n</code></pre> <p>Update the progress of a circuit generation.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_id</code> <code>str</code> <p>The circuit ID.</p> required <code>progress</code> <code>float</code> <p>Progress percentage (0-100).</p> required <code>progress_phase</code> <code>str | None</code> <p>Optional phase description.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_circuit_progress(\n    self,\n    circuit_id: str,\n    progress: float,\n    progress_phase: Optional[str] = None,\n) -&gt; bool:\n    \"\"\"Update the progress of a circuit generation.\n\n    Args:\n        circuit_id: The circuit ID.\n        progress: Progress percentage (0-100).\n        progress_phase: Optional phase description.\n\n    Returns:\n        True if update was successful.\n    \"\"\"\n    update_data: dict[str, Any] = {\"progress\": progress}\n    if progress_phase is not None:\n        update_data[\"progress_phase\"] = progress_phase\n\n    try:\n        result = self.circuit_collection.update_one(\n            {\"_id\": ObjectId(circuit_id)},\n            {\"$set\": update_data},\n        )\n    except Exception:\n        return False\n    return result.modified_count &gt; 0\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.update_circuit_status","title":"update_circuit_status","text":"<pre><code>update_circuit_status(\n    circuit_id: str,\n    status: str,\n    error_message: str | None = None,\n) -&gt; bool\n</code></pre> <p>Update the status of a circuit.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_id</code> <code>str</code> <p>The circuit ID.</p> required <code>status</code> <code>str</code> <p>New status (pending, running, completed, failed).</p> required <code>error_message</code> <code>str | None</code> <p>Optional error message for failed status.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def update_circuit_status(\n    self,\n    circuit_id: str,\n    status: str,\n    error_message: Optional[str] = None,\n) -&gt; bool:\n    \"\"\"Update the status of a circuit.\n\n    Args:\n        circuit_id: The circuit ID.\n        status: New status (pending, running, completed, failed).\n        error_message: Optional error message for failed status.\n\n    Returns:\n        True if update was successful.\n    \"\"\"\n    update_data: dict[str, Any] = {\"status\": status}\n    if error_message is not None:\n        update_data[\"error_message\"] = error_message\n\n    try:\n        result = self.circuit_collection.update_one(\n            {\"_id\": ObjectId(circuit_id)},\n            {\"$set\": update_data},\n        )\n    except Exception:\n        return False\n    return result.modified_count &gt; 0\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.store_raw_graph","title":"store_raw_graph","text":"<pre><code>store_raw_graph(\n    circuit_id: str, graph_data: dict[str, Any]\n) -&gt; bool\n</code></pre> <p>Store raw graph data to GridFS and update circuit record.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_id</code> <code>str</code> <p>The circuit ID.</p> required <code>graph_data</code> <code>dict[str, Any]</code> <p>The raw graph data dictionary with numpy arrays.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if storage was successful.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def store_raw_graph(self, circuit_id: str, graph_data: dict[str, Any]) -&gt; bool:\n    \"\"\"Store raw graph data to GridFS and update circuit record.\n\n    Args:\n        circuit_id: The circuit ID.\n        graph_data: The raw graph data dictionary with numpy arrays.\n\n    Returns:\n        True if storage was successful.\n    \"\"\"\n    if not self.is_gridfs_enabled():\n        self.enable_gridfs()\n\n    assert self.fs is not None\n\n    try:\n        # Convert numpy arrays to GridFS references\n        processed_data = self._to_gridfs(graph_data)\n\n        # Store in GridFS as a single document\n        import pickle\n\n        # Use pickle for complex data with GridFS references\n        graph_bytes = pickle.dumps(processed_data)\n        graph_id = self.fs.put(graph_bytes, filename=f\"circuit_{circuit_id}_graph\")\n\n        # Update circuit record with graph ID\n        result = self.circuit_collection.update_one(\n            {\"_id\": ObjectId(circuit_id)},\n            {\"$set\": {\"raw_graph_id\": str(graph_id)}},\n        )\n        return result.modified_count &gt; 0\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.load_raw_graph","title":"load_raw_graph","text":"<pre><code>load_raw_graph(circuit_id: str) -&gt; dict[str, Any] | None\n</code></pre> <p>Load raw graph data from GridFS.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_id</code> <code>str</code> <p>The circuit ID.</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>The raw graph data dictionary with numpy arrays, or None if not found.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def load_raw_graph(self, circuit_id: str) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Load raw graph data from GridFS.\n\n    Args:\n        circuit_id: The circuit ID.\n\n    Returns:\n        The raw graph data dictionary with numpy arrays, or None if not found.\n    \"\"\"\n    if not self.is_gridfs_enabled():\n        self.enable_gridfs()\n\n    assert self.fs is not None\n\n    try:\n        circuit = self.circuit_collection.find_one({\"_id\": ObjectId(circuit_id)})\n        if circuit is None or circuit.get(\"raw_graph_id\") is None:\n            return None\n\n        graph_id = ObjectId(circuit[\"raw_graph_id\"])\n        if not self.fs.exists(graph_id):\n            return None\n\n        import pickle\n\n        graph_bytes = self.fs.get(graph_id).read()\n        processed_data = pickle.loads(graph_bytes)\n\n        # Convert GridFS references back to numpy arrays\n        return self._from_gridfs(processed_data)\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/infrastructure/#lm_saes.MongoClient.get_circuit_status","title":"get_circuit_status","text":"<pre><code>get_circuit_status(\n    circuit_id: str,\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Get just the status information for a circuit.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_id</code> <code>str</code> <p>The circuit ID.</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Dict with status, progress, progress_phase, and error_message.</p> Source code in <code>src/lm_saes/database.py</code> <pre><code>def get_circuit_status(self, circuit_id: str) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Get just the status information for a circuit.\n\n    Args:\n        circuit_id: The circuit ID.\n\n    Returns:\n        Dict with status, progress, progress_phase, and error_message.\n    \"\"\"\n    try:\n        circuit = self.circuit_collection.find_one(\n            {\"_id\": ObjectId(circuit_id)},\n            projection={\n                \"status\": 1,\n                \"progress\": 1,\n                \"progress_phase\": 1,\n                \"error_message\": 1,\n            },\n        )\n    except Exception:\n        return None\n\n    if circuit is None:\n        return None\n\n    return {\n        \"status\": circuit.get(\"status\", CircuitStatus.PENDING),\n        \"progress\": circuit.get(\"progress\", 0.0),\n        \"progress_phase\": circuit.get(\"progress_phase\"),\n        \"error_message\": circuit.get(\"error_message\"),\n    }\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#models","title":"Models","text":"<p>Sparse dictionary model architectures and their configuration classes.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig","title":"BaseSAEConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModelConfig</code>, <code>ABC</code></p> <p>Base class for SAE configs with common settings that are able to apply to various SAE variants. This class should not be used directly but only as a base config class for other SAE variants like SAEConfig, CrossCoderConfig, etc.</p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>sae_type</code>                 (<code>str</code>)             </li> <li> <code>d_model</code>                 (<code>int</code>)             </li> <li> <code>expansion_factor</code>                 (<code>float</code>)             </li> <li> <code>use_decoder_bias</code>                 (<code>bool</code>)             </li> <li> <code>act_fn</code>                 (<code>Literal['relu', 'jumprelu', 'topk', 'batchtopk', 'batchlayertopk', 'layertopk']</code>)             </li> <li> <code>norm_activation</code>                 (<code>Literal['token-wise', 'batch-wise', 'dataset-wise', 'inference']</code>)             </li> <li> <code>sparsity_include_decoder_norm</code>                 (<code>bool</code>)             </li> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>use_triton_kernel</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_triton_spmm_kernel</code>                 (<code>float</code>)             </li> <li> <code>jumprelu_threshold_window</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.sae_type","title":"sae_type  <code>pydantic-field</code>","text":"<pre><code>sae_type: str\n</code></pre> <p>The type of the sparse dictionary. Must be one of the registered SAE types.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.d_model","title":"d_model  <code>pydantic-field</code>","text":"<pre><code>d_model: int\n</code></pre> <p>The dimension of the input/label activation space. In common settings where activations come from a transformer, this is the dimension of the model (may also known as hidden_size).</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.expansion_factor","title":"expansion_factor  <code>pydantic-field</code>","text":"<pre><code>expansion_factor: float\n</code></pre> <p>The expansion factor of the sparse dictionary. The hidden dimension of the sparse dictionary <code>d_sae</code> is <code>d_model * expansion_factor</code>.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.use_decoder_bias","title":"use_decoder_bias  <code>pydantic-field</code>","text":"<pre><code>use_decoder_bias: bool = True\n</code></pre> <p>Whether to use a bias term in the decoder. Including bias term may make it easier to train a better sparse dictionary, in exchange for increased architectural complexity.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.act_fn","title":"act_fn  <code>pydantic-field</code>","text":"<pre><code>act_fn: Literal[\n    \"relu\",\n    \"jumprelu\",\n    \"topk\",\n    \"batchtopk\",\n    \"batchlayertopk\",\n    \"layertopk\",\n] = \"relu\"\n</code></pre> <p>The activation function to use for the sparse dictionary. Currently supported activation functions are <code>relu</code>, <code>jumprelu</code>, <code>topk</code>, <code>batchtopk</code>, <code>batchlayertopk</code>, and <code>layertopk</code>.</p> <ul> <li><code>relu</code>: ReLU activation function. Used in the most vanilla SAE settings.</li> <li><code>jumprelu</code>: JumpReLU activation function, adding a trainable element-wise threshold that pre-activations must pass to be activated, which is formally defined as :math:<code>f(x) = \\max(0, x - \\theta)</code> where :math:<code>\\theta</code> is the threshold. Proposed in Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders.</li> <li><code>topk</code>: TopK activation function. Retains the top K activations per sample, zeroing out the rest. Proposed in Scaling and evaluating sparse autoencoders.</li> <li><code>batchtopk</code>: BatchTopK activation function. Batch TopK relaxes TopK function to batch-level, ing the top <code>k * batch_size</code> activations per batch and zeroing out the rest. This allows more adaptive allocation of latents on each sample. Proposed in BatchTopK Sparse Autoencoders.</li> <li><code>batchlayertopk</code>: (For CrossLayerTranscoder only) Extension of BatchTopK to layer-and-batch-aware, retaining the top <code>k * batch_size * n_layers</code> activations per batch and layer and zeroing out the rest.</li> <li><code>layertopk</code>: (For CrossLayerTranscoder only) Extension of BatchTopK to layer-aware, retaining the top <code>k * n_layers</code> activations per layer and zeroing out the rest. Note that this activation function does not take batch dimension into account.</li> </ul>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.norm_activation","title":"norm_activation  <code>pydantic-field</code>","text":"<pre><code>norm_activation: Literal[\n    \"token-wise\", \"batch-wise\", \"dataset-wise\", \"inference\"\n] = \"dataset-wise\"\n</code></pre> <p>The activation normalization strategy to use for the input/label activations. During call of <code>normalize_activations</code> (which will be called by the Trainer during training), the input/label activations will be normalized to an average norm of :math:<code>\\sqrt{d_{model}}</code>. This allows easier hyperparameter (mostly learning rate) transfer between different scale of model activations, since the MSE loss without normalization is proportional to the square of the activation norm.</p> <p>Different activation normalization strategy determines in what view the norm is averaged, with the following options: - <code>token-wise</code>: Norm is directly computed for activation from each token. No averaging is performed. - <code>batch-wise</code>: Norm is computed for each batch, then averaged over the batch dimension. - <code>dataset-wise</code>: Norm is computed from several samples from the activation. Compared to <code>batch-wise</code>, <code>dataset-wise</code> gives a fixed value of average norm for all activations, preserving the linearity of pre-activation encoding and decoding. - <code>inference</code>: No normalization is performed. A inference mode is produced after calling <code>standardize_parameters_of_dataset_norm</code> method, which folds the dataset-wise average norm into the weights and biases of the model. Switching to <code>inference</code> mode doesn't affect the encoding and decoding as a whole, that is, the reconstructed activations keep the same as the denormalized reconstructed activations in <code>dataset-wise</code> mode. However, the feature activations will reflect the activation scale. This allows real magnitude of feature activations to present during inference.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.sparsity_include_decoder_norm","title":"sparsity_include_decoder_norm  <code>pydantic-field</code>","text":"<pre><code>sparsity_include_decoder_norm: bool = True\n</code></pre> <p>Whether to include the decoder norm term in feature activation gating. If true, the pre-activation hidden states will be scaled by the decoder norm before applying the activation function, and then scale back after the activation function. Formally, considering activation function :math:<code>f(x)</code>, an activation gating function :math:<code>g(x)</code> is defined as :math:<code>g(x) = f(x) / x</code> (element-wise division). When <code>sparsity_include_decoder_norm</code> is True, we replace :math:<code>f(x)</code> with :math:<code>x * g(x * || W_     ext{dec} ||)</code>. This effectively suppresses the training dynamics that model tries to increase the decoder norm in exchange of a smaller feature activation magnitude, resulting in lower sparsity loss (L1 norm).</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.top_k","title":"top_k  <code>pydantic-field</code>","text":"<pre><code>top_k: int = 50\n</code></pre> <p>The k value to use for the topk family of activation functions. For vanilla TopK, the L0 norm of the feature activations will be exactly equal to <code>top_k</code>.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.use_triton_kernel","title":"use_triton_kernel  <code>pydantic-field</code>","text":"<pre><code>use_triton_kernel: bool = False\n</code></pre> <p>Whether to use the Triton SpMM kernel for the sparse matrix multiplication. Currently only supported for vanilla SAE.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.sparsity_threshold_for_triton_spmm_kernel","title":"sparsity_threshold_for_triton_spmm_kernel  <code>pydantic-field</code>","text":"<pre><code>sparsity_threshold_for_triton_spmm_kernel: float = 0.996\n</code></pre> <p>The sparsity threshold for the Triton SpMM kernel. Only when feature activation sparsity reaches this threshold, the Triton SpMM kernel will be used for the sparse matrix multiplication. This is useful for JumpReLU or TopK with a k annealing schedule, where the sparsity is not guaranteed throughout the training.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.jumprelu_threshold_window","title":"jumprelu_threshold_window  <code>pydantic-field</code>","text":"<pre><code>jumprelu_threshold_window: float = 2.0\n</code></pre> <p>The window size for the JumpReLU threshold. When pre-activations are element-wise in the window-neighborhood of the threshold, the threshold will begin to receive gradient. See Anthropic's Circuits Update - January 2025 for more details on how JumpReLU is optimized (where they refer to this window as :math:<code>\\epsilon</code>).</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.d_sae","title":"d_sae  <code>property</code>","text":"<pre><code>d_sae: int\n</code></pre> <p>The hidden dimension of the sparse dictionary. Calculated as <code>d_model * expansion_factor</code>.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.associated_hook_points","title":"associated_hook_points  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>associated_hook_points: list[str]\n</code></pre> <p>List of hook points used by the SAE, including all input and label hook points. This is used to retrieve useful data from the input activation source.</p>"},{"location":"reference/models/#lm_saes.BaseSAEConfig.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(pretrained_name_or_path: str, **kwargs)\n</code></pre> <p>Load the config of the sparse dictionary from a pretrained name or path. Config is read from /config.json (for local storage) or //config.json (for HuggingFace Hub).</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_name_or_path</code> <code>str</code> <p>The path to the pretrained sparse dictionary.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the config constructor.</p> <code>{}</code> Source code in <code>src/lm_saes/abstract_sae.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, pretrained_name_or_path: str, **kwargs):\n    \"\"\"Load the config of the sparse dictionary from a pretrained name or path. Config is read from &lt;pretrained_name_or_path&gt;/config.json (for local storage) or &lt;repo_id&gt;/&lt;name&gt;/config.json (for HuggingFace Hub).\n\n    Args:\n        pretrained_name_or_path (str): The path to the pretrained sparse dictionary.\n        **kwargs: Additional keyword arguments to pass to the config constructor.\n    \"\"\"\n    sae_type = auto_infer_pretrained_sae_type(pretrained_name_or_path)\n    if sae_type == PretrainedSAEType.LOCAL:\n        path = os.path.join(pretrained_name_or_path, \"config.json\")\n    elif sae_type == PretrainedSAEType.HUGGINGFACE:\n        repo_id, name = pretrained_name_or_path.split(\":\")\n        path = os.path.join(hf_hub_download(repo_id=repo_id, filename=f\"{name}/config.json\"), \"config.json\")\n    elif sae_type == PretrainedSAEType.SAELENS:\n        raise ValueError(\n            \"Currently not support directly generating config from SAELens. Try converting the whole model from SAELens through `from_saelens` or `from_pretrained` method instead.\"\n        )\n    else:\n        raise ValueError(f\"Unsupported pretrained type: {sae_type}\")\n\n    with open(path, \"r\") as f:\n        sae_config = json.load(f)\n\n    if cls is BaseSAEConfig:\n        cls = SAE_TYPE_TO_CONFIG_CLASS[sae_config[\"sae_type\"]]\n\n    return cls.model_validate({**sae_config, **kwargs})\n</code></pre>"},{"location":"reference/models/#lm_saes.SAEConfig","title":"SAEConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseSAEConfig</code></p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>d_model</code>                 (<code>int</code>)             </li> <li> <code>expansion_factor</code>                 (<code>float</code>)             </li> <li> <code>use_decoder_bias</code>                 (<code>bool</code>)             </li> <li> <code>act_fn</code>                 (<code>Literal['relu', 'jumprelu', 'topk', 'batchtopk', 'batchlayertopk', 'layertopk']</code>)             </li> <li> <code>norm_activation</code>                 (<code>Literal['token-wise', 'batch-wise', 'dataset-wise', 'inference']</code>)             </li> <li> <code>sparsity_include_decoder_norm</code>                 (<code>bool</code>)             </li> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>use_triton_kernel</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_triton_spmm_kernel</code>                 (<code>float</code>)             </li> <li> <code>jumprelu_threshold_window</code>                 (<code>float</code>)             </li> <li> <code>sae_type</code>                 (<code>str</code>)             </li> <li> <code>hook_point_in</code>                 (<code>str</code>)             </li> <li> <code>hook_point_out</code>                 (<code>str</code>)             </li> <li> <code>use_glu_encoder</code>                 (<code>bool</code>)             </li> </ul>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder","title":"SparseAutoEncoder","text":"<pre><code>SparseAutoEncoder(\n    cfg: SAEConfig, device_mesh: DeviceMesh | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractSparseAutoEncoder</code></p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>def __init__(self, cfg: SAEConfig, device_mesh: DeviceMesh | None = None):\n    super(SparseAutoEncoder, self).__init__(cfg, device_mesh=device_mesh)\n    self.cfg = cfg\n\n    if device_mesh is None:\n        self.W_E = nn.Parameter(torch.empty(cfg.d_model, cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n        self.b_E = nn.Parameter(torch.empty(cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n        self.W_D = nn.Parameter(torch.empty(cfg.d_sae, cfg.d_model, device=cfg.device, dtype=cfg.dtype))\n        if cfg.use_decoder_bias:\n            self.b_D = nn.Parameter(torch.empty(cfg.d_model, device=cfg.device, dtype=cfg.dtype))\n\n        if cfg.use_glu_encoder:\n            self.W_E_glu = nn.Parameter(torch.empty(cfg.d_model, cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n            self.b_E_glu = nn.Parameter(torch.empty(cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n    else:\n        self.W_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.d_model,\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"W_E\"].placements(device_mesh),\n            )\n        )\n        self.b_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"b_E\"].placements(device_mesh),\n            )\n        )\n        self.W_D = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.d_sae,\n                cfg.d_model,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"W_D\"].placements(device_mesh),\n            )\n        )\n        if cfg.use_decoder_bias:\n            self.b_D = nn.Parameter(\n                torch.distributed.tensor.empty(\n                    cfg.d_model,\n                    dtype=cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=self.dim_maps()[\"b_D\"].placements(device_mesh),\n                )\n            )\n        if cfg.use_glu_encoder:\n            self.W_E_glu = nn.Parameter(\n                torch.distributed.tensor.empty(\n                    cfg.d_model,\n                    cfg.d_sae,\n                    dtype=cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=self.dim_maps()[\"W_E_glu\"].placements(device_mesh),\n                )\n            )\n            self.b_E_glu = nn.Parameter(\n                torch.distributed.tensor.empty(\n                    cfg.d_sae,\n                    dtype=cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=self.dim_maps()[\"b_E_glu\"].placements(device_mesh),\n                )\n            )\n\n    self.hook_hidden_pre = HookPoint()\n    self.hook_feature_acts = HookPoint()\n    self.hook_reconstructed = HookPoint()\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.encoder_norm","title":"encoder_norm","text":"<pre><code>encoder_norm(keepdim: bool = False)\n</code></pre> <p>Compute the norm of the encoder weight.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>@override\ndef encoder_norm(self, keepdim: bool = False):\n    \"\"\"Compute the norm of the encoder weight.\"\"\"\n    if not isinstance(self.W_E, DTensor):\n        return torch.norm(self.W_E, p=2, dim=0, keepdim=keepdim).to(self.cfg.device)\n    else:\n        assert self.device_mesh is not None\n        return DTensor.from_local(\n            torch.norm(self.W_E.to_local(), p=2, dim=0, keepdim=keepdim),\n            device_mesh=self.device_mesh,\n            placements=DimMap({\"model\": 1 if keepdim else 0}).placements(self.device_mesh),\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.decoder_norm","title":"decoder_norm","text":"<pre><code>decoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Compute the norm of the decoder weight.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>@override\ndef decoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Compute the norm of the decoder weight.\"\"\"\n    if not isinstance(self.W_D, DTensor):\n        return torch.norm(self.W_D, p=2, dim=1, keepdim=keepdim).to(self.cfg.device)\n    else:\n        assert self.device_mesh is not None\n        return DTensor.from_local(\n            torch.norm(self.W_D.to_local(), p=2, dim=1, keepdim=keepdim),\n            device_mesh=self.device_mesh,\n            placements=DimMap({\"model\": 0}).placements(self.device_mesh),\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.set_decoder_to_fixed_norm","title":"set_decoder_to_fixed_norm","text":"<pre><code>set_decoder_to_fixed_norm(value: float, force_exact: bool)\n</code></pre> <p>Set the decoder weights to a fixed norm.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>@override\n@torch.no_grad()\ndef set_decoder_to_fixed_norm(self, value: float, force_exact: bool):\n    \"\"\"Set the decoder weights to a fixed norm.\"\"\"\n    if force_exact:\n        self.W_D.mul_(value / self.decoder_norm(keepdim=True))\n    else:\n        self.W_D.mul_(value / torch.clamp(self.decoder_norm(keepdim=True), min=value))\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.set_encoder_to_fixed_norm","title":"set_encoder_to_fixed_norm","text":"<pre><code>set_encoder_to_fixed_norm(value: float)\n</code></pre> <p>Set the encoder weights to a fixed norm.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>@torch.no_grad()\ndef set_encoder_to_fixed_norm(self, value: float):\n    \"\"\"Set the encoder weights to a fixed norm.\"\"\"\n    self.W_E.mul_(value / self.encoder_norm(keepdim=True))\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.dim_maps","title":"dim_maps","text":"<pre><code>dim_maps() -&gt; dict[str, DimMap]\n</code></pre> <p>Return a dictionary mapping parameter names to dimension maps.</p> <p>Returns:</p> Type Description <code>dict[str, DimMap]</code> <p>A dictionary mapping parameter names to DimMap objects.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>def dim_maps(self) -&gt; dict[str, DimMap]:\n    \"\"\"Return a dictionary mapping parameter names to dimension maps.\n\n    Returns:\n        A dictionary mapping parameter names to DimMap objects.\n    \"\"\"\n    parent_maps = super().dim_maps()\n    sae_maps = {\n        \"W_E\": DimMap({\"model\": 1}),\n        \"W_D\": DimMap({\"model\": 0}),\n        \"b_E\": DimMap({\"model\": 0}),\n    }\n    if self.cfg.use_decoder_bias:\n        sae_maps[\"b_D\"] = DimMap({})\n    if self.cfg.use_glu_encoder:\n        sae_maps[\"W_E_glu\"] = DimMap({\"model\": 1})\n        sae_maps[\"b_E_glu\"] = DimMap({\"model\": 0})\n    return parent_maps | sae_maps\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.standardize_parameters_of_dataset_norm","title":"standardize_parameters_of_dataset_norm","text":"<pre><code>standardize_parameters_of_dataset_norm()\n</code></pre> <p>Standardize the parameters of the model to account for dataset_norm during inference. This function should be called during inference by the Initializer.</p> <p>During training, the activations correspond to an input <code>x</code> where the norm is sqrt(d_model). However, during inference, the norm of the input <code>x</code> corresponds to the dataset_norm. To ensure consistency between training and inference, the activations during inference are scaled by the factor:</p> <pre><code>scaled_activation = training_activation * (dataset_norm / sqrt(d_model))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_average_activation_norm</code> <code>dict[str, float]</code> <p>A dictionary where keys represent in or out and values specify the average activation norm of the dataset during inference.</p> <p>dataset_average_activation_norm = {     self.cfg.hook_point_in: 1.0,     self.cfg.hook_point_out: 1.0, }</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the internal parameters to reflect the standardized activations and change the norm_activation to \"inference\" mode.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>@torch.no_grad()\ndef standardize_parameters_of_dataset_norm(self):  # should be overridden by subclasses due to side effects\n    \"\"\"\n    Standardize the parameters of the model to account for dataset_norm during inference.\n    This function should be called during inference by the Initializer.\n\n    During training, the activations correspond to an input `x` where the norm is sqrt(d_model).\n    However, during inference, the norm of the input `x` corresponds to the dataset_norm.\n    To ensure consistency between training and inference, the activations during inference\n    are scaled by the factor:\n\n        scaled_activation = training_activation * (dataset_norm / sqrt(d_model))\n\n    Args:\n        dataset_average_activation_norm (dict[str, float]):\n            A dictionary where keys represent in or out and values\n            specify the average activation norm of the dataset during inference.\n\n            dataset_average_activation_norm = {\n                self.cfg.hook_point_in: 1.0,\n                self.cfg.hook_point_out: 1.0,\n            }\n\n    Returns:\n        None: Updates the internal parameters to reflect the standardized activations and change the norm_activation to \"inference\" mode.\n    \"\"\"\n    assert self.cfg.norm_activation == \"dataset-wise\"\n    assert self.dataset_average_activation_norm is not None\n    input_norm_factor: float = (\n        math.sqrt(self.cfg.d_model) / self.dataset_average_activation_norm[self.cfg.hook_point_in]\n    )\n    output_norm_factor: float = (\n        math.sqrt(self.cfg.d_model) / self.dataset_average_activation_norm[self.cfg.hook_point_out]\n    )\n    self.b_E.div_(input_norm_factor)\n    if self.cfg.use_decoder_bias:\n        assert self.b_D is not None, \"Decoder bias should exist if use_decoder_bias is True\"\n        self.b_D.div_(output_norm_factor)\n    self.W_D.mul_(input_norm_factor / output_norm_factor)\n    self.cfg.norm_activation = \"inference\"\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.encode","title":"encode","text":"<pre><code>encode(\n    x: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: Literal[False] = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"]\n)\n</code></pre><pre><code>encode(\n    x: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: Literal[True],\n    **kwargs,\n) -&gt; tuple[\n    Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"],\n    Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"],\n]\n</code></pre> <pre><code>encode(\n    x: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: bool = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"]\n    | tuple[\n        Float[Tensor, \"batch d_sae\"]\n        | Float[Tensor, \"batch seq_len d_sae\"],\n        Float[Tensor, \"batch d_sae\"]\n        | Float[Tensor, \"batch seq_len d_sae\"],\n    ]\n)\n</code></pre> <p>Encode input tensor through the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch d_model'] | Float[Tensor, 'batch seq_len d_model']</code> <p>Input tensor of shape (batch, d_model) or (batch, seq_len, d_model)</p> required <code>return_hidden_pre</code> <code>bool</code> <p>If True, also return the pre-activation hidden states</p> <code>False</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae'] | tuple[Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae'], Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae']]</code> <p>If return_hidden_pre is False: Feature activations tensor of shape (batch, d_sae) or (batch, seq_len, d_sae)</p> <code>Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae'] | tuple[Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae'], Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae']]</code> <p>If return_hidden_pre is True: Tuple of (feature_acts, hidden_pre) where both have shape (batch, d_sae) or (batch, seq_len, d_sae)</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>def encode(\n    self,\n    x: Union[\n        Float[torch.Tensor, \"batch d_model\"],\n        Float[torch.Tensor, \"batch seq_len d_model\"],\n    ],\n    return_hidden_pre: bool = False,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch d_sae\"],\n    Float[torch.Tensor, \"batch seq_len d_sae\"],\n    tuple[\n        Union[\n            Float[torch.Tensor, \"batch d_sae\"],\n            Float[torch.Tensor, \"batch seq_len d_sae\"],\n        ],\n        Union[\n            Float[torch.Tensor, \"batch d_sae\"],\n            Float[torch.Tensor, \"batch seq_len d_sae\"],\n        ],\n    ],\n]:\n    \"\"\"Encode input tensor through the sparse autoencoder.\n\n    Args:\n        x: Input tensor of shape (batch, d_model) or (batch, seq_len, d_model)\n        return_hidden_pre: If True, also return the pre-activation hidden states\n\n    Returns:\n        If return_hidden_pre is False:\n            Feature activations tensor of shape (batch, d_sae) or (batch, seq_len, d_sae)\n        If return_hidden_pre is True:\n            Tuple of (feature_acts, hidden_pre) where both have shape (batch, d_sae) or (batch, seq_len, d_sae)\n    \"\"\"\n    # Pass through encoder\n    hidden_pre = x @ self.W_E + self.b_E\n\n    # Apply GLU if configured\n    if self.cfg.use_glu_encoder:\n        hidden_pre_glu = torch.sigmoid(x @ self.W_E_glu + self.b_E_glu)\n        hidden_pre = hidden_pre * hidden_pre_glu\n\n    hidden_pre = self.hook_hidden_pre(hidden_pre)\n\n    # Scale feature activations by decoder norm if configured\n    if self.cfg.sparsity_include_decoder_norm:\n        hidden_pre = hidden_pre * self.decoder_norm()\n\n    feature_acts = self.activation_function(hidden_pre)\n    feature_acts = self.hook_feature_acts(feature_acts)\n\n    if self.cfg.sparsity_include_decoder_norm:\n        feature_acts = feature_acts / self.decoder_norm()\n        hidden_pre = hidden_pre / self.decoder_norm()\n\n    if return_hidden_pre:\n        return feature_acts, hidden_pre\n    return feature_acts\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.decode_coo","title":"decode_coo","text":"<pre><code>decode_coo(\n    feature_acts: Float[Tensor, \"seq_len d_sae\"],\n) -&gt; Float[Tensor, \"seq_len d_model\"]\n</code></pre> <p>Decode feature activations back to model space using COO format.</p> Source code in <code>src/lm_saes/sae.py</code> <pre><code>def decode_coo(\n    self,\n    feature_acts: Float[torch.sparse.Tensor, \"seq_len d_sae\"],\n) -&gt; Float[torch.Tensor, \"seq_len d_model\"]:\n    \"\"\"Decode feature activations back to model space using COO format.\"\"\"\n    reconstructed = feature_acts.to(torch.float32) @ self.W_D.to(torch.float32)\n    if self.cfg.use_decoder_bias:\n        reconstructed = reconstructed + self.b_D\n    return reconstructed.to(self.cfg.dtype)\n</code></pre>"},{"location":"reference/models/#lm_saes.SparseAutoEncoder.init_W_D_with_active_subspace","title":"init_W_D_with_active_subspace","text":"<pre><code>init_W_D_with_active_subspace(\n    batch: dict[str, Tensor], d_active_subspace: int\n)\n</code></pre> <p>Initialize W_D with the active subspace.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch.</p> required <code>d_active_subspace</code> <code>int</code> <p>The dimension of the active subspace.</p> required Source code in <code>src/lm_saes/sae.py</code> <pre><code>@override\n@torch.no_grad()\n@torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\ndef init_W_D_with_active_subspace(self, batch: dict[str, torch.Tensor], d_active_subspace: int):\n    \"\"\"Initialize W_D with the active subspace.\n\n    Args:\n        batch: The batch.\n        d_active_subspace: The dimension of the active subspace.\n    \"\"\"\n    label = self.prepare_label(batch)\n    if self.device_mesh is not None:\n        assert isinstance(label, DTensor)\n        label = label.to_local()\n        torch.distributed.broadcast(tensor=label, group=self.device_mesh.get_group(\"data\"), group_src=0)\n    demeaned_label = label - label.mean(dim=0)\n    U, S, V = torch.svd(demeaned_label.T.to(torch.float32))\n    proj_weight = U[:, :d_active_subspace]  # [d_model, d_active_subspace]\n    self.W_D.copy_(self.W_D.data[:, :d_active_subspace] @ proj_weight.T.to(self.cfg.dtype))\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoderConfig","title":"CrossCoderConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseSAEConfig</code></p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>d_model</code>                 (<code>int</code>)             </li> <li> <code>expansion_factor</code>                 (<code>float</code>)             </li> <li> <code>use_decoder_bias</code>                 (<code>bool</code>)             </li> <li> <code>act_fn</code>                 (<code>Literal['relu', 'jumprelu', 'topk', 'batchtopk', 'batchlayertopk', 'layertopk']</code>)             </li> <li> <code>norm_activation</code>                 (<code>Literal['token-wise', 'batch-wise', 'dataset-wise', 'inference']</code>)             </li> <li> <code>sparsity_include_decoder_norm</code>                 (<code>bool</code>)             </li> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>use_triton_kernel</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_triton_spmm_kernel</code>                 (<code>float</code>)             </li> <li> <code>jumprelu_threshold_window</code>                 (<code>float</code>)             </li> <li> <code>sae_type</code>                 (<code>str</code>)             </li> <li> <code>hook_points</code>                 (<code>list[str]</code>)             </li> </ul>"},{"location":"reference/models/#lm_saes.CrossCoderConfig.hook_points","title":"hook_points  <code>pydantic-field</code>","text":"<pre><code>hook_points: list[str]\n</code></pre> <p>Hook points for each head. Crosscoder reads from these hook points (simultaneously) and writes to the same hook points.</p>"},{"location":"reference/models/#lm_saes.CrossCoder","title":"CrossCoder","text":"<pre><code>CrossCoder(\n    cfg: CrossCoderConfig,\n    device_mesh: DeviceMesh | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractSparseAutoEncoder</code></p> <p>Sparse AutoEncoder model.</p> <p>An autoencoder model that learns to compress the input activation tensor into a high-dimensional but sparse feature activation tensor.</p> <p>Can also act as a transcoder model, which learns to compress the input activation tensor into a feature activation tensor, and then reconstruct a label activation tensor from the feature activation tensor.</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>def __init__(self, cfg: CrossCoderConfig, device_mesh: Optional[DeviceMesh] = None):\n    super(CrossCoder, self).__init__(cfg, device_mesh)\n    self.cfg = cfg\n\n    # Assertions\n    assert cfg.sparsity_include_decoder_norm, \"Sparsity should include decoder norm in CrossCoder\"\n    assert cfg.use_decoder_bias, \"Decoder bias should be used in CrossCoder\"\n    assert not cfg.use_triton_kernel, \"Triton kernel is not supported in CrossCoder\"\n\n    # Initialize weights and biases\n    if device_mesh is None:\n        self.W_E = nn.Parameter(\n            torch.empty(cfg.n_heads, cfg.d_model, cfg.d_sae, device=cfg.device, dtype=cfg.dtype)\n        )\n        self.b_E = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n        self.W_D = nn.Parameter(\n            torch.empty(cfg.n_heads, cfg.d_sae, cfg.d_model, device=cfg.device, dtype=cfg.dtype)\n        )\n        self.b_D = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, device=cfg.device, dtype=cfg.dtype))\n    else:\n        self.W_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.n_heads,\n                cfg.d_model,\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"W_E\"].placements(device_mesh),\n            )\n        )\n        self.b_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.n_heads,\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"b_E\"].placements(device_mesh),\n            )\n        )\n        self.W_D = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.n_heads,\n                cfg.d_sae,\n                cfg.d_model,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"W_D\"].placements(device_mesh),\n            )\n        )\n        self.b_D = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.n_heads,\n                cfg.d_model,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"b_D\"].placements(device_mesh),\n            )\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.specs","title":"specs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>specs: type[TensorSpecs] = CrossCoderSpecs\n</code></pre> <p>Tensor specs for CrossCoder with n_heads dimension.</p>"},{"location":"reference/models/#lm_saes.CrossCoder.init_parameters","title":"init_parameters","text":"<pre><code>init_parameters(**kwargs) -&gt; None\n</code></pre> <p>Initialize the weights of the model.</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@torch.no_grad()\ndef init_parameters(self, **kwargs) -&gt; None:\n    \"\"\"Initialize the weights of the model.\"\"\"\n    super().init_parameters(**kwargs)\n    # Initialize a single head's weights\n    W_E_per_head = torch.empty(\n        self.cfg.d_model, self.cfg.d_sae, device=self.cfg.device, dtype=self.cfg.dtype\n    ).uniform_(-kwargs[\"encoder_uniform_bound\"], kwargs[\"encoder_uniform_bound\"])\n    W_D_per_head = torch.empty(\n        self.cfg.d_sae, self.cfg.d_model, device=self.cfg.device, dtype=self.cfg.dtype\n    ).uniform_(-kwargs[\"decoder_uniform_bound\"], kwargs[\"decoder_uniform_bound\"])\n\n    # Repeat for all heads\n    if self.device_mesh is None:\n        W_E = einops.repeat(W_E_per_head, \"d_model d_sae -&gt; n_heads d_model d_sae\", n_heads=self.cfg.n_heads)\n        W_D = einops.repeat(W_D_per_head, \"d_sae d_model -&gt; n_heads d_sae d_model\", n_heads=self.cfg.n_heads)\n        b_E = torch.zeros(self.cfg.n_heads, self.cfg.d_sae, device=self.cfg.device, dtype=self.cfg.dtype)\n        b_D = torch.zeros(self.cfg.n_heads, self.cfg.d_model, device=self.cfg.device, dtype=self.cfg.dtype)\n    else:\n        with timer.time(\"init_parameters_distributed\"):\n            W_E_slices = self.dim_maps()[\"W_E\"].local_slices(\n                (self.cfg.n_heads, self.cfg.d_model, self.cfg.d_sae), self.device_mesh\n            )\n            W_D_slices = self.dim_maps()[\"W_D\"].local_slices(\n                (self.cfg.n_heads, self.cfg.d_sae, self.cfg.d_model), self.device_mesh\n            )\n            W_E_head_repeats = get_slice_length(W_E_slices[0], self.cfg.n_heads)\n            W_D_head_repeats = get_slice_length(W_D_slices[0], self.cfg.n_heads)\n            W_E_local = einops.repeat(\n                W_E_per_head[*W_E_slices[1:]], \"d_model d_sae -&gt; n_heads d_model d_sae\", n_heads=W_E_head_repeats\n            )\n            W_D_local = einops.repeat(\n                W_D_per_head[*W_D_slices[1:]], \"d_sae d_model -&gt; n_heads d_sae d_model\", n_heads=W_D_head_repeats\n            )\n            W_E = DTensor.from_local(\n                W_E_local, self.device_mesh, self.dim_maps()[\"W_E\"].placements(self.device_mesh)\n            )\n            W_D = DTensor.from_local(\n                W_D_local, self.device_mesh, self.dim_maps()[\"W_D\"].placements(self.device_mesh)\n            )\n            b_E = torch.distributed.tensor.zeros(\n                self.cfg.n_heads,\n                self.cfg.d_sae,\n                device_mesh=self.device_mesh,\n                placements=self.dim_maps()[\"b_E\"].placements(self.device_mesh),\n                dtype=self.cfg.dtype,\n            )\n            b_D = torch.distributed.tensor.zeros(\n                self.cfg.n_heads,\n                self.cfg.d_model,\n                device_mesh=self.device_mesh,\n                placements=self.dim_maps()[\"b_D\"].placements(self.device_mesh),\n                dtype=self.cfg.dtype,\n            )\n\n    # Assign to parameters\n    self.W_E.copy_(W_E)\n    self.W_D.copy_(W_D)\n    self.b_E.copy_(b_E)\n    self.b_D.copy_(b_D)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.encode","title":"encode","text":"<pre><code>encode(\n    x: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: Literal[False] = False,\n    *,\n    no_einsum: bool = True,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"]\n)\n</code></pre><pre><code>encode(\n    x: Float[Tensor, \"batch n_heads d_model\"]\n    | Float[Tensor, \"batch seq_len n_heads d_model\"],\n    return_hidden_pre: Literal[True],\n    *,\n    no_einsum: bool = True,\n    **kwargs,\n) -&gt; tuple[\n    Float[Tensor, \"batch n_heads d_sae\"]\n    | Float[Tensor, \"batch seq_len n_heads d_sae\"],\n    Float[Tensor, \"batch n_heads d_sae\"]\n    | Float[Tensor, \"batch seq_len n_heads d_sae\"],\n]\n</code></pre> <pre><code>encode(\n    x: Float[Tensor, \"batch n_heads d_model\"]\n    | Float[Tensor, \"batch seq_len n_heads d_model\"],\n    return_hidden_pre: bool = False,\n    *,\n    no_einsum: bool = True,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch n_heads d_sae\"]\n    | Float[Tensor, \"batch seq_len n_heads d_sae\"]\n    | tuple[\n        Float[Tensor, \"batch n_heads d_sae\"]\n        | Float[Tensor, \"batch seq_len n_heads d_sae\"],\n        Float[Tensor, \"batch n_heads d_sae\"]\n        | Float[Tensor, \"batch seq_len n_heads d_sae\"],\n    ]\n)\n</code></pre> <p>Encode the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch n_heads d_model'] | Float[Tensor, 'batch seq_len n_heads d_model']</code> <p>Input tensor of shape (..., n_heads, d_model).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, 'batch n_heads d_sae'] | Float[Tensor, 'batch seq_len n_heads d_sae'] | tuple[Float[Tensor, 'batch n_heads d_sae'] | Float[Tensor, 'batch seq_len n_heads d_sae'], Float[Tensor, 'batch n_heads d_sae'] | Float[Tensor, 'batch seq_len n_heads d_sae']]</code> <p>Encoded tensor of shape (..., n_heads, d_sae).</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@override\n@timer.time(\"encode\")\ndef encode(\n    self,\n    x: Union[\n        Float[torch.Tensor, \"batch n_heads d_model\"],\n        Float[torch.Tensor, \"batch seq_len n_heads d_model\"],\n    ],\n    return_hidden_pre: bool = False,\n    *,\n    no_einsum: bool = True,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch n_heads d_sae\"],\n    Float[torch.Tensor, \"batch seq_len n_heads d_sae\"],\n    tuple[\n        Union[\n            Float[torch.Tensor, \"batch n_heads d_sae\"],\n            Float[torch.Tensor, \"batch seq_len n_heads d_sae\"],\n        ],\n        Union[\n            Float[torch.Tensor, \"batch n_heads d_sae\"],\n            Float[torch.Tensor, \"batch seq_len n_heads d_sae\"],\n        ],\n    ],\n]:\n    \"\"\"Encode the input tensor.\n\n    Args:\n        x: Input tensor of shape (..., n_heads, d_model).\n\n    Returns:\n        Encoded tensor of shape (..., n_heads, d_sae).\n    \"\"\"\n    # Apply encoding per head\n    hidden_pre = self._apply_encoding(x, no_einsum=no_einsum)\n\n    # Sum across heads and add bias\n    if not isinstance(hidden_pre, DTensor):\n        accumulated_hidden_pre = torch.sum(hidden_pre, dim=-2)  # \"... n_heads d_sae -&gt; ... d_sae\"\n    else:\n        accumulated_hidden_pre = cast(\n            DTensor,\n            cast(\n                DTensor,\n                local_map(\n                    lambda x: torch.sum(x, dim=-2, keepdim=True),\n                    list(hidden_pre.placements),\n                )(hidden_pre),\n            ).sum(dim=-2),\n        )  # \"... n_heads d_sae -&gt; ... d_sae\"\n\n        with timer.time(\"encode_redistribute_tensor_pre_repeat\"):\n            accumulated_hidden_pre = DimMap({\"data\": 0, \"model\": -1}).redistribute(accumulated_hidden_pre)\n\n    accumulated_hidden_pre = einops.repeat(\n        accumulated_hidden_pre, \"... d_sae -&gt; ... n_heads d_sae\", n_heads=self.cfg.n_heads\n    )\n\n    with timer.time(\"encode_redistribute_tensor_post_repeat\"):\n        if isinstance(accumulated_hidden_pre, DTensor):\n            accumulated_hidden_pre = DimMap({\"data\": 0, \"head\": -2, \"model\": -1}).redistribute(\n                accumulated_hidden_pre\n            )\n\n    # Apply activation function\n    feature_acts = self.activation_function(accumulated_hidden_pre * self.decoder_norm()) / self.decoder_norm()\n\n    if return_hidden_pre:\n        return feature_acts, accumulated_hidden_pre\n    return feature_acts\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.decode","title":"decode","text":"<pre><code>decode(\n    feature_acts: Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"],\n    *,\n    no_einsum: bool = True,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"]\n)\n</code></pre> <p>Decode the encoded tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Encoded tensor of shape (n_heads, d_sae).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, 'batch d_model'] | Float[Tensor, 'batch seq_len d_model']</code> <p>Decoded tensor of shape (n_heads, d_model).</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@override\n@timer.time(\"decode\")\ndef decode(\n    self,\n    feature_acts: Union[\n        Float[torch.Tensor, \"batch d_sae\"],\n        Float[torch.Tensor, \"batch seq_len d_sae\"],\n    ],\n    *,\n    no_einsum: bool = True,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch d_model\"],\n    Float[torch.Tensor, \"batch seq_len d_model\"],\n]:  # may be overridden by subclasses\n    \"\"\"Decode the encoded tensor.\n\n    Args:\n        x: Encoded tensor of shape (n_heads, d_sae).\n\n    Returns:\n        Decoded tensor of shape (n_heads, d_model).\n    \"\"\"\n    return self._apply_decoding(feature_acts, no_einsum=no_einsum)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.decoder_norm","title":"decoder_norm","text":"<pre><code>decoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Calculate the norm of the decoder weights.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Norm of decoder weights of shape (n_heads, d_sae).</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@override\ndef decoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Calculate the norm of the decoder weights.\n\n    Returns:\n        Norm of decoder weights of shape (n_heads, d_sae).\n    \"\"\"\n    with timer.time(\"decoder_norm_computation\"):\n        if not isinstance(self.W_D, DTensor):\n            return torch.norm(self.W_D, dim=-1, keepdim=keepdim)\n        else:\n            assert self.device_mesh is not None\n            return DTensor.from_local(\n                torch.norm(self.W_D.to_local(), dim=-1, keepdim=keepdim),\n                device_mesh=self.device_mesh,\n                placements=DimMap({\"head\": 0, \"model\": 1}).placements(self.device_mesh),\n            )\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.encoder_norm","title":"encoder_norm","text":"<pre><code>encoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Calculate the norm of the encoder weights.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Norm of encoder weights of shape (n_heads, d_sae).</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@override\n@timer.time(\"encoder_norm\")\ndef encoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Calculate the norm of the encoder weights.\n\n    Returns:\n        Norm of encoder weights of shape (n_heads, d_sae).\n    \"\"\"\n    return torch.norm(self.W_E, dim=-2, keepdim=keepdim)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.standardize_parameters_of_dataset_norm","title":"standardize_parameters_of_dataset_norm","text":"<pre><code>standardize_parameters_of_dataset_norm()\n</code></pre> <p>Standardize the parameters of the model to account for dataset_norm during inference.</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@override\n@timer.time(\"standardize_parameters_of_dataset_norm\")\n@torch.no_grad()\ndef standardize_parameters_of_dataset_norm(self):\n    \"\"\"\n    Standardize the parameters of the model to account for dataset_norm during inference.\n    \"\"\"\n    assert self.cfg.norm_activation == \"dataset-wise\"\n    assert self.dataset_average_activation_norm is not None\n    norm_factors = torch.tensor(\n        [\n            math.sqrt(self.cfg.d_model) / self.dataset_average_activation_norm[hook_point]\n            for hook_point in self.cfg.hook_points\n        ],\n        dtype=self.cfg.dtype,\n        device=self.cfg.device,\n    )\n    self.b_E.div_(norm_factors.view(self.cfg.n_heads, 1))\n    self.b_D.div_(norm_factors.view(self.cfg.n_heads, 1))\n    self.cfg.norm_activation = \"inference\"\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.compute_training_metrics","title":"compute_training_metrics","text":"<pre><code>compute_training_metrics(\n    *,\n    feature_acts: Tensor,\n    l_rec: Tensor,\n    l0: Tensor,\n    explained_variance: Tensor,\n    **kwargs,\n) -&gt; dict[str, float]\n</code></pre> <p>Compute per-head training metrics for CrossCoder.</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>@override\n@torch.no_grad()\ndef compute_training_metrics(\n    self,\n    *,\n    feature_acts: torch.Tensor,\n    l_rec: torch.Tensor,\n    l0: torch.Tensor,\n    explained_variance: torch.Tensor,\n    **kwargs,\n) -&gt; dict[str, float]:\n    \"\"\"Compute per-head training metrics for CrossCoder.\"\"\"\n    assert explained_variance.ndim == 1 and len(explained_variance) == len(self.cfg.hook_points)\n    feature_act_spec = self.specs.feature_acts(feature_acts)\n    l0_spec = tuple(spec for spec in feature_act_spec if spec != \"sae\")\n    l_rec_spec = tuple(\n        spec for spec in feature_act_spec if spec != \"model\" and spec != \"batch\" and spec != \"context\"\n    )\n    metrics = {}\n    for i, k in enumerate(self.cfg.hook_points):\n        metrics.update(\n            {\n                f\"crosscoder_metrics/{k}/explained_variance\": item(explained_variance[i].mean()),\n                f\"crosscoder_metrics/{k}/l0\": item(l0.select(l0_spec.index(\"heads\"), i).mean()),\n                f\"crosscoder_metrics/{k}/l_rec\": item(l_rec.select(l_rec_spec.index(\"heads\"), i).mean()),\n            }\n        )\n    indices = feature_acts.amax(dim=1).nonzero(as_tuple=True)\n    activated_feature_acts = feature_acts.permute(0, 2, 1)[indices].permute(1, 0)\n    activated_decoder_norms = full_tensor(self.decoder_norm())[:, indices[1]]\n    mean_decoder_norm_non_activated_in_activated = item(activated_decoder_norms[activated_feature_acts == 0].mean())\n    mean_decoder_norm_activated_in_activated = item(activated_decoder_norms[activated_feature_acts != 0].mean())\n    metrics.update(\n        {\n            \"crosscoder_metrics/mean_decoder_norm_non_activated_in_activated\": mean_decoder_norm_non_activated_in_activated,\n            \"crosscoder_metrics/mean_decoder_norm_activated_in_activated\": mean_decoder_norm_activated_in_activated,\n        }\n    )\n    return metrics\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossCoder.dim_maps","title":"dim_maps","text":"<pre><code>dim_maps() -&gt; dict[str, DimMap]\n</code></pre> <p>Return a dictionary mapping parameter names to dimension maps.</p> <p>Returns:</p> Type Description <code>dict[str, DimMap]</code> <p>A dictionary mapping parameter names to DimMap objects.</p> Source code in <code>src/lm_saes/crosscoder.py</code> <pre><code>def dim_maps(self) -&gt; dict[str, DimMap]:\n    \"\"\"Return a dictionary mapping parameter names to dimension maps.\n\n    Returns:\n        A dictionary mapping parameter names to DimMap objects.\n    \"\"\"\n    parent_maps = super().dim_maps()\n    crosscoder_maps = {\n        \"W_E\": DimMap({\"head\": 0, \"model\": 2}),\n        \"W_D\": DimMap({\"head\": 0, \"model\": 1}),\n        \"b_E\": DimMap({\"head\": 0, \"model\": 1}),\n        \"b_D\": DimMap({\"head\": 0}),\n    }\n    return parent_maps | crosscoder_maps\n</code></pre>"},{"location":"reference/models/#lm_saes.CLTConfig","title":"CLTConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseSAEConfig</code></p> <p>Configuration for Cross Layer Transcoder (CLT).</p> <p>A CLT consists of L encoders and L(L+1)/2 decoders where each encoder at layer L reads from the residual stream at that layer and can decode to layers L through L-1.</p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>d_model</code>                 (<code>int</code>)             </li> <li> <code>expansion_factor</code>                 (<code>float</code>)             </li> <li> <code>use_decoder_bias</code>                 (<code>bool</code>)             </li> <li> <code>norm_activation</code>                 (<code>Literal['token-wise', 'batch-wise', 'dataset-wise', 'inference']</code>)             </li> <li> <code>sparsity_include_decoder_norm</code>                 (<code>bool</code>)             </li> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>use_triton_kernel</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_triton_spmm_kernel</code>                 (<code>float</code>)             </li> <li> <code>jumprelu_threshold_window</code>                 (<code>float</code>)             </li> <li> <code>sae_type</code>                 (<code>str</code>)             </li> <li> <code>act_fn</code>                 (<code>Literal['relu', 'jumprelu', 'topk', 'batchtopk', 'batchlayertopk', 'layertopk']</code>)             </li> <li> <code>init_cross_layer_decoder_all_zero</code>                 (<code>bool</code>)             </li> <li> <code>hook_points_in</code>                 (<code>list[str]</code>)             </li> <li> <code>hook_points_out</code>                 (<code>list[str]</code>)             </li> <li> <code>decode_with_csr</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_csr</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/models/#lm_saes.CLTConfig.hook_points_in","title":"hook_points_in  <code>pydantic-field</code>","text":"<pre><code>hook_points_in: list[str]\n</code></pre> <p>List of hook points to capture input activations from, one for each layer.</p>"},{"location":"reference/models/#lm_saes.CLTConfig.hook_points_out","title":"hook_points_out  <code>pydantic-field</code>","text":"<pre><code>hook_points_out: list[str]\n</code></pre> <p>List of hook points to capture output activations from, one for each layer.</p>"},{"location":"reference/models/#lm_saes.CLTConfig.decode_with_csr","title":"decode_with_csr  <code>pydantic-field</code>","text":"<pre><code>decode_with_csr: bool = False\n</code></pre> <p>Whether to decode with CSR matrices. If <code>True</code>, will use CSR matrices for decoding. If <code>False</code>, will use dense matrices for decoding.</p>"},{"location":"reference/models/#lm_saes.CLTConfig.sparsity_threshold_for_csr","title":"sparsity_threshold_for_csr  <code>pydantic-field</code>","text":"<pre><code>sparsity_threshold_for_csr: float = 0.05\n</code></pre> <p>The sparsity threshold for the CSR matrices. If the sparsity of the feature activations reaches this threshold, the CSR matrices will be used for decoding. The current conditioning for sparsity is dependent on usage of TopK family of activation functions, so this will not work with other activation functions like <code>relu</code> or <code>jumprelu</code>.</p>"},{"location":"reference/models/#lm_saes.CLTConfig.n_layers","title":"n_layers  <code>property</code>","text":"<pre><code>n_layers: int\n</code></pre> <p>Number of layers in the CLT.</p>"},{"location":"reference/models/#lm_saes.CLTConfig.n_decoders","title":"n_decoders  <code>property</code>","text":"<pre><code>n_decoders: int\n</code></pre> <p>Number of decoders in the CLT.</p>"},{"location":"reference/models/#lm_saes.CLTConfig.associated_hook_points","title":"associated_hook_points  <code>property</code>","text":"<pre><code>associated_hook_points: list[str]\n</code></pre> <p>All hook points used by the CLT.</p>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder","title":"CrossLayerTranscoder","text":"<pre><code>CrossLayerTranscoder(\n    cfg: CLTConfig, device_mesh: DeviceMesh | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractSparseAutoEncoder</code></p> <p>Cross Layer Transcoder (CLT) implementation.</p> <p>A CLT has L encoders (one per layer) and L(L+1)/2 decoders arranged in an upper triangular pattern. Each encoder at layer L reads from the residual stream at that layer, and features can decode to layers L through L-1.</p> <p>We store all parameters in the same object and shard them across GPUs for efficient distributed training.</p> <p>Initialize the Cross Layer Transcoder.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CLTConfig</code> <p>Configuration for the CLT.</p> required <code>device_mesh</code> <code>DeviceMesh | None</code> <p>Device mesh for distributed training.</p> <code>None</code> Source code in <code>src/lm_saes/clt.py</code> <pre><code>def __init__(self, cfg: CLTConfig, device_mesh: Optional[DeviceMesh] = None):\n    \"\"\"Initialize the Cross Layer Transcoder.\n\n    Args:\n        cfg: Configuration for the CLT.\n        device_mesh: Device mesh for distributed training.\n    \"\"\"\n    super().__init__(cfg, device_mesh)\n    self.cfg = cfg\n    # CLT requires specific configuration settings\n    # assert not cfg.sparsity_include_decoder_norm, \"CLT requires sparsity_include_decoder_norm=False\"\n    # assert cfg.use_decoder_bias, \"CLT requires use_decoder_bias=True\"\n\n    # Initialize weights and biases for cross-layer architecture\n    if device_mesh is None:\n        # L encoders: one for each layer\n        self.W_E = nn.Parameter(\n            torch.empty(cfg.n_layers, cfg.d_model, cfg.d_sae, device=cfg.device, dtype=cfg.dtype)\n        )\n        self.b_E = nn.Parameter(torch.empty(cfg.n_layers, cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n\n        # L decoder groups: W_D[i] contains decoders from layers 0..i to layer i\n        self.W_D = nn.ParameterList(\n            [\n                nn.Parameter(data=torch.empty(i + 1, cfg.d_sae, cfg.d_model, device=cfg.device, dtype=cfg.dtype))\n                for i in range(cfg.n_layers)\n            ]\n        )\n\n        # L decoder biases: one bias per target layer\n        self.b_D = nn.ParameterList(\n            [\n                nn.Parameter(torch.empty(cfg.d_model, device=cfg.device, dtype=cfg.dtype))\n                for _ in range(cfg.n_layers)\n            ]\n        )\n    else:\n        # Distributed initialization - shard along feature dimension\n        self.W_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.n_layers,\n                cfg.d_model,\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"W_E\"].placements(device_mesh),\n            )  # shard along d_sae\n        )\n        self.b_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.n_layers,\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=self.dim_maps()[\"b_E\"].placements(device_mesh),\n            )  # shard along d_sae\n        )\n\n        # L decoder groups: W_D[i] contains decoders from layers 0..i to layer i\n        self.W_D = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.distributed.tensor.empty(\n                        i + 1,\n                        cfg.d_sae,\n                        cfg.d_model,\n                        dtype=cfg.dtype,\n                        device_mesh=device_mesh,\n                        placements=self.dim_maps()[\"W_D\"].placements(device_mesh),\n                    )\n                )  # shard along d_sae\n                for i in range(cfg.n_layers)\n            ]\n        )\n\n        self.b_D = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.distributed.tensor.empty(\n                        cfg.d_model,\n                        dtype=cfg.dtype,\n                        device_mesh=device_mesh,\n                        placements=self.dim_maps()[\"b_D\"].placements(device_mesh),\n                    )\n                )\n                for _ in range(cfg.n_layers)\n            ]\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.specs","title":"specs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>specs: type[TensorSpecs] = CrossLayerTranscoderSpecs\n</code></pre> <p>Tensor specs for CrossLayerTranscoder with layer dimension.</p>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.init_parameters","title":"init_parameters","text":"<pre><code>init_parameters(**kwargs)\n</code></pre> <p>Initialize parameters.</p> <p>Encoders: uniformly initialized in range (-1/sqrt(d_sae), 1/sqrt(d_sae)) Decoders at layer L: uniformly initialized in range (-1/sqrt(Ld_model), 1/sqrt(Ld_model))</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\n@torch.no_grad()\ndef init_parameters(self, **kwargs):\n    \"\"\"Initialize parameters.\n\n    Encoders: uniformly initialized in range (-1/sqrt(d_sae), 1/sqrt(d_sae))\n    Decoders at layer L: uniformly initialized in range (-1/sqrt(L*d_model), 1/sqrt(L*d_model))\n    \"\"\"\n    super().init_parameters(**kwargs)  # jump ReLU threshold is initialized in super()\n\n    # Initialize encoder weights and biases\n    encoder_bound = 1.0 / math.sqrt(self.cfg.d_sae)\n\n    if self.device_mesh is None:\n        # Non-distributed initialization\n\n        # Initialize encoder weights: (n_layers, d_model, d_sae)\n        W_E = torch.empty(\n            self.cfg.n_layers, self.cfg.d_model, self.cfg.d_sae, device=self.cfg.device, dtype=self.cfg.dtype\n        ).uniform_(-encoder_bound, encoder_bound)\n\n        # Initialize encoder biases: (n_layers, d_sae) - set to zero\n        nn.init.zeros_(self.b_E)\n\n        # Initialize decoder weights\n        W_D_initialized = []\n        scale = 1.0 / math.sqrt(self.cfg.n_layers * self.cfg.d_model)\n        for layer_to in range(self.cfg.n_layers):\n            # Initialize decoder weights for layer layer_to\n            # W_D[layer_to] has shape (layer_to+1, d_sae, d_model)\n            # Scale by 1/sqrt(L*d_model) where L is the number of contributing layers\n\n            W_D_layer = torch.empty(\n                layer_to + 1, self.cfg.d_sae, self.cfg.d_model, device=self.cfg.device, dtype=self.cfg.dtype\n            )\n            nn.init.uniform_(W_D_layer, -scale, scale)\n            if self.cfg.init_cross_layer_decoder_all_zero:\n                W_D_layer[:-1] = 0\n            W_D_initialized.append(W_D_layer)\n\n        # Initialize decoder biases\n        for layer_to in range(self.cfg.n_layers):\n            # Initialize decoder bias for layer layer_to to zero\n            nn.init.zeros_(self.b_D[layer_to])\n\n    else:\n        # Distributed initialization\n        # Initialize encoder weights\n        W_E_local = torch.empty(\n            self.cfg.n_layers, self.cfg.d_model, self.cfg.d_sae, device=self.cfg.device, dtype=self.cfg.dtype\n        ).uniform_(-encoder_bound, encoder_bound)\n        W_E = self.dim_maps()[\"W_E\"].distribute(W_E_local, self.device_mesh)\n\n        # Initialize encoder biases\n        nn.init.zeros_(self.b_E)\n\n        # Initialize decoder weights for each layer\n        W_D_initialized = []\n        for layer_to in range(self.cfg.n_layers):\n            decoder_bound = 1.0 / math.sqrt(self.cfg.n_layers * self.cfg.d_model)\n            W_D_layer_local = torch.empty(\n                layer_to + 1, self.cfg.d_sae, self.cfg.d_model, device=self.cfg.device, dtype=self.cfg.dtype\n            ).uniform_(-decoder_bound, decoder_bound)\n            if self.cfg.init_cross_layer_decoder_all_zero:\n                W_D_layer_local[:-1] = 0\n            W_D_layer = self.dim_maps()[\"W_D\"].distribute(tensor=W_D_layer_local, device_mesh=self.device_mesh)\n            W_D_initialized.append(W_D_layer)\n\n        # Initialize decoder biases\n        for layer_to in range(self.cfg.n_layers):\n            # Initialize decoder bias for layer layer_to to zero\n            nn.init.zeros_(self.b_D[layer_to])\n\n    # Copy initialized values to parameters\n    self.W_E.copy_(W_E)\n\n    for layer_to, W_D_layer in enumerate(W_D_initialized):\n        self.W_D[layer_to].copy_(W_D_layer)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.get_decoder_weights","title":"get_decoder_weights","text":"<pre><code>get_decoder_weights(layer_to: int) -&gt; Tensor\n</code></pre> <p>Get decoder weights for all layers from 0..layer_to to layer_to.</p> <p>Parameters:</p> Name Type Description Default <code>layer_to</code> <code>int</code> <p>Target layer (0 to n_layers-1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoder weights for all source layers to the specified target layer</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>def get_decoder_weights(self, layer_to: int) -&gt; torch.Tensor:\n    \"\"\"Get decoder weights for all layers from 0..layer_to to layer_to.\n\n    Args:\n        layer_to: Target layer (0 to n_layers-1)\n\n    Returns:\n        Decoder weights for all source layers to the specified target layer\n    \"\"\"\n    return self.W_D[layer_to]\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.encode","title":"encode","text":"<pre><code>encode(\n    x: Float[Tensor, \"batch n_layers d_model\"]\n    | Float[Tensor, \"batch seq_len n_layers d_model\"],\n    return_hidden_pre: Literal[False] = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch n_layers d_sae\"]\n    | Float[Tensor, \"batch seq_len n_layers d_sae\"]\n)\n</code></pre><pre><code>encode(\n    x: Float[Tensor, \"batch n_layers d_model\"]\n    | Float[Tensor, \"batch seq_len n_layers d_model\"],\n    return_hidden_pre: Literal[True],\n    **kwargs,\n) -&gt; tuple[\n    Float[Tensor, \"batch n_layers d_sae\"]\n    | Float[Tensor, \"batch seq_len n_layers d_sae\"],\n    Float[Tensor, \"batch n_layers d_sae\"]\n    | Float[Tensor, \"batch seq_len n_layers d_sae\"],\n]\n</code></pre> <pre><code>encode(\n    x: Float[Tensor, \"batch n_layers d_model\"]\n    | Float[Tensor, \"batch seq_len n_layers d_model\"],\n    return_hidden_pre: bool = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch n_layers d_sae\"]\n    | Float[Tensor, \"batch seq_len n_layers d_sae\"]\n    | tuple[\n        Float[Tensor, \"batch n_layers d_sae\"]\n        | Float[Tensor, \"batch seq_len n_layers d_sae\"],\n        Float[Tensor, \"batch n_layers d_sae\"]\n        | Float[Tensor, \"batch seq_len n_layers d_sae\"],\n    ]\n)\n</code></pre> <p>Encode input activations to CLT features using L encoders.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch n_layers d_model'] | Float[Tensor, 'batch seq_len n_layers d_model']</code> <p>Input activations from all layers (..., n_layers, d_model)</p> required <code>return_hidden_pre</code> <code>bool</code> <p>Whether to return pre-activation values</p> <code>False</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch n_layers d_sae'] | Float[Tensor, 'batch seq_len n_layers d_sae'] | tuple[Float[Tensor, 'batch n_layers d_sae'] | Float[Tensor, 'batch seq_len n_layers d_sae'], Float[Tensor, 'batch n_layers d_sae'] | Float[Tensor, 'batch seq_len n_layers d_sae']]</code> <p>Feature activations for all layers (..., n_layers, d_sae)</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef encode(\n    self,\n    x: Union[\n        Float[torch.Tensor, \"batch n_layers d_model\"],\n        Float[torch.Tensor, \"batch seq_len n_layers d_model\"],\n    ],\n    return_hidden_pre: bool = False,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch n_layers d_sae\"],\n    Float[torch.Tensor, \"batch seq_len n_layers d_sae\"],\n    tuple[\n        Union[\n            Float[torch.Tensor, \"batch n_layers d_sae\"],\n            Float[torch.Tensor, \"batch seq_len n_layers d_sae\"],\n        ],\n        Union[\n            Float[torch.Tensor, \"batch n_layers d_sae\"],\n            Float[torch.Tensor, \"batch seq_len n_layers d_sae\"],\n        ],\n    ],\n]:\n    \"\"\"Encode input activations to CLT features using L encoders.\n\n    Args:\n        x: Input activations from all layers (..., n_layers, d_model)\n        return_hidden_pre: Whether to return pre-activation values\n\n    Returns:\n        Feature activations for all layers (..., n_layers, d_sae)\n    \"\"\"\n    with timer.time(\"encoder_matmul\"):\n        hidden_pre = torch.einsum(\"...ld,lds-&gt;...ls\", x, self.W_E) + self.b_E\n\n    if self.cfg.sparsity_include_decoder_norm:\n        hidden_pre = hidden_pre * self.decoder_norm_per_feature()\n\n    # Apply activation function (ReLU, TopK, etc.)\n    with timer.time(\"activation_function\"):\n        feature_acts = self.activation_function(hidden_pre)\n\n    if self.cfg.sparsity_include_decoder_norm:\n        feature_acts = feature_acts / self.decoder_norm_per_feature()\n\n    if return_hidden_pre:\n        return feature_acts, hidden_pre\n    return feature_acts\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.encode_single_layer","title":"encode_single_layer","text":"<pre><code>encode_single_layer(\n    x: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"],\n    layer: int,\n    return_hidden_pre: bool = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"]\n    | tuple[\n        Float[Tensor, \"batch d_sae\"]\n        | Float[Tensor, \"batch seq_len d_sae\"],\n        Float[Tensor, \"batch d_sae\"]\n        | Float[Tensor, \"batch seq_len d_sae\"],\n    ]\n)\n</code></pre> <p>Encode input activations to CLT features using L encoders.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch d_model'] | Float[Tensor, 'batch seq_len d_model']</code> <p>Input activations from a given layer (..., d_model)</p> required <code>layer</code> <code>int</code> <p>The layer to encode</p> required <code>return_hidden_pre</code> <code>bool</code> <p>Whether to return pre-activation values</p> <code>False</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae'] | tuple[Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae'], Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae']]</code> <p>Feature activations for the given layer (..., d_sae)</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>def encode_single_layer(\n    self,\n    x: Union[\n        Float[torch.Tensor, \"batch d_model\"],\n        Float[torch.Tensor, \"batch seq_len d_model\"],\n    ],\n    layer: int,\n    return_hidden_pre: bool = False,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch d_sae\"],\n    Float[torch.Tensor, \"batch seq_len d_sae\"],\n    tuple[\n        Union[\n            Float[torch.Tensor, \"batch d_sae\"],\n            Float[torch.Tensor, \"batch seq_len d_sae\"],\n        ],\n        Union[\n            Float[torch.Tensor, \"batch d_sae\"],\n            Float[torch.Tensor, \"batch seq_len d_sae\"],\n        ],\n    ],\n]:\n    \"\"\"Encode input activations to CLT features using L encoders.\n\n    Args:\n        x: Input activations from a given layer (..., d_model)\n        layer: The layer to encode\n        return_hidden_pre: Whether to return pre-activation values\n\n    Returns:\n        Feature activations for the given layer (..., d_sae)\n    \"\"\"\n    # Apply each encoder to its corresponding layer: x[..., layer, :] @ W_E[layer] + b_E[layer]\n    hidden_pre = torch.einsum(\"...d,ds-&gt;...s\", x, self.W_E[layer]) + self.b_E[layer]\n\n    # print(f'{x.shape=} {self.W_E[layer].shape=} {self.b_E[layer].shape=}')\n\n    if self.cfg.sparsity_include_decoder_norm:\n        # print(f'{hidden_pre.shape=} {self.decoder_norm_per_feature(layer=layer).shape=}')\n        hidden_pre = hidden_pre * self.decoder_norm_per_feature(layer=layer)\n\n    # Apply activation function (ReLU, TopK, etc.)\n    if self.cfg.act_fn.lower() == \"jumprelu\":\n        assert isinstance(self.activation_function, JumpReLU)\n        jumprelu_threshold = self.activation_function.get_jumprelu_threshold()\n        feature_acts = hidden_pre * hidden_pre.gt(jumprelu_threshold[layer])\n    else:\n        feature_acts = self.activation_function(hidden_pre)\n\n    if return_hidden_pre:\n        return feature_acts, hidden_pre\n    return feature_acts\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.decode","title":"decode","text":"<pre><code>decode(\n    feature_acts: Float[Tensor, \"batch n_layers d_sae\"]\n    | Float[Tensor, \"batch seq_len n_layers d_sae\"]\n    | list[Float[Tensor, \"seq_len d_sae\"]],\n    batch_first: bool = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"n_layers batch d_model\"]\n    | Float[Tensor, \"n_layers batch seq_len d_model\"]\n    | Float[Tensor, \"batch n_layers d_model\"]\n    | Float[Tensor, \"batch seq_len n_layers d_model\"]\n)\n</code></pre> <p>Decode CLT features to output activations using the upper triangular pattern.</p> <p>The output at layer L is the sum of contributions from all layers 0 through L: y_L = \u03a3_{i=0}^{L} W_D[i\u2192L] @ feature_acts[..., i, :] + b_D[L]</p> <p>Parameters:</p> Name Type Description Default <code>feature_acts</code> <code>Float[Tensor, 'batch n_layers d_sae'] | Float[Tensor, 'batch seq_len n_layers d_sae'] | list[Float[Tensor, 'seq_len d_sae']]</code> <p>CLT feature activations (..., n_layers, d_sae)</p> required <p>Returns:</p> Type Description <code>Float[Tensor, 'n_layers batch d_model'] | Float[Tensor, 'n_layers batch seq_len d_model'] | Float[Tensor, 'batch n_layers d_model'] | Float[Tensor, 'batch seq_len n_layers d_model']</code> <p>Reconstructed activations for all layers (..., n_layers, d_model)</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef decode(\n    self,\n    feature_acts: Union[\n        Float[torch.Tensor, \"batch n_layers d_sae\"],\n        Float[torch.Tensor, \"batch seq_len n_layers d_sae\"],\n        List[Float[torch.sparse.Tensor, \"seq_len d_sae\"]],\n    ],\n    batch_first: bool = False,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"n_layers batch d_model\"],\n    Float[torch.Tensor, \"n_layers batch seq_len d_model\"],\n    Float[torch.Tensor, \"batch n_layers d_model\"],\n    Float[torch.Tensor, \"batch seq_len n_layers d_model\"],\n]:\n    \"\"\"Decode CLT features to output activations using the upper triangular pattern.\n\n    The output at layer L is the sum of contributions from all layers 0 through L:\n    y_L = \u03a3_{i=0}^{L} W_D[i\u2192L] @ feature_acts[..., i, :] + b_D[L]\n\n    Args:\n        feature_acts: CLT feature activations (..., n_layers, d_sae)\n\n    Returns:\n        Reconstructed activations for all layers (..., n_layers, d_model)\n    \"\"\"\n    # TODO: make this cleaner\n\n    reconstructed = []\n    # For each output layer L\n    if (\n        isinstance(feature_acts, list)\n        and isinstance(feature_acts[0], torch.Tensor)\n        and feature_acts[0].layout == torch.sparse_coo\n    ):\n        decode_single_output_layer = self._decode_single_output_layer_coo\n    elif self.cfg.decode_with_csr:\n        if self.current_k / (self.cfg.d_sae * self.cfg.n_layers) &lt; self.cfg.sparsity_threshold_for_csr:\n            decode_single_output_layer = self._decode_single_output_layer_csr\n            assert not isinstance(feature_acts, list), (\n                \"feature_acts must not be a list when decode_with_csr is True\"\n            )\n            if isinstance(feature_acts, DTensor):\n                feature_acts = feature_acts.to_local()\n            if feature_acts.layout != torch.sparse_csr:\n                feature_acts = [fa.to_sparse_csr() for fa in feature_acts.permute(1, 0, 2)]\n        else:\n            decode_single_output_layer = self._decode_single_output_layer_dense\n    else:\n        decode_single_output_layer = self._decode_single_output_layer_dense\n\n    for layer_to in range(self.cfg.n_layers):\n        # we only compute W_D @ feature_acts here, without b_D\n        contribution = decode_single_output_layer(feature_acts, layer_to)  # type: ignore\n\n        # Add bias contribution (single bias vector for this target layer)\n        contribution = contribution + self.b_D[layer_to]  # (d_model,)\n        if isinstance(contribution, DTensor):\n            contribution = DimMap({\"data\": 0}).redistribute(contribution)\n\n        reconstructed.append(contribution)\n\n    return torch.stack(reconstructed, dim=1 if batch_first else 0)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.decoder_norm","title":"decoder_norm","text":"<pre><code>decoder_norm(keepdim: bool = False)\n</code></pre> <p>Compute the effective norm of decoder weights for each feature.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef decoder_norm(self, keepdim: bool = False):\n    \"\"\"Compute the effective norm of decoder weights for each feature.\"\"\"\n    # Collect norms from all decoder groups\n    return torch.ones(self.cfg.n_decoders, device=self.cfg.device, dtype=self.cfg.dtype)\n    return torch.ones(self.cfg.n_decoders, device=self.cfg.device, dtype=self.cfg.dtype)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.encoder_norm","title":"encoder_norm","text":"<pre><code>encoder_norm(keepdim: bool = False)\n</code></pre> <p>Compute the norm of encoder weights averaged across layers.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef encoder_norm(self, keepdim: bool = False):\n    \"\"\"Compute the norm of encoder weights averaged across layers.\"\"\"\n    if not isinstance(self.W_E, DTensor):\n        return torch.norm(self.W_E, p=2, dim=1, keepdim=keepdim).to(self.cfg.device)\n    else:\n        assert self.device_mesh is not None\n        return DTensor.from_local(\n            torch.norm(self.W_E.to_local(), p=2, dim=1, keepdim=keepdim),\n            device_mesh=self.device_mesh,\n            placements=DimMap({\"model\": 1 if keepdim else 0}).placements(self.device_mesh),\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.decoder_bias_norm","title":"decoder_bias_norm","text":"<pre><code>decoder_bias_norm()\n</code></pre> <p>Compute the norm of decoder bias for each target layer.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef decoder_bias_norm(self):\n    \"\"\"Compute the norm of decoder bias for each target layer.\"\"\"\n    return torch.ones(self.cfg.n_layers, device=self.cfg.device, dtype=self.cfg.dtype)\n    return torch.ones(self.cfg.n_layers, device=self.cfg.device, dtype=self.cfg.dtype)\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.set_encoder_to_fixed_norm","title":"set_encoder_to_fixed_norm","text":"<pre><code>set_encoder_to_fixed_norm(value: float)\n</code></pre> <p>Set encoder weights to fixed norm.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\n@torch.no_grad()\ndef set_encoder_to_fixed_norm(self, value: float):\n    \"\"\"Set encoder weights to fixed norm.\"\"\"\n    raise NotImplementedError(\"set_encoder_to_fixed_norm does not make sense for CLT\")\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.keep_only_decoders_for_layer_from","title":"keep_only_decoders_for_layer_from","text":"<pre><code>keep_only_decoders_for_layer_from(layer_from: int)\n</code></pre> <p>Keep only the decoder norm for the given layer.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@torch.no_grad()\ndef keep_only_decoders_for_layer_from(self, layer_from: int):\n    \"\"\"Keep only the decoder norm for the given layer.\"\"\"\n    new_W_D = []\n    for layer_to, decoder_weights in enumerate(self.W_D):\n        if layer_to &gt;= layer_from:\n            new_W_D.append(decoder_weights[layer_from])\n    self.decoders_for_layer_from = (layer_from, new_W_D)\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.decoder_norm_per_feature","title":"decoder_norm_per_feature","text":"<pre><code>decoder_norm_per_feature(\n    layer: int | None = None,\n) -&gt; Float[Tensor, \"n_layers d_sae\"]\n</code></pre> <p>Compute the norm of decoder weights for each feature. If layer is not None, only compute the norm for the decoder weights from layer to subsequent layers.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@torch.no_grad()\ndef decoder_norm_per_feature(\n    self,\n    layer: int | None = None,\n) -&gt; Float[torch.Tensor, \"n_layers d_sae\"]:\n    \"\"\"\n    Compute the norm of decoder weights for each feature.\n    If layer is not None, only compute the norm for the decoder weights from layer to subsequent layers.\n    \"\"\"\n\n    if self.device_mesh is None:\n        decoder_norms = torch.zeros(\n            self.cfg.n_layers,\n            self.cfg.d_sae,\n            dtype=self.cfg.dtype,\n            device=self.cfg.device,\n        )\n    else:\n        decoder_norms = torch.distributed.tensor.zeros(\n            self.cfg.n_layers,\n            self.cfg.d_sae,\n            dtype=self.cfg.dtype,\n            device_mesh=self.device_mesh,\n            placements=self.dim_maps()[\"decoder_norms\"].placements(self.device_mesh),\n        )\n    if layer is not None:\n        if getattr(self, \"decoders_for_layer_from\", None) is not None:\n            kept_layer_from, kept_decoders = getattr(self, \"decoders_for_layer_from\")\n            assert kept_layer_from == layer\n            for layer_to, decoder_weights in enumerate(kept_decoders):\n                layer_to += layer\n                decoder_norms[layer_to] = decoder_weights.pow(2).sum(dim=-1).sqrt()\n        else:\n            for layer_to, decoder_weights in enumerate(self.W_D[layer:]):\n                layer_to += layer\n                decoder_norms[layer_to] = decoder_weights[layer].pow(2).sum(dim=-1).sqrt()\n    else:\n        for layer_to, decoder_weights in enumerate(self.W_D):\n            decoder_norms[: layer_to + 1] = decoder_norms[: layer_to + 1] + decoder_weights.pow(2).sum(dim=-1)\n        decoder_norms = decoder_norms.sqrt()\n    return decoder_norms\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.decoder_norm_per_decoder","title":"decoder_norm_per_decoder","text":"<pre><code>decoder_norm_per_decoder() -&gt; (\n    Float[Tensor, n_decoders] | DTensor\n)\n</code></pre> <p>Compute the L2 norm of decoder weights for each decoder (layer_from -&gt; layer_to). Returns:     norms: torch.Tensor or DTensor of shape (n_decoders,), where n_decoders = n_layers * (n_layers + 1) // 2</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>def decoder_norm_per_decoder(self) -&gt; Union[Float[torch.Tensor, \"n_decoders\"], DTensor]:  # noqa: F821\n    \"\"\"Compute the L2 norm of decoder weights for each decoder (layer_from -&gt; layer_to).\n    Returns:\n        norms: torch.Tensor or DTensor of shape (n_decoders,), where n_decoders = n_layers * (n_layers + 1) // 2\n    \"\"\"\n    n_decoders: int = self.cfg.n_layers * (self.cfg.n_layers + 1) // 2\n    if self.device_mesh is None:\n        decoder_norms = torch.zeros(\n            n_decoders,\n            self.cfg.d_sae,\n            dtype=self.cfg.dtype,\n            device=self.cfg.device,\n        )\n    else:\n        decoder_norms = torch.distributed.tensor.zeros(\n            n_decoders,\n            self.cfg.d_sae,\n            dtype=self.cfg.dtype,\n            device_mesh=self.device_mesh,\n            placements=self.dim_maps()[\"decoder_norms\"].placements(self.device_mesh),\n        )\n    idx = 0\n    for layer_to, decoder_weights in enumerate(self.W_D):\n        for layer_from in range(layer_to + 1):\n            decoder_norms[idx] = decoder_weights[layer_from].pow(2).sum(dim=-1)\n            idx += 1\n    decoder_norms = decoder_norms.sqrt().mean(dim=-1)\n    return decoder_norms\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.standardize_parameters_of_dataset_norm","title":"standardize_parameters_of_dataset_norm","text":"<pre><code>standardize_parameters_of_dataset_norm()\n</code></pre> <p>Standardize parameters for dataset-wise normalization during inference.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\n@torch.no_grad()\ndef standardize_parameters_of_dataset_norm(self):\n    \"\"\"Standardize parameters for dataset-wise normalization during inference.\"\"\"\n    assert self.cfg.norm_activation == \"dataset-wise\"\n    assert self.dataset_average_activation_norm is not None\n    dataset_average_activation_norm = self.dataset_average_activation_norm\n\n    def input_norm_factor(layer: int) -&gt; float:\n        return math.sqrt(self.cfg.d_model) / dataset_average_activation_norm[self.cfg.hook_points_in[layer]]\n\n    def output_norm_factor(layer: int) -&gt; float:\n        return math.sqrt(self.cfg.d_model) / dataset_average_activation_norm[self.cfg.hook_points_out[layer]]\n\n    # For CLT, we need to handle multiple input and output layers\n    for layer_from in range(self.cfg.n_layers):\n        # Adjust encoder bias for this layer\n        self.b_E.data[layer_from].div_(input_norm_factor(layer_from))\n\n        if self.cfg.act_fn.lower() == \"jumprelu\":\n            assert isinstance(self.activation_function, JumpReLU)\n            threshold = self.activation_function.log_jumprelu_threshold.data[layer_from].exp()\n            threshold = threshold / input_norm_factor(layer_from)\n            self.activation_function.log_jumprelu_threshold.data[layer_from] = torch.log(threshold)\n\n    for layer_to in range(self.cfg.n_layers):\n        self.b_D[layer_to].data.div_(output_norm_factor(layer_to))\n        for layer_from in range(layer_to + 1):\n            self.W_D[layer_to].data[layer_from].mul_(input_norm_factor(layer_from) / output_norm_factor(layer_to))\n\n    self.cfg.norm_activation = \"inference\"\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.prepare_input","title":"prepare_input","text":"<pre><code>prepare_input(\n    batch: dict[str, Tensor], **kwargs\n) -&gt; tuple[Tensor, dict[str, Any], dict[str, Any]]\n</code></pre> <p>Prepare input tensor from batch by stacking all layer activations from hook_points_in.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef prepare_input(\n    self, batch: \"dict[str, torch.Tensor]\", **kwargs\n) -&gt; \"tuple[torch.Tensor, dict[str, Any], dict[str, Any]]\":\n    \"\"\"Prepare input tensor from batch by stacking all layer activations from hook_points_in.\"\"\"\n    x_layers = []\n    for hook_point in self.cfg.hook_points_in:\n        if hook_point not in batch:\n            raise ValueError(f\"Missing hook point {hook_point} in batch\")\n        x_layers.append(batch[hook_point])\n    # it is a bug of DTensor, ideally, we should stack along dim=-2,but it will cause an error on shard dim.\n    x = torch.stack(x_layers, dim=x_layers[0].ndim - 1)  # (..., n_layers, d_model)\n\n    encoder_kwargs = {}\n    decoder_kwargs = {}\n    return x, encoder_kwargs, decoder_kwargs\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.prepare_input_single_layer","title":"prepare_input_single_layer","text":"<pre><code>prepare_input_single_layer(\n    batch: dict[str, Tensor], layer: int, **kwargs\n) -&gt; tuple[Tensor, dict[str, Any], dict[str, Any]]\n</code></pre> <p>Prepare input tensor from batch by stacking all layer activations from hook_points_in.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>def prepare_input_single_layer(\n    self, batch: \"dict[str, torch.Tensor]\", layer: int, **kwargs\n) -&gt; \"tuple[torch.Tensor, dict[str, Any], dict[str, Any]]\":\n    \"\"\"Prepare input tensor from batch by stacking all layer activations from hook_points_in.\"\"\"\n    hook_point_in = self.cfg.hook_points_in[layer]\n    if hook_point_in not in batch:\n        raise ValueError(f\"Missing hook point {hook_point_in} in batch\")\n    x = batch[hook_point_in]\n    return x, {}, {}\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.prepare_label","title":"prepare_label","text":"<pre><code>prepare_label(batch: dict[str, Tensor], **kwargs) -&gt; Tensor\n</code></pre> <p>Prepare label tensor from batch using hook_points_out.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\ndef prepare_label(self, batch: \"dict[str, torch.Tensor]\", **kwargs) -&gt; torch.Tensor:\n    \"\"\"Prepare label tensor from batch using hook_points_out.\"\"\"\n    x_layers = []\n    for hook_point in self.cfg.hook_points_out:\n        if hook_point not in batch:\n            raise ValueError(f\"Missing hook point {hook_point} in batch\")\n        x_layers.append(batch[hook_point])\n    labels = torch.stack(x_layers, dim=0)  # (n_layers, ..., d_model)\n    return labels\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.compute_training_metrics","title":"compute_training_metrics","text":"<pre><code>compute_training_metrics(\n    *,\n    l0: Tensor,\n    explained_variance_legacy: Tensor,\n    **kwargs,\n) -&gt; dict[str, float]\n</code></pre> <p>Compute per-layer training metrics for CLT.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@override\n@torch.no_grad()\ndef compute_training_metrics(\n    self,\n    *,\n    l0: torch.Tensor,\n    explained_variance_legacy: torch.Tensor,\n    **kwargs,\n) -&gt; dict[str, float]:\n    \"\"\"Compute per-layer training metrics for CLT.\"\"\"\n    assert explained_variance_legacy.ndim == 1 and len(explained_variance_legacy) == self.cfg.n_layers, (\n        f\"explained_variance_legacy should be of shape (n_layers,), but got {explained_variance_legacy.shape}\"\n    )\n    clt_per_layer_ev_dict = {\n        f\"metrics/explained_variance_L{l}\": item(explained_variance_legacy[l].mean())\n        for l in range(explained_variance_legacy.size(1))\n    }\n    clt_per_layer_l0_dict = {f\"metrics/l0_layer{l}\": item(l0[:, l].mean()) for l in range(l0.size(1))}\n    return {**clt_per_layer_ev_dict, **clt_per_layer_l0_dict}\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    batch: dict[str, Tensor],\n    *,\n    sparsity_loss_type: Literal[\n        \"power\", \"tanh\", \"tanh-quad\", None\n    ] = None,\n    tanh_stretch_coefficient: float = 4.0,\n    p: int = 1,\n    l1_coefficient: float = 1.0,\n    return_aux_data: Literal[True] = True,\n    **kwargs,\n) -&gt; dict[str, Any]\n</code></pre><pre><code>compute_loss(\n    batch: dict[str, Tensor],\n    *,\n    sparsity_loss_type: Literal[\n        \"power\", \"tanh\", \"tanh-quad\", None\n    ] = None,\n    tanh_stretch_coefficient: float = 4.0,\n    p: int = 1,\n    l1_coefficient: float = 1.0,\n    return_aux_data: Literal[False],\n    **kwargs,\n) -&gt; Float[Tensor, \" batch\"]\n</code></pre> <pre><code>compute_loss(\n    batch: dict[str, Tensor],\n    label: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"]\n    | None = None,\n    *,\n    sparsity_loss_type: Literal[\n        \"power\", \"tanh\", \"tanh-quad\", None\n    ] = None,\n    tanh_stretch_coefficient: float = 4.0,\n    frequency_scale: float = 0.01,\n    p: int = 1,\n    l1_coefficient: float = 1.0,\n    return_aux_data: bool = True,\n    **kwargs,\n) -&gt; Float[Tensor, \" batch\"] | dict[str, Any]\n</code></pre> <p>Compute the loss for the autoencoder. Ensure that the input activations are normalized by calling <code>normalize_activations</code> before calling this method.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>@timer.time(\"compute_loss\")\ndef compute_loss(\n    self,\n    batch: dict[str, torch.Tensor],\n    label: (\n        Optional[\n            Union[\n                Float[torch.Tensor, \"batch d_model\"],\n                Float[torch.Tensor, \"batch seq_len d_model\"],\n            ]\n        ]\n    ) = None,\n    *,\n    sparsity_loss_type: Literal[\"power\", \"tanh\", \"tanh-quad\", None] = None,\n    tanh_stretch_coefficient: float = 4.0,\n    frequency_scale: float = 0.01,\n    p: int = 1,\n    l1_coefficient: float = 1.0,\n    return_aux_data: bool = True,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \" batch\"],\n    dict[str, Any],\n]:\n    \"\"\"Compute the loss for the autoencoder.\n    Ensure that the input activations are normalized by calling `normalize_activations` before calling this method.\n    \"\"\"\n    x, encoder_kwargs, decoder_kwargs = self.prepare_input(batch)\n    label = self.prepare_label(batch, **kwargs)\n\n    with timer.time(\"encode\"):\n        feature_acts = self.encode(x, **encoder_kwargs)\n\n    with timer.time(\"decode\"):\n        reconstructed = self.decode(feature_acts, **decoder_kwargs)\n\n    with timer.time(\"loss_calculation\"):\n        l_rec = (reconstructed - label).pow(2)\n        l_rec = l_rec.sum(dim=-1).mean()\n        if isinstance(l_rec, DTensor):\n            l_rec: Tensor = l_rec.full_tensor()\n        loss_dict: dict[str, Optional[torch.Tensor]] = {\n            \"l_rec\": l_rec,\n        }\n        loss = l_rec\n\n        if sparsity_loss_type is not None:\n            decoder_norm: Union[Float[torch.Tensor, \"n_layers d_sae\"], DTensor] = self.decoder_norm_per_feature()\n            with timer.time(\"sparsity_loss_calculation\"):\n                if sparsity_loss_type == \"power\":\n                    l_s = torch.norm(feature_acts * decoder_norm, p=p, dim=-1)\n                elif sparsity_loss_type == \"tanh\":\n                    l_s = torch.tanh(tanh_stretch_coefficient * feature_acts * decoder_norm).sum(dim=-1)\n                elif sparsity_loss_type == \"tanh-quad\":\n                    approx_frequency = einops.reduce(\n                        torch.tanh(tanh_stretch_coefficient * feature_acts * decoder_norm),\n                        \"... d_sae -&gt; d_sae\",\n                        \"mean\",\n                    )\n                    l_s = (approx_frequency * (1 + approx_frequency / frequency_scale)).sum(dim=-1)\n                else:\n                    raise ValueError(f\"sparsity_loss_type f{sparsity_loss_type} not supported.\")\n                if isinstance(l_s, DTensor):\n                    l_s = l_s.full_tensor()\n                l_s = l1_coefficient * l_s\n                # WARNING: Some DTensor bugs make if l1_coefficient * l_s goes before full_tensor, the l1_coefficient value will be internally cached. Furthermore, it will cause the backward pass to fail with redistribution error. See https://github.com/pytorch/pytorch/issues/153603 and https://github.com/pytorch/pytorch/issues/153615 .\n                loss_dict[\"l_s\"] = l_s\n                loss = loss + l_s.mean()\n        else:\n            loss_dict[\"l_s\"] = None\n\n    if return_aux_data:\n        return {\n            \"loss\": loss,\n            **loss_dict,\n            \"label\": label,\n            \"mask\": batch.get(\"mask\"),\n            \"n_tokens\": batch[\"tokens\"].numel() if batch.get(\"mask\") is None else int(item(batch[\"mask\"].sum())),\n            \"feature_acts\": feature_acts,\n            \"reconstructed\": reconstructed,\n        }\n    return loss\n</code></pre>"},{"location":"reference/models/#lm_saes.CrossLayerTranscoder.dim_maps","title":"dim_maps","text":"<pre><code>dim_maps() -&gt; dict[str, DimMap]\n</code></pre> <p>Return dimension maps for distributed training along feature dimension.</p> Source code in <code>src/lm_saes/clt.py</code> <pre><code>def dim_maps(self) -&gt; \"dict[str, DimMap]\":\n    \"\"\"Return dimension maps for distributed training along feature dimension.\"\"\"\n    base_maps = super().dim_maps()\n\n    clt_maps = {\n        \"W_E\": DimMap({\"model\": 2}),  # Shard along d_sae dimension\n        \"b_E\": DimMap({\"model\": 1}),  # Shard along d_sae dimension\n        \"W_D\": DimMap({\"model\": 1}),  # Shard along d_sae dimension\n        \"b_D\": DimMap({}),  # Replicate decoder biases\n        \"decoder_norms\": DimMap({\"model\": 1}),  # Shard along d_sae dimension\n    }\n\n    return base_maps | clt_maps\n</code></pre>"},{"location":"reference/models/#lm_saes.LorsaConfig","title":"LorsaConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseSAEConfig</code></p> <p>Configuration for Low Rank Sparse Attention.</p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>d_model</code>                 (<code>int</code>)             </li> <li> <code>expansion_factor</code>                 (<code>float</code>)             </li> <li> <code>use_decoder_bias</code>                 (<code>bool</code>)             </li> <li> <code>act_fn</code>                 (<code>Literal['relu', 'jumprelu', 'topk', 'batchtopk', 'batchlayertopk', 'layertopk']</code>)             </li> <li> <code>norm_activation</code>                 (<code>Literal['token-wise', 'batch-wise', 'dataset-wise', 'inference']</code>)             </li> <li> <code>sparsity_include_decoder_norm</code>                 (<code>bool</code>)             </li> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>use_triton_kernel</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_triton_spmm_kernel</code>                 (<code>float</code>)             </li> <li> <code>jumprelu_threshold_window</code>                 (<code>float</code>)             </li> <li> <code>sae_type</code>                 (<code>str</code>)             </li> <li> <code>hook_point_in</code>                 (<code>str</code>)             </li> <li> <code>hook_point_out</code>                 (<code>str</code>)             </li> <li> <code>n_qk_heads</code>                 (<code>int</code>)             </li> <li> <code>d_qk_head</code>                 (<code>int</code>)             </li> <li> <code>positional_embedding_type</code>                 (<code>Literal['rotary', 'none']</code>)             </li> <li> <code>rotary_dim</code>                 (<code>int</code>)             </li> <li> <code>rotary_base</code>                 (<code>int</code>)             </li> <li> <code>rotary_adjacent_pairs</code>                 (<code>bool</code>)             </li> <li> <code>rotary_scale</code>                 (<code>int</code>)             </li> <li> <code>use_NTK_by_parts_rope</code>                 (<code>bool</code>)             </li> <li> <code>NTK_by_parts_factor</code>                 (<code>float</code>)             </li> <li> <code>NTK_by_parts_low_freq_factor</code>                 (<code>float</code>)             </li> <li> <code>NTK_by_parts_high_freq_factor</code>                 (<code>float</code>)             </li> <li> <code>old_context_len</code>                 (<code>int</code>)             </li> <li> <code>n_ctx</code>                 (<code>int</code>)             </li> <li> <code>attn_scale</code>                 (<code>float | None</code>)             </li> <li> <code>use_post_qk_ln</code>                 (<code>bool</code>)             </li> <li> <code>normalization_type</code>                 (<code>Literal['LN', 'RMS'] | None</code>)             </li> <li> <code>eps</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/models/#lm_saes.LorsaConfig.associated_hook_points","title":"associated_hook_points  <code>property</code>","text":"<pre><code>associated_hook_points: list[str]\n</code></pre> <p>All hook points used by Lorsa.</p>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention","title":"LowRankSparseAttention","text":"<pre><code>LowRankSparseAttention(\n    cfg: LorsaConfig, device_mesh: DeviceMesh | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractSparseAutoEncoder</code></p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>def __init__(self, cfg: LorsaConfig, device_mesh: Optional[DeviceMesh] = None):\n    super().__init__(cfg, device_mesh=device_mesh)\n    self.cfg = cfg\n\n    if device_mesh is None:\n        # Local parameters\n        def _get_param_with_shape(shape: tuple[int, ...]) -&gt; nn.Parameter:\n            return nn.Parameter(\n                torch.empty(\n                    shape,\n                    dtype=self.cfg.dtype,\n                    device=self.cfg.device,\n                )\n            )\n\n        self.W_Q = _get_param_with_shape((self.cfg.n_qk_heads, self.cfg.d_model, self.cfg.d_qk_head))\n        self.W_K = _get_param_with_shape((self.cfg.n_qk_heads, self.cfg.d_model, self.cfg.d_qk_head))\n        self.W_V = _get_param_with_shape((self.cfg.n_ov_heads, self.cfg.d_model))\n        self.W_O = _get_param_with_shape((self.cfg.n_ov_heads, self.cfg.d_model))\n        self.b_Q = _get_param_with_shape((self.cfg.n_qk_heads, self.cfg.d_qk_head))\n        self.b_K = _get_param_with_shape((self.cfg.n_qk_heads, self.cfg.d_qk_head))\n        self.b_V = _get_param_with_shape((self.cfg.n_ov_heads,))\n        if self.cfg.use_decoder_bias:\n            self.b_D = _get_param_with_shape((self.cfg.d_model,))\n    else:\n        # Distributed parameters with head sharding\n        dim_maps = self.dim_maps()\n\n        def _get_param_with_shape(shape: tuple[int, ...], placements: Sequence[Any]) -&gt; nn.Parameter:\n            return nn.Parameter(\n                torch.distributed.tensor.empty(\n                    shape,\n                    dtype=self.cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=placements,\n                )\n            )\n\n        self.W_Q = _get_param_with_shape(\n            (self.cfg.n_qk_heads, self.cfg.d_model, self.cfg.d_qk_head),\n            placements=dim_maps[\"W_Q\"].placements(device_mesh),\n        )\n        self.W_K = _get_param_with_shape(\n            (self.cfg.n_qk_heads, self.cfg.d_model, self.cfg.d_qk_head),\n            placements=dim_maps[\"W_K\"].placements(device_mesh),\n        )\n        self.W_V = _get_param_with_shape(\n            (self.cfg.n_ov_heads, self.cfg.d_model), placements=dim_maps[\"W_V\"].placements(device_mesh)\n        )\n        self.W_O = _get_param_with_shape(\n            (self.cfg.n_ov_heads, self.cfg.d_model), placements=dim_maps[\"W_O\"].placements(device_mesh)\n        )\n        self.b_Q = _get_param_with_shape(\n            (self.cfg.n_qk_heads, self.cfg.d_qk_head), placements=dim_maps[\"b_Q\"].placements(device_mesh)\n        )\n        self.b_K = _get_param_with_shape(\n            (self.cfg.n_qk_heads, self.cfg.d_qk_head), placements=dim_maps[\"b_K\"].placements(device_mesh)\n        )\n        self.b_V = _get_param_with_shape((self.cfg.n_ov_heads,), placements=dim_maps[\"b_V\"].placements(device_mesh))\n        if self.cfg.use_decoder_bias:\n            self.b_D = _get_param_with_shape(\n                (self.cfg.d_model,), placements=dim_maps[\"b_D\"].placements(device_mesh)\n            )\n\n    # Attention mask\n    mask = torch.tril(\n        torch.ones(\n            (self.cfg.n_ctx, self.cfg.n_ctx),\n            device=self.cfg.device,\n            dtype=self.cfg.dtype,\n        ).bool(),\n    )\n    if self.device_mesh is not None:\n        mask = DimMap({}).distribute(mask, self.device_mesh)\n    self.register_buffer(\"mask\", mask)\n\n    if self.device_mesh is not None:\n        IGNORE = DimMap({}).distribute(torch.tensor(-torch.inf, device=self.cfg.device), self.device_mesh)\n    else:\n        IGNORE = torch.tensor(-torch.inf, device=self.cfg.device)\n    self.register_buffer(\"IGNORE\", IGNORE)\n\n    if self.cfg.use_post_qk_ln:\n        # if self.cfg.normalization_type == \"LN\":\n        #     # TODO: fix this\n        #     pass\n        if self.cfg.normalization_type == \"RMS\":\n            self.qk_ln_type = RMSNormPerHead\n        else:\n            raise ValueError(f\"Invalid normalization type for QK-norm: {self.cfg.normalization_type}\")\n    else:\n        self.qk_ln_type = None\n\n    if self.cfg.use_post_qk_ln:\n        assert self.qk_ln_type is not None\n        self.ln_q = self.qk_ln_type(self.cfg, n_heads=self.cfg.n_qk_heads, device_mesh=device_mesh)\n        self.ln_k = self.qk_ln_type(self.cfg, n_heads=self.cfg.n_qk_heads, device_mesh=device_mesh)\n\n    if self.cfg.positional_embedding_type == \"rotary\":\n        # Applies a rotation to each two-element chunk of keys and queries pre dot producting to bake in relative position.\n        if self.cfg.rotary_dim is None:  # keep mypy happy\n            raise ValueError(\"Rotary dim must be provided for rotary positional embeddings\")\n        sin, cos = self._calculate_sin_cos_rotary(\n            self.cfg.rotary_dim,\n            self.cfg.n_ctx,\n            base=self.cfg.rotary_base,\n            dtype=self.cfg.dtype,\n            device=self.cfg.device,\n        )\n        if self.device_mesh is not None:\n            sin = DimMap({}).distribute(sin, self.device_mesh)\n            cos = DimMap({}).distribute(cos, self.device_mesh)\n        self.register_buffer(\"rotary_sin\", sin)\n        self.register_buffer(\"rotary_cos\", cos)\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.init_parameters","title":"init_parameters","text":"<pre><code>init_parameters(**kwargs)\n</code></pre> <p>Initialize parameters.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>def init_parameters(self, **kwargs):\n    \"\"\"Initialize parameters.\"\"\"\n    super().init_parameters(**kwargs)\n\n    torch.nn.init.xavier_uniform_(self.W_Q)\n    torch.nn.init.xavier_uniform_(self.W_K)\n\n    W_V_bound = 1 / math.sqrt(self.cfg.d_sae)\n    # torch.nn.init.uniform_(self.W_V, -W_V_bound, W_V_bound)\n    torch.nn.init.normal_(self.W_V, mean=0, std=W_V_bound)\n\n    W_O_bound = 1 / math.sqrt(self.cfg.d_model)\n    # torch.nn.init.uniform_(self.W_O, -W_O_bound, W_O_bound)\n    torch.nn.init.normal_(self.W_O, mean=0, std=W_O_bound)\n\n    torch.nn.init.zeros_(self.b_Q)\n    torch.nn.init.zeros_(self.b_K)\n    torch.nn.init.zeros_(self.b_V)\n    if self.cfg.use_decoder_bias:\n        torch.nn.init.zeros_(self.b_D)\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.init_lorsa_with_mhsa","title":"init_lorsa_with_mhsa","text":"<pre><code>init_lorsa_with_mhsa(\n    mhsa: Attention | GroupedQueryAttention,\n)\n</code></pre> <p>Initialize Lorsa with Original Multi Head Sparse Attention</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@torch.no_grad()\ndef init_lorsa_with_mhsa(self, mhsa: Attention | GroupedQueryAttention):\n    \"\"\"Initialize Lorsa with Original Multi Head Sparse Attention\"\"\"\n    assert self.cfg.n_qk_heads % mhsa.W_Q.size(0) == 0\n    assert self.cfg.d_qk_head == mhsa.W_Q.size(2)\n    assert self.dataset_average_activation_norm is not None\n    input_norm_factor = math.sqrt(self.cfg.d_model) / self.dataset_average_activation_norm[self.cfg.hook_point_in]\n    qk_exp_factor = self.cfg.n_qk_heads // mhsa.W_Q.size(0)\n    if self.device_mesh is not None:\n        model_parallel_rank = self.device_mesh.get_local_rank(mesh_dim=\"model\")\n        model_parallel_size = mesh_dim_size(self.device_mesh, \"model\")\n        lorsa_qk_start_idx = model_parallel_rank * self.cfg.n_qk_heads // model_parallel_size\n        lorsa_qk_end_idx = lorsa_qk_start_idx + self.cfg.n_qk_heads // model_parallel_size\n        lorsa_qk_indices = torch.arange(lorsa_qk_start_idx, lorsa_qk_end_idx)\n        W_Q_local = mhsa.W_Q[lorsa_qk_indices // qk_exp_factor] / input_norm_factor\n        W_K_local = mhsa.W_K[lorsa_qk_indices // qk_exp_factor] / input_norm_factor\n        W_Q = DTensor.from_local(\n            W_Q_local,\n            device_mesh=self.device_mesh,\n            placements=self.dim_maps()[\"W_Q\"].placements(self.device_mesh),\n        )\n        W_K = DTensor.from_local(\n            W_K_local,\n            device_mesh=self.device_mesh,\n            placements=self.dim_maps()[\"W_K\"].placements(self.device_mesh),\n        )\n        self.W_Q.copy_(W_Q)\n        self.W_K.copy_(W_K)\n        if self.cfg.use_post_qk_ln and self.cfg.normalization_type == \"RMS\":\n            assert FORKED_TL, \"Post-QK layer normalization requires the forked TransformerLens (lmsaes).\"\n            ln_q_w_local = mhsa.ln_q.w[lorsa_qk_indices // qk_exp_factor]  # type: ignore[attr-defined]\n            if mhsa.cfg.n_key_value_heads is not None:\n                ln_k_w_local = torch.repeat_interleave(\n                    mhsa.ln_k.w,  # type: ignore[attr-defined]\n                    mhsa.cfg.n_heads // mhsa.cfg.n_key_value_heads,\n                    dim=0,\n                )[lorsa_qk_indices // qk_exp_factor]\n            else:\n                ln_k_w_local = mhsa.ln_k.w[lorsa_qk_indices // qk_exp_factor]  # type: ignore[attr-defined]\n            ln_q_w = DTensor.from_local(\n                ln_q_w_local,\n                device_mesh=self.device_mesh,\n                placements=self.ln_q.dim_maps()[\"w\"].placements(self.device_mesh),\n            )\n            ln_k_w = DTensor.from_local(\n                ln_k_w_local,\n                device_mesh=self.device_mesh,\n                placements=self.ln_k.dim_maps()[\"w\"].placements(self.device_mesh),\n            )\n            self.ln_q.w.copy_(ln_q_w)\n            self.ln_k.w.copy_(ln_k_w)\n    else:\n        self.W_Q = nn.Parameter(\n            torch.repeat_interleave(mhsa.W_Q, qk_exp_factor, dim=0).to(self.cfg.dtype) / input_norm_factor\n        )\n        self.W_K = nn.Parameter(\n            torch.repeat_interleave(mhsa.W_K, qk_exp_factor, dim=0).to(self.cfg.dtype) / input_norm_factor\n        )\n        if self.cfg.use_post_qk_ln and self.cfg.normalization_type == \"RMS\":\n            assert FORKED_TL, \"Post-QK layer normalization requires the forked TransformerLens (lmsaes).\"\n            self.ln_q.w = nn.Parameter(\n                torch.repeat_interleave(mhsa.ln_q.w, qk_exp_factor, dim=0).to(self.cfg.dtype)  # type: ignore[attr-defined]\n            )\n            self.ln_k.w = nn.Parameter(\n                torch.repeat_interleave(mhsa.ln_k.w, self.ln_k.w.size(0) // mhsa.ln_k.w.size(0), dim=0).to(  # type: ignore[attr-defined]\n                    self.cfg.dtype\n                )\n            )\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.init_W_D_with_active_subspace_per_head","title":"init_W_D_with_active_subspace_per_head","text":"<pre><code>init_W_D_with_active_subspace_per_head(\n    batch: dict[str, Tensor],\n    mhsa: Attention | GroupedQueryAttention,\n)\n</code></pre> <p>Initialize W_D with the active subspace for each head.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@torch.no_grad()\n@torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\ndef init_W_D_with_active_subspace_per_head(\n    self, batch: dict[str, torch.Tensor], mhsa: Attention | GroupedQueryAttention\n):\n    \"\"\"\n    Initialize W_D with the active subspace for each head.\n    \"\"\"\n    x = self.prepare_input(batch)[0]\n    if isinstance(x, DTensor):\n        x = x.to_local()\n\n    captured_z = None\n\n    def capture_hook(tensor, hook):\n        nonlocal captured_z\n        captured_z = tensor.clone().detach()\n        return tensor\n\n    mhsa.hook_z.add_hook(capture_hook)\n    _ = mhsa.forward(\n        query_input=x,\n        key_input=x,\n        value_input=x,\n    )\n    output_per_head = torch.einsum(\"b s n h, n h d -&gt; b s n d\", captured_z, mhsa.W_O)\n    n_ov_per_orig_head = self.cfg.n_ov_heads // mhsa.cfg.n_heads\n    if self.device_mesh is not None:\n        assert isinstance(self.W_O, DTensor)\n        assert isinstance(self.W_V, DTensor)\n        model_parallel_rank = self.device_mesh.get_local_rank(mesh_dim=\"model\")\n        model_parallel_size = mesh_dim_size(self.device_mesh, \"model\")\n        orig_start_idx = model_parallel_rank * mhsa.cfg.n_heads // model_parallel_size\n        orig_end_idx = orig_start_idx + mhsa.cfg.n_heads // model_parallel_size\n        W_O_local = torch.empty_like(self.W_O.to_local())\n        W_V_local = torch.empty_like(self.W_V.to_local())\n        for orig_head_index in range(orig_start_idx, orig_end_idx):\n            output = output_per_head[:, :, orig_head_index, :]\n            output_flattened = output.flatten(0, 1)\n            demeaned_output = output_flattened - output_flattened.mean(dim=0)\n            U, S, V = torch.svd(demeaned_output.T.to(torch.float32))\n            proj_weight = U[:, : self.cfg.d_qk_head]\n            start_idx = (orig_head_index - orig_start_idx) * n_ov_per_orig_head\n            end_idx = min(start_idx + n_ov_per_orig_head, W_O_local.size(0))\n            W_O_local[start_idx:end_idx] = (\n                self.W_O.to_local()[start_idx:end_idx, : self.cfg.d_qk_head] @ proj_weight.T\n            )\n            W_V_local[start_idx:end_idx] = (\n                W_O_local[start_idx:end_idx] @ (mhsa.W_V[orig_head_index] @ mhsa.W_O[orig_head_index]).T\n            )\n        W_V_local = W_V_local / W_V_local.norm(dim=1, keepdim=True)\n        W_O_local = W_O_local / W_O_local.norm(dim=1, keepdim=True)\n        torch.distributed.broadcast(tensor=W_O_local, group=self.device_mesh.get_group(\"data\"), group_src=0)\n        torch.distributed.broadcast(tensor=W_V_local, group=self.device_mesh.get_group(\"data\"), group_src=0)\n        W_O_global = DTensor.from_local(\n            W_O_local, device_mesh=self.device_mesh, placements=self.dim_maps()[\"W_O\"].placements(self.device_mesh)\n        )\n        W_V_global = DTensor.from_local(\n            W_V_local, device_mesh=self.device_mesh, placements=self.dim_maps()[\"W_V\"].placements(self.device_mesh)\n        )\n        self.W_O.copy_(W_O_global)\n        self.W_V.copy_(W_V_global)\n    else:\n        for orig_head_index in range(mhsa.cfg.n_heads):\n            output = output_per_head[:, :, orig_head_index, :]\n            output_flattened = output.flatten(0, 1)\n            demeaned_output = output_flattened - output_flattened.mean(dim=0)\n            U, S, V = torch.svd(demeaned_output.T.to(torch.float32))\n            proj_weight = U[:, : self.cfg.d_qk_head]\n            self.W_O[orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head] = (\n                self.W_O[\n                    orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head,\n                    : self.cfg.d_qk_head,\n                ]\n                @ proj_weight.T\n            )\n            self.W_V[orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head] = (\n                self.W_O[orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head]\n                @ (mhsa.W_V[orig_head_index] @ mhsa.W_O[orig_head_index]).T\n            )\n        self.W_V.copy_(self.W_V / self.W_V.norm(dim=1, keepdim=True))\n        self.W_O.copy_(self.W_O / self.W_O.norm(dim=1, keepdim=True))\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.init_W_V_with_active_subspace_per_head","title":"init_W_V_with_active_subspace_per_head","text":"<pre><code>init_W_V_with_active_subspace_per_head(\n    batch: dict[str, Tensor],\n    mhsa: Attention | GroupedQueryAttention,\n)\n</code></pre> <p>Initialize W_D with the active subspace for each head.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@torch.no_grad()\n@torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\ndef init_W_V_with_active_subspace_per_head(\n    self, batch: dict[str, torch.Tensor], mhsa: Attention | GroupedQueryAttention\n):\n    \"\"\"\n    Initialize W_D with the active subspace for each head.\n    \"\"\"\n    x = self.prepare_input(batch)[0]\n    if isinstance(x, DTensor):\n        x = x.to_local()\n\n    v_per_head = (\n        x.reshape(-1, self.cfg.d_model) @ mhsa.W_V.permute(1, 0, 2).reshape(mhsa.cfg.d_model, mhsa.cfg.d_model)\n    ).reshape(-1, mhsa.cfg.n_heads, mhsa.cfg.d_head)\n    captured_v = torch.einsum(\"bnh,nhd-&gt;bnd\", v_per_head, mhsa.W_V.permute(0, 2, 1))\n\n    n_ov_per_orig_head = self.cfg.n_ov_heads // mhsa.cfg.n_heads\n    if self.device_mesh is not None:\n        assert isinstance(self.W_O, DTensor)\n        assert isinstance(self.W_V, DTensor)\n        model_parallel_rank = self.device_mesh.get_local_rank(mesh_dim=\"model\")\n        model_parallel_size = mesh_dim_size(self.device_mesh, \"model\")\n        orig_start_idx = model_parallel_rank * mhsa.cfg.n_heads // model_parallel_size\n        orig_end_idx = orig_start_idx + mhsa.cfg.n_heads // model_parallel_size\n        W_O_local = torch.empty_like(self.W_O.to_local())\n        W_V_local = torch.empty_like(self.W_V.to_local())\n        for orig_head_index in range(orig_start_idx, orig_end_idx):\n            v = captured_v[:, orig_head_index]\n            demeaned_v = v - v.mean(dim=0)\n            U, S, V = torch.svd(demeaned_v.T.to(torch.float32))\n            proj_weight = U[:, : self.cfg.d_qk_head]\n            start_idx = (orig_head_index - orig_start_idx) * n_ov_per_orig_head\n            end_idx = min(start_idx + n_ov_per_orig_head, W_O_local.size(0))\n            W_V_local[start_idx:end_idx] = (\n                self.W_V.to_local()[start_idx:end_idx, : self.cfg.d_qk_head] @ proj_weight.T\n            )\n            W_O_local[start_idx:end_idx] = (\n                W_V_local[start_idx:end_idx] @ mhsa.W_V[orig_head_index] @ mhsa.W_O[orig_head_index]\n            )\n        W_V_local = W_V_local / W_V_local.norm(dim=1, keepdim=True)\n        W_O_local = W_O_local / W_O_local.norm(dim=1, keepdim=True)\n        torch.distributed.broadcast(tensor=W_O_local, group=self.device_mesh.get_group(\"data\"), group_src=0)\n        torch.distributed.broadcast(tensor=W_V_local, group=self.device_mesh.get_group(\"data\"), group_src=0)\n        W_O_global = DTensor.from_local(\n            W_O_local, device_mesh=self.device_mesh, placements=self.dim_maps()[\"W_O\"].placements(self.device_mesh)\n        )\n        W_V_global = DTensor.from_local(\n            W_V_local, device_mesh=self.device_mesh, placements=self.dim_maps()[\"W_V\"].placements(self.device_mesh)\n        )\n        self.W_O.copy_(W_O_global)\n        self.W_V.copy_(W_V_global)\n    else:\n        for orig_head_index in range(mhsa.cfg.n_heads):\n            v = captured_v[:, orig_head_index]\n            demeaned_v = v - v.mean(dim=0)\n            U, S, V = torch.svd(demeaned_v.T.to(torch.float32))\n            proj_weight = U[:, : self.cfg.d_qk_head]\n            self.W_V[orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head] = (\n                self.W_V[\n                    orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head,\n                    : self.cfg.d_qk_head,\n                ]\n                @ proj_weight.T\n            )\n            self.W_O[orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head] = (\n                self.W_V[orig_head_index * n_ov_per_orig_head : (orig_head_index + 1) * n_ov_per_orig_head]\n                @ mhsa.W_V[orig_head_index]\n                @ mhsa.W_O[orig_head_index]\n            )\n        self.W_V.copy_(self.W_V / self.W_V.norm(dim=1, keepdim=True))\n        self.W_O.copy_(self.W_O / self.W_O.norm(dim=1, keepdim=True))\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.encoder_norm","title":"encoder_norm","text":"<pre><code>encoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Norm of encoder (Q/K weights).</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef encoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Norm of encoder (Q/K weights).\"\"\"\n    if not isinstance(self.W_V, DTensor):\n        return torch.norm(self.W_V, p=2, dim=1, keepdim=keepdim).to(self.cfg.device)\n    else:\n        assert self.device_mesh is not None\n        return DTensor.from_local(\n            torch.norm(self.W_V.to_local(), p=2, dim=1, keepdim=keepdim),\n            device_mesh=self.device_mesh,\n            placements=self.dim_maps()[\"W_V\"].placements(self.device_mesh),\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.decoder_norm","title":"decoder_norm","text":"<pre><code>decoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Norm of decoder (O weights).</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef decoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Norm of decoder (O weights).\"\"\"\n    if not isinstance(self.W_O, DTensor):\n        return torch.norm(self.W_O, p=2, dim=1, keepdim=keepdim).to(self.cfg.device)\n    else:\n        assert self.device_mesh is not None\n        return DTensor.from_local(\n            torch.norm(self.W_O.to_local(), p=2, dim=1, keepdim=keepdim),\n            device_mesh=self.device_mesh,\n            placements=self.dim_maps()[\"W_O\"].placements(self.device_mesh),\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.decoder_bias_norm","title":"decoder_bias_norm","text":"<pre><code>decoder_bias_norm() -&gt; Tensor\n</code></pre> <p>Norm of decoder bias.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef decoder_bias_norm(self) -&gt; torch.Tensor:\n    \"\"\"Norm of decoder bias.\"\"\"\n    if not self.cfg.use_decoder_bias:\n        raise ValueError(\"Decoder bias not used\")\n    return torch.norm(self.b_D, p=2, dim=0, keepdim=True)\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.transform_to_unit_decoder_norm","title":"transform_to_unit_decoder_norm","text":"<pre><code>transform_to_unit_decoder_norm()\n</code></pre> <p>Transform to unit decoder norm.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\n@torch.no_grad()\ndef transform_to_unit_decoder_norm(self):\n    \"\"\"Transform to unit decoder norm.\"\"\"\n    norm = self.decoder_norm(keepdim=True)\n    self.W_O /= norm\n    self.W_V *= norm\n    self.b_V *= norm.squeeze()\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.standardize_parameters_of_dataset_norm","title":"standardize_parameters_of_dataset_norm","text":"<pre><code>standardize_parameters_of_dataset_norm()\n</code></pre> <p>Standardize parameters for dataset norm.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\n@torch.no_grad()\ndef standardize_parameters_of_dataset_norm(self):\n    \"\"\"Standardize parameters for dataset norm.\"\"\"\n    assert self.cfg.norm_activation == \"dataset-wise\"\n    assert self.dataset_average_activation_norm is not None\n\n    hook_point_in = self.cfg.hook_point_in\n    hook_point_out = self.cfg.hook_point_out\n\n    input_norm_factor = math.sqrt(self.cfg.d_model) / self.dataset_average_activation_norm[hook_point_in]\n    output_norm_factor = math.sqrt(self.cfg.d_model) / self.dataset_average_activation_norm[hook_point_out]\n\n    self.W_Q.data *= input_norm_factor\n    self.W_K.data *= input_norm_factor\n\n    self.W_V.data *= input_norm_factor\n\n    self.W_O.data = self.W_O.data / output_norm_factor\n    self.b_D.data = self.b_D.data / output_norm_factor\n\n    self.cfg.norm_activation = \"inference\"\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.compute_hidden_pre","title":"compute_hidden_pre","text":"<pre><code>compute_hidden_pre(\n    x: Float[Tensor, \"batch seq_len d_model\"],\n) -&gt; Float[Tensor, \"batch seq_len d_sae\"]\n</code></pre> <p>Compute the hidden pre-activations.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>def compute_hidden_pre(\n    self, x: Float[torch.Tensor, \"batch seq_len d_model\"]\n) -&gt; Float[torch.Tensor, \"batch seq_len d_sae\"]:\n    \"\"\"Compute the hidden pre-activations.\"\"\"\n    q, k, v = self._compute_qkv(x)\n    query = q.permute(0, 2, 1, 3)\n    key = k.permute(0, 2, 1, 3)\n    value = v.reshape(*k.shape[:3], -1).permute(0, 2, 1, 3)\n    with sdpa_kernel(\n        backends=[\n            SDPBackend.FLASH_ATTENTION,\n            SDPBackend.CUDNN_ATTENTION,\n            SDPBackend.EFFICIENT_ATTENTION,\n            SDPBackend.MATH,\n        ]\n    ):\n        z = F.scaled_dot_product_attention(\n            query, key, value, scale=1 / self.attn_scale, is_causal=True, enable_gqa=True\n        )\n    return z.permute(0, 2, 1, 3).reshape(*v.shape)\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.encode","title":"encode","text":"<pre><code>encode(\n    x: Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: Literal[False] = False,\n    **kwargs,\n) -&gt; Float[Tensor, \"batch seq_len d_sae\"]\n</code></pre><pre><code>encode(\n    x: Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: Literal[True],\n    **kwargs,\n) -&gt; tuple[\n    Float[Tensor, \"batch seq_len d_sae\"],\n    Float[Tensor, \"batch seq_len d_sae\"],\n]\n</code></pre> <pre><code>encode(\n    x: Float[Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: bool = False,\n    return_attention_pattern: bool = False,\n    return_attention_score: bool = False,\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch seq_len d_sae\"]\n    | tuple[\n        Float[Tensor, \"batch seq_len d_sae\"],\n        Float[Tensor, \"batch seq_len d_sae\"],\n    ]\n    | tuple[\n        Float[Tensor, \"batch seq_len d_sae\"],\n        Float[Tensor, \"batch seq_len d_sae\"],\n        Float[Tensor, \"batch n_qk_heads q_pos k_pos\"],\n    ]\n)\n</code></pre> <p>Encode to sparse head activations.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef encode(\n    self,\n    x: Float[torch.Tensor, \"batch seq_len d_model\"],\n    return_hidden_pre: bool = False,\n    return_attention_pattern: bool = False,\n    return_attention_score: bool = False,\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch seq_len d_sae\"],\n    Tuple[\n        Float[torch.Tensor, \"batch seq_len d_sae\"],\n        Float[torch.Tensor, \"batch seq_len d_sae\"],\n    ],\n    Tuple[\n        Float[torch.Tensor, \"batch seq_len d_sae\"],\n        Float[torch.Tensor, \"batch seq_len d_sae\"],\n        Float[torch.Tensor, \"batch n_qk_heads q_pos k_pos\"],\n    ],\n]:\n    \"\"\"Encode to sparse head activations.\"\"\"\n    # Compute Q, K, V\n    q, k, v = self._compute_qkv(x)\n\n    pattern: Optional[torch.Tensor] = None\n    scores: Optional[torch.Tensor] = None\n\n    if not (return_attention_pattern or return_attention_score):\n        query = q.permute(0, 2, 1, 3)\n        key = k.permute(0, 2, 1, 3)\n        value = v.reshape(*k.shape[:3], -1).permute(0, 2, 1, 3)\n        with sdpa_kernel(\n            backends=[\n                SDPBackend.FLASH_ATTENTION,\n                SDPBackend.CUDNN_ATTENTION,\n                SDPBackend.EFFICIENT_ATTENTION,\n                SDPBackend.MATH,\n            ]\n        ):\n            z = F.scaled_dot_product_attention(\n                query, key, value, scale=1 / self.attn_scale, is_causal=True, enable_gqa=True\n            )\n        hidden_pre = z.permute(0, 2, 1, 3).reshape(*v.shape)\n    else:\n        # Attention pattern\n        # n_qk_heads batch q_pos k_pos\n        q = q.permute(2, 0, 1, 3)  # (n_qk_heads, batch, seq_len, d_qk_head)\n        k = k.permute(2, 0, 3, 1)  # (n_qk_heads, batch, d_qk_head, seq_len)\n        scores = torch.einsum(\"nbqd,nbdk-&gt;nbqk\", q, k) / self.attn_scale\n        scores = self._apply_causal_mask(scores)\n        pattern = F.softmax(scores, dim=-1)\n\n        # Head outputs\n        hidden_pre = self._compute_head_outputs(pattern, v)\n\n    # Scale feature activations by decoder norm if configured\n    if self.cfg.sparsity_include_decoder_norm:\n        hidden_pre = hidden_pre * self.decoder_norm()\n\n    feature_acts = self.activation_function(hidden_pre)\n\n    if self.cfg.sparsity_include_decoder_norm:\n        feature_acts = feature_acts / self.decoder_norm()\n        hidden_pre = hidden_pre / self.decoder_norm()\n\n    return_values: list[torch.Tensor] = [feature_acts]\n    if return_hidden_pre:\n        return_values.append(hidden_pre)\n    if return_attention_pattern and pattern is not None:\n        return_values.append(pattern.permute(1, 0, 2, 3))\n    if return_attention_score and scores is not None:\n        return_values.append(scores.permute(1, 0, 2, 3))\n    return tuple(return_values) if len(return_values) &gt; 1 else return_values[0]  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.decode","title":"decode","text":"<pre><code>decode(feature_acts, **kwargs)\n</code></pre> <p>Decode head activations to output.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef decode(self, feature_acts, **kwargs):\n    \"\"\"Decode head activations to output.\"\"\"\n    if feature_acts.layout == torch.sparse_coo:\n        return (\n            torch.sparse.mm(\n                feature_acts.to(torch.float32),\n                self.W_O.to(torch.float32),\n            ).to(self.cfg.dtype)\n            + self.b_D\n        )\n    out = torch.einsum(\"bps,sd-&gt;bpd\", feature_acts, self.W_O)\n    if self.cfg.use_decoder_bias:\n        out = out + self.b_D\n    if isinstance(out, DTensor):\n        out = DimMap({\"data\": 0}).redistribute(out)\n    return out\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.set_decoder_to_fixed_norm","title":"set_decoder_to_fixed_norm","text":"<pre><code>set_decoder_to_fixed_norm(value: float, force_exact: bool)\n</code></pre> <p>Set decoder weights to a fixed norm.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\n@torch.no_grad()\ndef set_decoder_to_fixed_norm(self, value: float, force_exact: bool):\n    \"\"\"Set decoder weights to a fixed norm.\"\"\"\n    if force_exact:\n        self.W_O.mul_(value / self.decoder_norm(keepdim=True).mean())\n    else:\n        self.W_O.mul_(value / torch.clamp(self.decoder_norm(keepdim=True).mean(), min=value))\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.set_encoder_to_fixed_norm","title":"set_encoder_to_fixed_norm","text":"<pre><code>set_encoder_to_fixed_norm(value: float)\n</code></pre> <p>Set encoder weights to fixed norm.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\n@torch.no_grad()\ndef set_encoder_to_fixed_norm(self, value: float):\n    \"\"\"Set encoder weights to fixed norm.\"\"\"\n    raise NotImplementedError(\"set_encoder_to_fixed_norm does not make sense for lorsa\")\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.dim_maps","title":"dim_maps","text":"<pre><code>dim_maps() -&gt; dict[str, DimMap]\n</code></pre> <p>Return a dictionary mapping parameter names to dimension maps.</p> <p>Returns:</p> Type Description <code>dict[str, DimMap]</code> <p>A dictionary mapping parameter names to DimMap objects.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef dim_maps(self) -&gt; dict[str, DimMap]:\n    \"\"\"Return a dictionary mapping parameter names to dimension maps.\n\n    Returns:\n        A dictionary mapping parameter names to DimMap objects.\n    \"\"\"\n    base_maps = super().dim_maps()\n    return {\n        **base_maps,\n        \"W_Q\": DimMap({\"model\": 0}),\n        \"W_K\": DimMap({\"model\": 0}),\n        \"W_V\": DimMap({\"model\": 0}),\n        \"W_O\": DimMap({\"model\": 0}),\n        \"b_Q\": DimMap({\"model\": 0}),\n        \"b_K\": DimMap({\"model\": 0}),\n        \"b_V\": DimMap({\"model\": 0}),\n        \"b_D\": DimMap({}),\n    }\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.prepare_input","title":"prepare_input","text":"<pre><code>prepare_input(\n    batch: dict[str, Tensor], **kwargs\n) -&gt; tuple[Tensor, dict[str, Any], dict[str, Any]]\n</code></pre> <p>Prepare input tensor.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef prepare_input(\n    self, batch: dict[str, torch.Tensor], **kwargs\n) -&gt; tuple[torch.Tensor, dict[str, Any], dict[str, Any]]:\n    \"\"\"Prepare input tensor.\"\"\"\n    x = batch[self.cfg.hook_point_in]\n    return x, {}, {}\n</code></pre>"},{"location":"reference/models/#lm_saes.LowRankSparseAttention.prepare_label","title":"prepare_label","text":"<pre><code>prepare_label(batch: dict[str, Tensor], **kwargs)\n</code></pre> <p>Prepare label tensor.</p> Source code in <code>src/lm_saes/lorsa.py</code> <pre><code>@override\ndef prepare_label(self, batch: dict[str, torch.Tensor], **kwargs):\n    \"\"\"Prepare label tensor.\"\"\"\n    label = batch[self.cfg.hook_point_out]\n    return label\n</code></pre>"},{"location":"reference/models/#lm_saes.MOLTConfig","title":"MOLTConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseSAEConfig</code></p> <p>Configuration for Mixture of Linear Transforms (MOLT).</p> <p>MOLT is a more efficient alternative to transcoders that sparsely replaces MLP computation in transformers. It converts dense MLP layers into sparse, interpretable linear transforms.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>d_model</code>                 (<code>int</code>)             </li> <li> <code>expansion_factor</code>                 (<code>float</code>)             </li> <li> <code>use_decoder_bias</code>                 (<code>bool</code>)             </li> <li> <code>act_fn</code>                 (<code>Literal['relu', 'jumprelu', 'topk', 'batchtopk', 'batchlayertopk', 'layertopk']</code>)             </li> <li> <code>norm_activation</code>                 (<code>Literal['token-wise', 'batch-wise', 'dataset-wise', 'inference']</code>)             </li> <li> <code>sparsity_include_decoder_norm</code>                 (<code>bool</code>)             </li> <li> <code>top_k</code>                 (<code>int</code>)             </li> <li> <code>use_triton_kernel</code>                 (<code>bool</code>)             </li> <li> <code>sparsity_threshold_for_triton_spmm_kernel</code>                 (<code>float</code>)             </li> <li> <code>jumprelu_threshold_window</code>                 (<code>float</code>)             </li> <li> <code>sae_type</code>                 (<code>str</code>)             </li> <li> <code>hook_point_in</code>                 (<code>str</code>)             </li> <li> <code>hook_point_out</code>                 (<code>str</code>)             </li> <li> <code>rank_counts</code>                 (<code>dict[int, int]</code>)             </li> </ul>"},{"location":"reference/models/#lm_saes.MOLTConfig.hook_point_in","title":"hook_point_in  <code>pydantic-field</code>","text":"<pre><code>hook_point_in: str\n</code></pre> <p>Hook point to capture input activations from.</p>"},{"location":"reference/models/#lm_saes.MOLTConfig.hook_point_out","title":"hook_point_out  <code>pydantic-field</code>","text":"<pre><code>hook_point_out: str\n</code></pre> <p>Hook point to output activations to.</p>"},{"location":"reference/models/#lm_saes.MOLTConfig.rank_counts","title":"rank_counts  <code>pydantic-field</code>","text":"<pre><code>rank_counts: dict[int, int]\n</code></pre> <p>Dictionary mapping rank values to their integer counts. Example: {4: 128, 8: 256, 16: 128} means 128 transforms of rank 4, 256 transforms of rank 8, and 128 transforms of rank 16.</p>"},{"location":"reference/models/#lm_saes.MOLTConfig.d_sae","title":"d_sae  <code>property</code>","text":"<pre><code>d_sae: int\n</code></pre> <p>Calculate d_sae based on total rank counts.</p>"},{"location":"reference/models/#lm_saes.MOLTConfig.available_ranks","title":"available_ranks  <code>property</code>","text":"<pre><code>available_ranks: list[int]\n</code></pre> <p>Get sorted list of available ranks.</p>"},{"location":"reference/models/#lm_saes.MOLTConfig.num_rank_types","title":"num_rank_types  <code>property</code>","text":"<pre><code>num_rank_types: int\n</code></pre> <p>Number of different rank types.</p>"},{"location":"reference/models/#lm_saes.MOLTConfig.generate_rank_assignments","title":"generate_rank_assignments","text":"<pre><code>generate_rank_assignments() -&gt; list[int]\n</code></pre> <p>Generate rank assignment for each of the d_sae linear transforms.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>List of rank assignments for each transform.</p> <code>list[int]</code> <p>For example: [1, 1, 1, 1, 2, 2, 4].</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>def generate_rank_assignments(self) -&gt; list[int]:\n    \"\"\"Generate rank assignment for each of the d_sae linear transforms.\n\n    Returns:\n        List of rank assignments for each transform.\n        For example: [1, 1, 1, 1, 2, 2, 4].\n    \"\"\"\n    assignments = []\n    for rank in sorted(self.rank_counts.keys()):\n        assignments.extend([rank] * self.rank_counts[rank])\n    return assignments\n</code></pre>"},{"location":"reference/models/#lm_saes.MOLTConfig.get_local_rank_assignments","title":"get_local_rank_assignments","text":"<pre><code>get_local_rank_assignments(\n    model_parallel_size: int,\n) -&gt; list[int]\n</code></pre> <p>Get rank assignments for a specific local device in distributed running.</p> <p>Each device gets all rank groups, with each group evenly divided across devices. This ensures consistent encoder/decoder sharding without feature_acts redistribution.</p> <p>Parameters:</p> Name Type Description Default <code>model_parallel_size</code> <code>int</code> <p>Number of model parallel devices for training and inference.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>List of rank assignments for this local device</p> <code>list[int]</code> <p>For example:</p> <code>list[int]</code> <p>global_rank_assignments = [1, 1, 2, 2], model_parallel_size = 2 -&gt; local_rank_assignments = [1, 2]</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>def get_local_rank_assignments(self, model_parallel_size: int) -&gt; list[int]:\n    \"\"\"Get rank assignments for a specific local device in distributed running.\n\n    Each device gets all rank groups, with each group evenly divided across devices.\n    This ensures consistent encoder/decoder sharding without feature_acts redistribution.\n\n    Args:\n        model_parallel_size: Number of model parallel devices for training and inference.\n\n    Returns:\n        List of rank assignments for this local device\n        For example:\n        global_rank_assignments = [1, 1, 2, 2], model_parallel_size = 2 -&gt; local_rank_assignments = [1, 2]\n    \"\"\"\n    local_assignments = []\n    for rank in sorted(self.rank_counts.keys()):\n        global_count = self.rank_counts[rank]\n\n        # Verify even division\n        assert global_count % model_parallel_size == 0, (\n            f\"Transform rank {rank} global count {global_count} not divisible by \"\n            f\"model_parallel_size {model_parallel_size}\"\n        )\n\n        local_count = global_count // model_parallel_size\n        local_assignments.extend([rank] * local_count)\n\n    return local_assignments\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform","title":"MixtureOfLinearTransform","text":"<pre><code>MixtureOfLinearTransform(\n    cfg: MOLTConfig, device_mesh: DeviceMesh | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractSparseAutoEncoder</code></p> <p>Mixture of Linear Transforms (MOLT) model.</p> <p>MOLT is a sparse autoencoder variant that uses d_sae linear transforms, each with its own rank for UtVt decomposition.</p> <p>Mathematical Formulation: - Encoder: \u03d5(et \u00b7 x - bt) where \u03d5 is the activation function - Decoder: \u03a3\u1d62 f\u1d62 \u00b7 (U\u1d62 @ V\u1d62 @ x) where f\u1d62 are feature activations - Decoder norm: ||U\u1d62V\u1d62||_F for each transform i</p> <p>The rank of each transform is determined by the rank_counts configuration, allowing for adaptive model capacity allocation.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>def __init__(self, cfg: MOLTConfig, device_mesh: DeviceMesh | None = None) -&gt; None:\n    super().__init__(cfg, device_mesh=device_mesh)\n    self.cfg = cfg\n\n    # Generate rank assignment for each linear transform\n    if device_mesh is not None:\n        # In distributed training/inference, get local rank assignments\n        # Use model dimension for tensor parallelism\n        mesh_dim_names = device_mesh.mesh_dim_names\n        if mesh_dim_names is None:\n            model_dim_index = 0\n        else:\n            model_dim_index = mesh_dim_names.index(\"model\") if \"model\" in mesh_dim_names else 0\n        local_rank = device_mesh.get_local_rank(\n            mesh_dim=model_dim_index\n        )  # this rank stands for device rank of this process\n        model_parallel_size = device_mesh.size(mesh_dim=model_dim_index)\n\n        self.rank_assignments = cfg.get_local_rank_assignments(model_parallel_size)\n\n        for k, v in cfg.rank_counts.items():\n            logger.info(\n                f\"Rank {k} has {v} global transforms, device rank {local_rank} has {self.rank_assignments.count(k)} transforms\"\n            )\n    else:\n        # Non-distributed case\n        self.rank_assignments = cfg.generate_rank_assignments()\n\n    # Encoder parameters (standard SAE encoder)\n    if device_mesh is None:\n        self.W_E = nn.Parameter(torch.empty(cfg.d_model, cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n        self.b_E = nn.Parameter(torch.empty(cfg.d_sae, device=cfg.device, dtype=cfg.dtype))\n\n        # Decoder parameters: d_sae linear transforms, each with UtVt decomposition\n        # Group by rank for efficient parameter storage\n        self.U_matrices = nn.ParameterDict()\n        self.V_matrices = nn.ParameterDict()\n\n        for rank in cfg.available_ranks:\n            count = sum(1 for r in self.rank_assignments if r == rank)\n            # Always create parameters for all rank types for consistency\n            # In non-distributed case, we can skip empty tensors\n            if count &gt; 0:\n                self.U_matrices[str(rank)] = nn.Parameter(\n                    torch.empty(count, cfg.d_model, rank, device=cfg.device, dtype=cfg.dtype)\n                )\n                self.V_matrices[str(rank)] = nn.Parameter(\n                    torch.empty(count, rank, cfg.d_model, device=cfg.device, dtype=cfg.dtype)\n                )\n\n        if cfg.use_decoder_bias:\n            self.b_D = nn.Parameter(torch.empty(cfg.d_model, device=cfg.device, dtype=cfg.dtype))\n    else:\n        # Distributed initialization\n        w_e_placements = self.dim_maps()[\"W_E\"].placements(device_mesh)\n        b_e_placements = self.dim_maps()[\"b_E\"].placements(device_mesh)\n        self.W_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.d_model,\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=w_e_placements,\n            )\n        )\n\n        self.b_E = nn.Parameter(\n            torch.distributed.tensor.empty(\n                cfg.d_sae,\n                dtype=cfg.dtype,\n                device_mesh=device_mesh,\n                placements=b_e_placements,\n            )\n        )\n\n        # Decoder parameters: d_sae linear transforms, each with UtVt decomposition\n        # Group by rank for efficient parameter storage\n        self.U_matrices = nn.ParameterDict()\n        self.V_matrices = nn.ParameterDict()\n\n        for rank in cfg.available_ranks:\n            local_count = sum(1 for r in self.rank_assignments if r == rank)\n            assert local_count &gt; 0, f\"Rank {rank} has local_count=0, sharding logic error\"\n\n            # Create DTensor with GLOBAL shape\n            self.U_matrices[str(rank)] = nn.Parameter(\n                torch.distributed.tensor.empty(\n                    self.cfg.rank_counts[rank],  # GLOBAL count\n                    cfg.d_model,\n                    rank,\n                    dtype=cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=self.dim_maps()[\"U_matrices\"].placements(device_mesh),\n                )\n            )\n\n            self.V_matrices[str(rank)] = nn.Parameter(\n                torch.distributed.tensor.empty(\n                    self.cfg.rank_counts[rank],  # GLOBAL count\n                    rank,\n                    cfg.d_model,\n                    dtype=cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=self.dim_maps()[\"V_matrices\"].placements(device_mesh),\n                )\n            )\n\n        if cfg.use_decoder_bias:\n            self.b_D = nn.Parameter(\n                torch.distributed.tensor.empty(\n                    cfg.d_model,\n                    dtype=cfg.dtype,\n                    device_mesh=device_mesh,\n                    placements=self.dim_maps()[\"b_D\"].placements(device_mesh),\n                )\n            )\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.dim_maps","title":"dim_maps","text":"<pre><code>dim_maps() -&gt; dict[str, DimMap]\n</code></pre> <p>Return dimension maps for distributed training.</p> <p>Encoder and decoder use consistent sharding: - W_E sharded along d_sae (output) dimension - U/V matrices sharded along transform count (first) dimension This ensures feature_acts from encoder can directly feed decoder without redistribution.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>def dim_maps(self) -&gt; dict[str, DimMap]:\n    \"\"\"Return dimension maps for distributed training.\n\n    Encoder and decoder use consistent sharding:\n    - W_E sharded along d_sae (output) dimension\n    - U/V matrices sharded along transform count (first) dimension\n    This ensures feature_acts from encoder can directly feed decoder without redistribution.\n    \"\"\"\n    base_maps = super().dim_maps()\n\n    molt_maps = {\n        \"W_E\": DimMap({\"model\": 1}),  # Shard along d_sae dimension\n        \"b_E\": DimMap({\"model\": 0}),  # Shard along d_sae dimension\n        # U and V matrices sharded along transform count dimension\n        # This matches the W_E sharding pattern for feature_acts compatibility\n        \"U_matrices\": DimMap({\"model\": 0}),  # Shard along transform count\n        \"V_matrices\": DimMap({\"model\": 0}),  # Shard along transform count\n        \"b_D\": DimMap({}),  # Replicate decoder bias\n    }\n\n    return base_maps | molt_maps\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.encoder_norm","title":"encoder_norm","text":"<pre><code>encoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Compute the norm of the encoder weight.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>@override\n@timer.time(\"encoder_norm\")\ndef encoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Compute the norm of the encoder weight.\"\"\"\n    if not isinstance(self.W_E, DTensor):\n        return torch.norm(self.W_E, p=2, dim=0, keepdim=keepdim).to(self.cfg.device)\n    else:\n        assert self.device_mesh is not None\n        return DTensor.from_local(\n            torch.norm(self.W_E.to_local(), p=2, dim=0, keepdim=keepdim),\n            device_mesh=self.device_mesh,\n            placements=DimMap({\"model\": 1 if keepdim else 0}).placements(self.device_mesh),\n        )\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.decoder_norm","title":"decoder_norm","text":"<pre><code>decoder_norm(keepdim: bool = False) -&gt; Tensor\n</code></pre> <p>Compute the Frobenius norm of each linear transform's UtVt decomposition.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>@override\n@timer.time(\"decoder_norm\")\ndef decoder_norm(self, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Compute the Frobenius norm of each linear transform's UtVt decomposition.\"\"\"\n    # Pre-compute norms for all rank groups and concatenate\n    norm_list = []\n\n    for rank in self.cfg.available_ranks:\n        rank_str = str(rank)\n        if rank_str in self.U_matrices:\n            U = self.U_matrices[rank_str]  # (count, d_model, rank)\n            V = self.V_matrices[rank_str]  # (count, rank, d_model)\n\n            assert isinstance(U, DTensor) == isinstance(V, DTensor), \"U and V must have the same type\"\n            # Handle DTensor case - work with local shards\n            if isinstance(U, DTensor) and isinstance(V, DTensor):\n                U_local = U.to_local()\n                V_local = V.to_local()\n\n                # Compute ||U_i @ V_i||_F for each transform (local shard)\n                UV_local = torch.bmm(U_local, V_local)  # (local_count, d_model, d_model)\n                UV_norms_local = torch.norm(UV_local.view(UV_local.shape[0], -1), p=\"fro\", dim=1)  # (local_count,)\n\n                # Convert back to DTensor with proper placement\n                assert self.device_mesh is not None\n                UV_norms = DTensor.from_local(\n                    UV_norms_local,\n                    device_mesh=self.device_mesh,\n                    placements=self.dim_maps()[\"U_matrices\"].placements(self.device_mesh)[\n                        0:1\n                    ],  # Only keep first dimension placement\n                )\n                norm_list.append(UV_norms)\n            else:\n                # Non-distributed case\n                UV = torch.bmm(U, V)  # (count, d_model, d_model)\n                UV_norms = torch.norm(UV.view(UV.shape[0], -1), p=\"fro\", dim=1)  # (count,)\n                norm_list.append(UV_norms)\n\n    if not norm_list:\n        if self.device_mesh is not None:\n            # Create replicated DTensor for zero norms\n            norms = DTensor.from_local(\n                torch.zeros(self.cfg.d_sae, device=self.cfg.device, dtype=self.cfg.dtype),\n                device_mesh=self.device_mesh,\n                placements=self.dim_maps()[\"b_E\"].placements(self.device_mesh),  # Same as b_E sharding\n            )\n        else:\n            norms = torch.zeros(self.cfg.d_sae, device=self.cfg.device, dtype=self.cfg.dtype)\n    else:\n        # Concatenate all norms in correct order\n        if isinstance(norm_list[0], DTensor):\n            # CRITICAL FIX: Avoid full_tensor() to prevent numerical errors\n            # Instead, directly concatenate the DTensors which preserves numerical precision\n            assert self.device_mesh is not None\n\n            # Convert each DTensor norm to local tensor and concatenate locally\n            local_norms = [norm.to_local() for norm in norm_list]\n\n            # Concatenate local norms and convert back to DTensor\n            norms_local = torch.cat(local_norms, dim=0)\n            norms = DTensor.from_local(\n                norms_local,\n                device_mesh=self.device_mesh,\n                placements=self.dim_maps()[\"b_E\"].placements(self.device_mesh),  # Same as b_E (d_sae dimension)\n            )\n        else:\n            norms = torch.cat(norm_list, dim=0)  # (d_sae,)\n\n    if keepdim:\n        return norms.unsqueeze(-1)\n    else:\n        return norms\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.set_encoder_to_fixed_norm","title":"set_encoder_to_fixed_norm","text":"<pre><code>set_encoder_to_fixed_norm(value: float) -&gt; None\n</code></pre> <p>Set encoder weights to a fixed norm.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>@torch.no_grad()\n@timer.time(\"set_encoder_to_fixed_norm\")\ndef set_encoder_to_fixed_norm(self, value: float) -&gt; None:\n    \"\"\"Set encoder weights to a fixed norm.\"\"\"\n    self.W_E.mul_(value / self.encoder_norm(keepdim=True))\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.decode","title":"decode","text":"<pre><code>decode(\n    feature_acts: Float[Tensor, \"batch d_sae\"]\n    | Float[Tensor, \"batch seq_len d_sae\"],\n    **kwargs,\n) -&gt; (\n    Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"]\n)\n</code></pre> <p>Decode feature activations back to model space using MOLT transforms.</p> <p>Parameters:</p> Name Type Description Default <code>feature_acts</code> <code>Float[Tensor, 'batch d_sae'] | Float[Tensor, 'batch seq_len d_sae']</code> <p>Feature activations from encode()</p> required <code>**kwargs</code> <p>Must contain 'original_x' - the original input tensor</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch d_model'] | Float[Tensor, 'batch seq_len d_model']</code> <p>Reconstructed tensor in model space</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>@override\n@timer.time(\"decode\")\ndef decode(\n    self,\n    feature_acts: Union[\n        Float[torch.Tensor, \"batch d_sae\"],\n        Float[torch.Tensor, \"batch seq_len d_sae\"],\n    ],\n    **kwargs,\n) -&gt; Union[\n    Float[torch.Tensor, \"batch d_model\"],\n    Float[torch.Tensor, \"batch seq_len d_model\"],\n]:\n    \"\"\"Decode feature activations back to model space using MOLT transforms.\n\n    Args:\n        feature_acts: Feature activations from encode()\n        **kwargs: Must contain 'original_x' - the original input tensor\n\n    Returns:\n        Reconstructed tensor in model space\n    \"\"\"\n    assert \"original_x\" in kwargs, \"MOLT decode requires 'original_x' in kwargs\"\n\n    x = kwargs[\"original_x\"]\n\n    # Choose decoding strategy based on distributed setup\n    is_distributed = any(\n        isinstance(self.U_matrices[str(rank)], DTensor)\n        for rank in self.cfg.available_ranks\n        if str(rank) in self.U_matrices\n    )\n\n    if is_distributed:\n        reconstruction = self._decode_distributed(feature_acts, x)\n    else:\n        reconstruction = self._decode_single_gpu(feature_acts, x)\n\n    return reconstruction\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.compute_training_metrics","title":"compute_training_metrics","text":"<pre><code>compute_training_metrics(\n    *, l0: Tensor, feature_acts: Tensor, **kwargs\n) -&gt; dict[str, float]\n</code></pre> <p>Compute per-rank group training metrics for MOLT.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>@override\n@torch.no_grad()\ndef compute_training_metrics(\n    self,\n    *,\n    l0: torch.Tensor,\n    feature_acts: torch.Tensor,\n    **kwargs,\n) -&gt; dict[str, float]:\n    \"\"\"Compute per-rank group training metrics for MOLT.\"\"\"\n    metrics = {}\n    feature_idx = 0\n    total_rank_sum = 0.0\n\n    for rank in self.cfg.available_ranks:\n        rank_str = str(rank)\n        if rank_str in self.U_matrices:\n            # Extract features for this rank group\n            end_idx = (\n                feature_idx + self.cfg.rank_counts[rank]\n            )  # rank_counts[rank] is the GLOBAL count of this rank group\n            rank_features = feature_acts[..., feature_idx:end_idx]\n\n            # Count active transforms (l0) for this rank group\n            rank_l0 = (rank_features &gt; 0).float().sum(-1)\n            rank_l0_mean = item(rank_l0.mean())\n\n            # Record metrics\n            metrics[f\"molt_metrics/l0_rank{rank}\"] = rank_l0_mean\n            metrics[f\"molt_metrics/l0_rank{rank}_ratio\"] = rank_l0_mean / self.cfg.rank_counts[rank]\n            total_rank_sum += rank_l0_mean * rank\n\n            feature_idx += self.cfg.rank_counts[rank]\n\n    # Record total rank sum\n    metrics[\"molt_metrics/total_rank_sum\"] = total_rank_sum\n    return metrics\n</code></pre>"},{"location":"reference/models/#lm_saes.MixtureOfLinearTransform.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"],\n    encoder_kwargs: dict[str, Any] = {},\n    decoder_kwargs: dict[str, Any] = {},\n) -&gt; (\n    Float[Tensor, \"batch d_model\"]\n    | Float[Tensor, \"batch seq_len d_model\"]\n)\n</code></pre> <p>Forward pass through the autoencoder. Ensure that the input activations are normalized by calling <code>normalize_activations</code> before calling this method.</p> Source code in <code>src/lm_saes/molt.py</code> <pre><code>@override\n@timer.time(\"forward\")\ndef forward(\n    self,\n    x: Union[\n        Float[torch.Tensor, \"batch d_model\"],\n        Float[torch.Tensor, \"batch seq_len d_model\"],\n    ],\n    encoder_kwargs: dict[str, Any] = {},\n    decoder_kwargs: dict[str, Any] = {},\n) -&gt; Union[\n    Float[torch.Tensor, \"batch d_model\"],\n    Float[torch.Tensor, \"batch seq_len d_model\"],\n]:\n    \"\"\"Forward pass through the autoencoder.\n    Ensure that the input activations are normalized by calling `normalize_activations` before calling this method.\n    \"\"\"\n    feature_acts = self.encode(x, **encoder_kwargs)\n    reconstructed = self.decode(feature_acts, original_x=x)\n    return reconstructed\n</code></pre>"},{"location":"reference/runners/","title":"Runners","text":""},{"location":"reference/runners/#runners","title":"Runners","text":"<p>High-level runner functions and their settings for common workflows.</p>"},{"location":"reference/runners/#lm_saes.PretrainedSAE","title":"PretrainedSAE  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModelConfig</code></p> <p>Fields:</p> <ul> <li> <code>device</code>                 (<code>str</code>)             </li> <li> <code>dtype</code>                 (<code>dtype</code>)             </li> <li> <code>pretrained_name_or_path</code>                 (<code>str</code>)             </li> <li> <code>fold_activation_scale</code>                 (<code>bool</code>)             </li> <li> <code>strict_loading</code>                 (<code>bool</code>)             </li> </ul>"},{"location":"reference/runners/#lm_saes.TrainSAESettings","title":"TrainSAESettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for training a Sparse Autoencoder (SAE).</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: BaseSAEConfig | PretrainedSAE\n</code></pre> <p>Configuration for the SAE model architecture and parameters, or the path to a pretrained SAE.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.initializer","title":"initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>initializer: InitializerConfig | None = None\n</code></pre> <p>Configuration for model initialization. Should be None for a pretrained SAE.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: TrainerConfig\n</code></pre> <p>Configuration for training process</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.eval","title":"eval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval: bool = False\n</code></pre> <p>Whether to run in evaluation mode</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre> <p>Size of data parallel mesh</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for MongoDB</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the model/tokenizer to load.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainSAESettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.train_sae","title":"train_sae","text":"<pre><code>train_sae(settings: TrainSAESettings) -&gt; None\n</code></pre> <p>Train a SAE model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>TrainSAESettings</code> <p>Configuration settings for SAE training</p> required Source code in <code>src/lm_saes/runners/train.py</code> <pre><code>def train_sae(settings: TrainSAESettings) -&gt; None:\n    \"\"\"Train a SAE model.\n\n    Args:\n        settings: Configuration settings for SAE training\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(settings.data_parallel_size, settings.model_parallel_size),\n            mesh_dim_names=(\"data\", \"model\"),\n        )\n        if settings.model_parallel_size &gt; 1 or settings.data_parallel_size &gt; 1\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n    if mongo_client:\n        logger.info(\"MongoDB client initialized\")\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    # Load model and datasets\n    logger.info(\"Loading model and datasets\")\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory = ActivationFactory(settings.activation_factory, device_mesh=device_mesh)\n\n    logger.info(\"Processing activations stream\")\n    activations_stream = activation_factory.process(\n        model=model,\n        model_name=settings.model_name,\n        datasets=datasets,\n    )\n\n    logger.info(\"Initializing SAE\")\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n            resume=settings.wandb.wandb_resume,\n            id=settings.wandb.wandb_run_id,\n        )\n        if settings.wandb is not None and (device_mesh is None or mesh_rank(device_mesh) == 0)\n        else None\n    )\n\n    assert settings.initializer is None or not isinstance(settings.initializer, str), (\n        \"Cannot use an initializer for a pretrained SAE\"\n    )\n    if isinstance(settings.sae, PretrainedSAE):\n        sae = AbstractSparseAutoEncoder.from_pretrained(\n            settings.sae.pretrained_name_or_path,\n            device_mesh=device_mesh,\n            fold_activation_scale=settings.sae.fold_activation_scale,\n            strict_loading=settings.sae.strict_loading,\n            device=settings.sae.device,\n            dtype=settings.sae.dtype,\n        )\n    elif settings.initializer is not None:\n        initializer = Initializer(settings.initializer)\n        sae = initializer.initialize_sae_from_config(\n            settings.sae,\n            activation_stream=activations_stream,\n            device_mesh=device_mesh,\n            wandb_logger=wandb_logger,\n            model=model,\n        )\n    else:\n        sae = AbstractSparseAutoEncoder.from_config(settings.sae, device_mesh=device_mesh)\n\n    if settings.trainer.from_pretrained_path is not None:\n        trainer = Trainer.from_checkpoint(\n            sae,\n            settings.trainer.from_pretrained_path,\n        )\n        trainer.wandb_logger = wandb_logger\n    else:\n        trainer = Trainer(settings.trainer)\n\n    logger.info(f\"SAE initialized: {type(sae).__name__}\")\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    # TODO: implement eval_fn\n    eval_fn = (lambda x: None) if settings.eval else None\n\n    logger.info(\"Starting training\")\n\n    sae.cfg.save_hyperparameters(settings.trainer.exp_result_path)\n    end_of_stream = trainer.fit(\n        sae=sae, activation_stream=activations_stream, eval_fn=eval_fn, wandb_logger=wandb_logger\n    )\n    logger.info(\"Training completed, saving model\")\n    if end_of_stream:\n        trainer.save_checkpoint(\n            sae=sae,\n            checkpoint_path=settings.trainer.exp_result_path,\n        )\n    else:\n        sae.save_pretrained(\n            save_path=settings.trainer.exp_result_path,\n        )\n        if is_primary_rank(device_mesh) and mongo_client is not None:\n            assert settings.sae_name is not None and settings.sae_series is not None, (\n                \"sae_name and sae_series must be provided when saving to MongoDB\"\n            )\n            mongo_client.create_sae(\n                name=settings.sae_name,\n                series=settings.sae_series,\n                path=str(Path(settings.trainer.exp_result_path).absolute()),\n                cfg=sae.cfg,\n            )\n\n    if wandb_logger is not None:\n        wandb_logger.finish()\n        logger.info(\"WandB session closed\")\n\n    logger.info(\"SAE training completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings","title":"TrainCLTSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for training a Cross Layer Transcoder (CLT). CLT works with multiple layers and their corresponding hook points.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: CLTConfig | PretrainedSAE\n</code></pre> <p>Configuration for the CLT model architecture and parameters, or the path to a pretrained CLT.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.initializer","title":"initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>initializer: InitializerConfig | None = None\n</code></pre> <p>Configuration for model initialization</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: TrainerConfig\n</code></pre> <p>Configuration for training process</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.eval","title":"eval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval: bool = False\n</code></pre> <p>Whether to run in evaluation mode</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre> <p>Size of data parallel mesh</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for MongoDB</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the tokenizer to load. CLT requires a tokenizer to get the modality indices.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainCLTSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.train_clt","title":"train_clt","text":"<pre><code>train_clt(settings: TrainCLTSettings) -&gt; None\n</code></pre> <p>Train a Cross Layer Transcoder (CLT) model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>TrainCLTSettings</code> <p>Configuration settings for CLT training</p> required Source code in <code>src/lm_saes/runners/train.py</code> <pre><code>def train_clt(settings: TrainCLTSettings) -&gt; None:\n    \"\"\"Train a Cross Layer Transcoder (CLT) model.\n\n    Args:\n        settings: Configuration settings for CLT training\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(settings.data_parallel_size, settings.model_parallel_size),\n            mesh_dim_names=(\"data\", \"model\"),\n        )\n        if settings.model_parallel_size &gt; 1 or settings.data_parallel_size &gt; 1\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n    if mongo_client:\n        logger.info(\"MongoDB client initialized\")\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    # Load model and datasets\n    logger.info(\"Loading model and datasets\")\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory = ActivationFactory(settings.activation_factory, device_mesh=device_mesh)\n\n    logger.info(\"Processing activations stream\")\n    activations_stream = activation_factory.process(\n        model=model,\n        model_name=settings.model_name,\n        datasets=datasets,\n    )\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n            resume=settings.wandb.wandb_resume,\n            id=settings.wandb.wandb_run_id,\n        )\n        if settings.wandb is not None and (device_mesh is None or mesh_rank(device_mesh) == 0)\n        else None\n    )\n\n    logger.info(\"Initializing CLT\")\n    assert settings.initializer is None or not isinstance(settings.initializer, str), (\n        \"Cannot use an initializer for a pretrained CLT\"\n    )\n    if isinstance(settings.sae, PretrainedSAE):\n        sae = AbstractSparseAutoEncoder.from_pretrained(\n            settings.sae.pretrained_name_or_path,\n            device_mesh=device_mesh,\n            fold_activation_scale=settings.sae.fold_activation_scale,\n            strict_loading=settings.sae.strict_loading,\n            device=settings.sae.device,\n            dtype=settings.sae.dtype,\n        )\n    elif settings.initializer is not None:\n        initializer = Initializer(settings.initializer)\n        sae = initializer.initialize_sae_from_config(\n            settings.sae,\n            activation_stream=activations_stream,\n            device_mesh=device_mesh,\n            wandb_logger=wandb_logger,\n            model=model,\n        )\n    else:\n        sae = AbstractSparseAutoEncoder.from_config(settings.sae, device_mesh=device_mesh)\n\n    n_params = sum(p.numel() for p in sae.parameters())\n    logger.info(f\"CLT initialized with {n_params / 1e9:.2f}B parameters\")\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    # TODO: implement eval_fn\n    eval_fn = (lambda x: None) if settings.eval else None\n\n    logger.info(\"Starting CLT training\")\n    if settings.trainer.from_pretrained_path is not None:\n        trainer = Trainer.from_checkpoint(\n            sae,\n            settings.trainer.from_pretrained_path,\n        )\n        trainer.wandb_logger = wandb_logger\n    else:\n        trainer = Trainer(settings.trainer)\n    sae.cfg.save_hyperparameters(settings.trainer.exp_result_path)\n    end_of_stream = trainer.fit(\n        sae=sae, activation_stream=activations_stream, eval_fn=eval_fn, wandb_logger=wandb_logger\n    )\n\n    logger.info(\"Training completed, saving CLT model\")\n    if end_of_stream:\n        trainer.save_checkpoint(\n            sae=sae,\n            checkpoint_path=settings.trainer.exp_result_path,\n        )\n    else:\n        sae.save_pretrained(\n            save_path=settings.trainer.exp_result_path,\n        )\n        if is_primary_rank(device_mesh) and mongo_client is not None:\n            assert settings.sae_name is not None and settings.sae_series is not None, (\n                \"sae_name and sae_series must be provided when saving to MongoDB\"\n            )\n            mongo_client.create_sae(\n                name=settings.sae_name,\n                series=settings.sae_series,\n                path=str(Path(settings.trainer.exp_result_path).absolute()),\n                cfg=sae.cfg,\n            )\n\n    if wandb_logger is not None:\n        wandb_logger.finish()\n        logger.info(\"WandB session closed\")\n\n    logger.info(\"CLT training completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings","title":"TrainCrossCoderSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for training a CrossCoder. The main difference to TrainSAESettings is that the activation factory is a list of ActivationFactoryConfig, one for each head.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: CrossCoderConfig | PretrainedSAE\n</code></pre> <p>Configuration for the CrossCoder model architecture and parameters, or the path to a pretrained CrossCoder.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.initializer","title":"initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>initializer: InitializerConfig | None = None\n</code></pre> <p>Configuration for model initialization</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: TrainerConfig\n</code></pre> <p>Configuration for training process</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.activation_factories","title":"activation_factories  <code>instance-attribute</code>","text":"<pre><code>activation_factories: list[ActivationFactoryConfig]\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.eval","title":"eval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval: bool = False\n</code></pre> <p>Whether to run in evaluation mode</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre> <p>Size of data parallel mesh</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for MongoDB</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the model/tokenizer to load.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainCrossCoderSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.train_crosscoder","title":"train_crosscoder","text":"<pre><code>train_crosscoder(settings: TrainCrossCoderSettings) -&gt; None\n</code></pre> <p>Train a CrossCoder.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>TrainCrossCoderSettings</code> <p>Configuration settings for SAE training</p> required Source code in <code>src/lm_saes/runners/train.py</code> <pre><code>def train_crosscoder(settings: TrainCrossCoderSettings) -&gt; None:\n    \"\"\"Train a CrossCoder.\n\n    Args:\n        settings: Configuration settings for SAE training\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    assert isinstance(settings.sae, CrossCoderConfig), \"CrossCoderConfig is required for training a CrossCoder\"\n    assert all(\n        len(activation_factory.hook_points) == len(settings.activation_factories[0].hook_points)\n        for activation_factory in settings.activation_factories\n    ), \"Number of hook points of activation factories must be the same\"\n    assert (\n        len(settings.activation_factories) * len(settings.activation_factories[0].hook_points) == settings.sae.n_heads\n    ), \"Total number of hook points must match the number of heads in the CrossCoder\"\n    head_parallel_size = len(settings.activation_factories)\n\n    device_mesh = init_device_mesh(\n        device_type=settings.device_type,\n        mesh_shape=(settings.data_parallel_size, head_parallel_size, settings.model_parallel_size),\n        mesh_dim_names=(\"data\", \"head\", \"model\"),\n    )\n\n    logger.info(\n        f\"Device mesh initialized with {settings.sae.n_heads} heads, {head_parallel_size} head parallel size, {settings.data_parallel_size} data parallel size, {settings.model_parallel_size} model parallel size\"\n    )\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n    if mongo_client:\n        logger.info(\"MongoDB client initialized\")\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    # Load model and datasets\n    logger.info(\"Loading model and datasets\")\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory_mesh = device_mesh[\n        \"data\", \"model\"\n    ]  # Remove the head dimension, since each activation factory should only be responsible for a subset of the heads.\n\n    logger.info(\"Setting up activation factory for CrossCoder\")\n    activation_factory = ActivationFactory(\n        settings.activation_factories[device_mesh.get_local_rank(\"head\")], device_mesh=activation_factory_mesh\n    )\n\n    logger.info(\"Processing activations stream\")\n    activations_stream = activation_factory.process(\n        model=model,\n        model_name=settings.model_name,\n        datasets=datasets,\n    )\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n            resume=settings.wandb.wandb_resume,\n            id=settings.wandb.wandb_run_id,\n        )\n        if settings.wandb is not None and (device_mesh is None or mesh_rank(device_mesh) == 0)\n        else None\n    )\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    logger.info(\"Initializing CrossCoder\")\n    assert settings.initializer is None or not isinstance(settings.initializer, str), (\n        \"Cannot use an initializer for a pretrained CrossCoder\"\n    )\n    if isinstance(settings.sae, PretrainedSAE):\n        sae = AbstractSparseAutoEncoder.from_pretrained(\n            settings.sae.pretrained_name_or_path,\n            device_mesh=device_mesh,\n            fold_activation_scale=settings.sae.fold_activation_scale,\n            strict_loading=settings.sae.strict_loading,\n            device=settings.sae.device,\n            dtype=settings.sae.dtype,\n        )\n    elif settings.initializer is not None:\n        initializer = Initializer(settings.initializer)\n        sae = initializer.initialize_sae_from_config(\n            settings.sae,\n            activation_stream=activations_stream,\n            device_mesh=device_mesh,\n            wandb_logger=wandb_logger,\n            model=model,\n        )\n    else:\n        sae = AbstractSparseAutoEncoder.from_config(settings.sae, device_mesh=device_mesh)\n\n    logger.info(\"CrossCoder initialized\")\n\n    # TODO: implement eval_fn\n    eval_fn = (lambda x: None) if settings.eval else None\n\n    logger.info(\"Starting CrossCoder training\")\n    if settings.trainer.from_pretrained_path is not None:\n        trainer = Trainer.from_checkpoint(\n            sae,\n            settings.trainer.from_pretrained_path,\n        )\n        trainer.wandb_logger = wandb_logger\n    else:\n        trainer = Trainer(settings.trainer)\n\n    sae.cfg.save_hyperparameters(settings.trainer.exp_result_path)\n    end_of_stream = trainer.fit(\n        sae=sae, activation_stream=activations_stream, eval_fn=eval_fn, wandb_logger=wandb_logger\n    )\n\n    logger.info(\"Training completed, saving CrossCoder\")\n    if end_of_stream:\n        trainer.save_checkpoint(\n            sae=sae,\n            checkpoint_path=settings.trainer.exp_result_path,\n        )\n    else:\n        sae.save_pretrained(\n            save_path=settings.trainer.exp_result_path,\n        )\n        if is_primary_rank(device_mesh) and mongo_client is not None:\n            assert settings.sae_name is not None and settings.sae_series is not None, (\n                \"sae_name and sae_series must be provided when saving to MongoDB\"\n            )\n            mongo_client.create_sae(\n                name=settings.sae_name,\n                series=settings.sae_series,\n                path=str(Path(settings.trainer.exp_result_path).absolute()),\n                cfg=settings.sae,\n            )\n\n    if wandb_logger is not None:\n        wandb_logger.finish()\n        logger.info(\"WandB session closed\")\n\n    logger.info(\"CrossCoder training completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings","title":"TrainLorsaSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for training a Lorsa (Low-Rank Sparse Autoencoder) model.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: LorsaConfig | PretrainedSAE\n</code></pre> <p>Configuration for the Lorsa model architecture and parameters, or the path to a pretrained Lorsa.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the Lorsa model. Use as identifier for the Lorsa model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the Lorsa model. Use as identifier for the Lorsa model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.initializer","title":"initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>initializer: InitializerConfig | None = None\n</code></pre> <p>Configuration for model initialization</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: TrainerConfig\n</code></pre> <p>Configuration for training process</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.eval","title":"eval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval: bool = False\n</code></pre> <p>Whether to run in evaluation mode</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre> <p>Size of data parallel mesh</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for MongoDB</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the model/tokenizer to load.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainLorsaSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.train_lorsa","title":"train_lorsa","text":"<pre><code>train_lorsa(settings: TrainLorsaSettings) -&gt; None\n</code></pre> <p>Train a LORSA (Low-Rank Sparse Autoencoder) model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>TrainLorsaSettings</code> <p>Configuration settings for LORSA training</p> required Source code in <code>src/lm_saes/runners/train.py</code> <pre><code>def train_lorsa(settings: TrainLorsaSettings) -&gt; None:\n    \"\"\"Train a LORSA (Low-Rank Sparse Autoencoder) model.\n\n    Args:\n        settings: Configuration settings for LORSA training\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(settings.data_parallel_size, settings.model_parallel_size),\n            mesh_dim_names=(\"data\", \"model\"),\n        )\n        if settings.model_parallel_size &gt; 1 or settings.data_parallel_size &gt; 1\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n    if mongo_client:\n        logger.info(\"MongoDB client initialized\")\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    # Load model and datasets\n    logger.info(\"Loading model and datasets\")\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory = ActivationFactory(settings.activation_factory, device_mesh=device_mesh)\n\n    logger.info(\"Processing activations stream\")\n    activations_stream = activation_factory.process(\n        model=model,\n        model_name=settings.model_name,\n        datasets=datasets,\n    )\n\n    logger.info(\"Initializing lorsa\")\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n            resume=settings.wandb.wandb_resume,\n            id=settings.wandb.wandb_run_id,\n        )\n        if settings.wandb is not None and (device_mesh is None or device_mesh.get_rank() == 0)\n        else None\n    )\n\n    assert settings.initializer is None or not isinstance(settings.initializer, str), (\n        \"Cannot use an initializer for a pretrained Lorsa\"\n    )\n    if isinstance(settings.sae, PretrainedSAE):\n        sae = AbstractSparseAutoEncoder.from_pretrained(\n            settings.sae.pretrained_name_or_path,\n            device_mesh=device_mesh,\n            fold_activation_scale=settings.sae.fold_activation_scale,\n            strict_loading=settings.sae.strict_loading,\n            device=settings.sae.device,\n            dtype=settings.sae.dtype,\n        )\n    elif settings.initializer is not None:\n        initializer = Initializer(settings.initializer)\n        sae = initializer.initialize_sae_from_config(\n            settings.sae,\n            activation_stream=activations_stream,\n            device_mesh=device_mesh,\n            wandb_logger=wandb_logger,\n            model=model,\n        )\n    else:\n        sae = AbstractSparseAutoEncoder.from_config(settings.sae, device_mesh=device_mesh)\n\n    n_params = sum(p.numel() for p in sae.parameters())\n    logger.info(f\"lorsa initialized with {n_params / 1e9:.2f}B parameters\")\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    # TODO: implement eval_fn\n    eval_fn = (lambda x: None) if settings.eval else None\n\n    logger.info(\"Starting LORSA training\")\n    if settings.trainer.from_pretrained_path is not None:\n        trainer = Trainer.from_checkpoint(\n            sae,\n            settings.trainer.from_pretrained_path,\n        )\n        trainer.wandb_logger = wandb_logger\n    else:\n        trainer = Trainer(settings.trainer)\n\n    sae.cfg.save_hyperparameters(settings.trainer.exp_result_path)\n    end_of_stream = trainer.fit(\n        sae=sae, activation_stream=activations_stream, eval_fn=eval_fn, wandb_logger=wandb_logger\n    )\n\n    logger.info(\"Training completed, saving LORSA model\")\n    if end_of_stream:\n        trainer.save_checkpoint(\n            sae=sae,\n            checkpoint_path=settings.trainer.exp_result_path,\n        )\n    else:\n        sae.save_pretrained(\n            save_path=settings.trainer.exp_result_path,\n        )\n        if is_primary_rank(device_mesh) and mongo_client is not None:\n            assert settings.sae_name is not None and settings.sae_series is not None, (\n                \"sae_name and sae_series must be provided when saving to MongoDB\"\n            )\n            mongo_client.create_sae(\n                name=settings.sae_name,\n                series=settings.sae_series,\n                path=str(Path(settings.trainer.exp_result_path).absolute()),\n                cfg=sae.cfg,\n            )\n\n    if wandb_logger is not None:\n        wandb_logger.finish()\n        logger.info(\"WandB session closed\")\n\n    logger.info(\"LORSA training completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings","title":"TrainMOLTSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for training a Mixture of Linear Transforms (MOLT). MOLT is a more efficient alternative to transcoders that sparsely replaces MLP computation in transformers.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: MOLTConfig | PretrainedSAE\n</code></pre> <p>Configuration for the MOLT model architecture and parameters</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.initializer","title":"initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>initializer: InitializerConfig | None = None\n</code></pre> <p>Configuration for model initialization. Should be None for a pretrained MOLT.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: TrainerConfig\n</code></pre> <p>Configuration for training process</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.eval","title":"eval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval: bool = False\n</code></pre> <p>Whether to run in evaluation mode</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre> <p>Size of data parallel mesh</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for MongoDB</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the tokenizer to load. MOLT requires a tokenizer to get the modality indices.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.TrainMOLTSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.train_molt","title":"train_molt","text":"<pre><code>train_molt(settings: TrainMOLTSettings) -&gt; None\n</code></pre> <p>Train a Mixture of Linear Transforms (MOLT) model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>TrainMOLTSettings</code> <p>Configuration settings for MOLT training</p> required Source code in <code>src/lm_saes/runners/train.py</code> <pre><code>def train_molt(settings: TrainMOLTSettings) -&gt; None:\n    \"\"\"Train a Mixture of Linear Transforms (MOLT) model.\n\n    Args:\n        settings: Configuration settings for MOLT training\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(settings.model_parallel_size, settings.data_parallel_size),  # TODO: check the order\n            mesh_dim_names=(\"model\", \"data\"),\n        )\n        if settings.model_parallel_size &gt; 1 or settings.data_parallel_size &gt; 1\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n    if mongo_client:\n        logger.info(\"MongoDB client initialized\")\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    # Load model and datasets\n    logger.info(\"Loading model and datasets\")\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory = ActivationFactory(settings.activation_factory, device_mesh=device_mesh)\n\n    logger.info(\"Processing activations stream\")\n    activations_stream = activation_factory.process(\n        model=model,\n        model_name=settings.model_name,\n        datasets=datasets,\n    )\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n            resume=settings.wandb.wandb_resume,\n            id=settings.wandb.wandb_run_id,\n        )\n        if settings.wandb is not None and (device_mesh is None or mesh_rank(device_mesh) == 0)\n        else None\n    )\n\n    logger.info(\"Initializing MOLT\")\n\n    assert settings.initializer is None or not isinstance(settings.initializer, str), (\n        \"Cannot use an initializer for a pretrained MOLT\"\n    )\n    if isinstance(settings.sae, PretrainedSAE):\n        sae = AbstractSparseAutoEncoder.from_pretrained(\n            settings.sae.pretrained_name_or_path,\n            device_mesh=device_mesh,\n            fold_activation_scale=settings.sae.fold_activation_scale,\n            strict_loading=settings.sae.strict_loading,\n            device=settings.sae.device,\n            dtype=settings.sae.dtype,\n        )\n    elif settings.initializer is not None:\n        initializer = Initializer(settings.initializer)\n        sae = initializer.initialize_sae_from_config(\n            settings.sae,\n            activation_stream=activations_stream,\n            device_mesh=device_mesh,\n            wandb_logger=wandb_logger,\n            model=model,\n        )\n    else:\n        sae = AbstractSparseAutoEncoder.from_config(settings.sae, device_mesh=device_mesh)\n\n    logger.info(f\"MOLT initialized: {type(sae).__name__}\")\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    # TODO: implement eval_fn\n    eval_fn = (lambda x: None) if settings.eval else None\n\n    logger.info(\"Starting MOLT training\")\n    if settings.trainer.from_pretrained_path is not None:\n        trainer = Trainer.from_checkpoint(\n            sae,\n            settings.trainer.from_pretrained_path,\n        )\n        trainer.wandb_logger = wandb_logger\n    else:\n        trainer = Trainer(settings.trainer)\n\n    sae.cfg.save_hyperparameters(settings.trainer.exp_result_path)\n    end_of_stream = trainer.fit(\n        sae=sae, activation_stream=activations_stream, eval_fn=eval_fn, wandb_logger=wandb_logger\n    )\n\n    logger.info(\"Training completed, saving MOLT model\")\n    if end_of_stream:\n        trainer.save_checkpoint(\n            sae=sae,\n            checkpoint_path=settings.trainer.exp_result_path,\n        )\n    else:\n        sae.save_pretrained(\n            save_path=settings.trainer.exp_result_path,\n        )\n        if is_primary_rank(device_mesh) and mongo_client is not None:\n            assert settings.sae_name is not None and settings.sae_series is not None, (\n                \"sae_name and sae_series must be provided when saving to MongoDB\"\n            )\n            mongo_client.create_sae(\n                name=settings.sae_name,\n                series=settings.sae_series,\n                path=str(Path(settings.trainer.exp_result_path).absolute()),\n                cfg=sae.cfg,\n            )\n\n    if wandb_logger is not None:\n        wandb_logger.finish()\n        logger.info(\"WandB session closed\")\n\n    logger.info(\"MOLT training completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings","title":"EvaluateSAESettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for evaluating a Sparse Autoencoder.</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: PretrainedSAE\n</code></pre> <p>Path to a pretrained SAE model</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.eval","title":"eval  <code>instance-attribute</code>","text":"<pre><code>eval: EvalConfig\n</code></pre> <p>Configuration for evaluation</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.fold_activation_scale","title":"fold_activation_scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_activation_scale: bool = False\n</code></pre> <p>Whether to fold the activation scale.</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.EvaluateSAESettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.evaluate_sae","title":"evaluate_sae","text":"<pre><code>evaluate_sae(settings: EvaluateSAESettings) -&gt; None\n</code></pre> <p>Evaluate a SAE model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>EvaluateSAESettings</code> <p>Configuration settings for SAE evaluation</p> required Source code in <code>src/lm_saes/runners/eval.py</code> <pre><code>def evaluate_sae(settings: EvaluateSAESettings) -&gt; None:\n    \"\"\"Evaluate a SAE model.\n\n    Args:\n        settings: Configuration settings for SAE evaluation\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(settings.model_parallel_size,),\n            mesh_dim_names=(\"model\",),\n        )\n        if settings.model_parallel_size &gt; 1\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    activation_factory = ActivationFactory(settings.activation_factory)\n\n    logger.info(\"Loading SAE model\")\n\n    sae = AbstractSparseAutoEncoder.from_pretrained(\n        settings.sae.pretrained_name_or_path,\n        device_mesh=device_mesh,\n        fold_activation_scale=settings.fold_activation_scale,\n        device=settings.sae.device,\n        dtype=settings.sae.dtype,\n        strict_loading=settings.sae.strict_loading,\n    )\n\n    logger.info(f\"SAE model loaded: {type(sae).__name__}\")\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n        )\n        if settings.wandb is not None and (device_mesh is None or mesh_rank(device_mesh) == 0)\n        else None\n    )\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    logger.info(\"Processing activations for evaluation\")\n    activations = activation_factory.process()\n    evaluator = Evaluator(settings.eval)\n    evaluator.evaluate(sae, activations, wandb_logger)\n    logger.info(\"Evaluation completed\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings","title":"EvaluateCrossCoderSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for evaluating a CrossCoder model.</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: PretrainedSAE\n</code></pre> <p>Path to a pretrained CrossCoder model</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.activation_factories","title":"activation_factories  <code>instance-attribute</code>","text":"<pre><code>activation_factories: list[ActivationFactoryConfig]\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.eval","title":"eval  <code>instance-attribute</code>","text":"<pre><code>eval: EvalConfig\n</code></pre> <p>Configuration for evaluation</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.wandb","title":"wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.EvaluateCrossCoderSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.evaluate_crosscoder","title":"evaluate_crosscoder","text":"<pre><code>evaluate_crosscoder(\n    settings: EvaluateCrossCoderSettings,\n) -&gt; None\n</code></pre> <p>Evaluate a CrossCoder model. The key difference to evaluate_sae is that the activation factories are a list of ActivationFactoryConfig, one for each head; and the evaluating contains a device mesh transformation from head parallelism to model (feature) parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>EvaluateCrossCoderSettings</code> <p>Configuration settings for CrossCoder evaluation</p> required Source code in <code>src/lm_saes/runners/eval.py</code> <pre><code>@torch.no_grad()\ndef evaluate_crosscoder(settings: EvaluateCrossCoderSettings) -&gt; None:\n    \"\"\"Evaluate a CrossCoder model. The key difference to evaluate_sae is that the activation factories are a list of ActivationFactoryConfig, one for each head; and the evaluating contains a device mesh transformation from head parallelism to model (feature) parallelism.\n\n    Args:\n        settings: Configuration settings for CrossCoder evaluation\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    parallel_size = len(settings.activation_factories)\n\n    logger.info(f\"Analyzing CrossCoder with {parallel_size} parallel size\")\n\n    device_mesh = init_device_mesh(\n        device_type=settings.device_type,\n        mesh_shape=(parallel_size,),\n        mesh_dim_names=(\"head\",),\n    )\n\n    logger.info(\"Device meshes initialized for CrossCoder analysis\")\n\n    logger.info(\"Setting up activation factory for CrossCoder head\")\n    activation_factory = ActivationFactory(settings.activation_factories[device_mesh.get_local_rank(\"head\")])\n\n    logger.info(\"Loading CrossCoder model\")\n    sae = CrossCoder.from_pretrained(\n        settings.sae.pretrained_name_or_path,\n        device_mesh=device_mesh,\n        device=settings.sae.device,\n        dtype=settings.sae.dtype,\n        fold_activation_scale=settings.sae.fold_activation_scale,\n        strict_loading=settings.sae.strict_loading,\n    )\n\n    assert len(settings.activation_factories) * len(settings.activation_factories[0].hook_points) == sae.cfg.n_heads, (\n        \"Total number of hook points must match the number of heads in the CrossCoder\"\n    )\n\n    wandb_logger = (\n        wandb.init(\n            project=settings.wandb.wandb_project,\n            config=settings.model_dump(),\n            name=settings.wandb.exp_name,\n            entity=settings.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n        )\n        if settings.wandb is not None and (device_mesh is None or mesh_rank(device_mesh) == 0)\n        else None\n    )\n\n    if wandb_logger is not None:\n        logger.info(\"WandB logger initialized\")\n\n    logger.info(\"Processing activations for CrossCoder evaluation\")\n    activations = activation_factory.process()\n    evaluator = Evaluator(settings.eval)\n    evaluator.evaluate(sae, activations, wandb_logger)\n\n    logger.info(\"CrossCoder evaluation completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings","title":"AnalyzeSAESettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for analyzing a Sparse Autoencoder.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: PretrainedSAE\n</code></pre> <p>Configuration for the SAE model architecture and parameters</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the model/tokenizer to load.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer: FeatureAnalyzerConfig\n</code></pre> <p>Configuration for feature analysis</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.feature_analysis_name","title":"feature_analysis_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_analysis_name: str = 'default'\n</code></pre> <p>Name of the feature analysis.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for the MongoDB database.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.output_dir","title":"output_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_dir: Path | None = None\n</code></pre> <p>Directory to save analysis results. Only used if MongoDB client is not provided.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.AnalyzeSAESettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.analyze_sae","title":"analyze_sae","text":"<pre><code>analyze_sae(settings: AnalyzeSAESettings) -&gt; None\n</code></pre> <p>Analyze a SAE model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AnalyzeSAESettings</code> <p>Configuration settings for SAE analysis</p> required Source code in <code>src/lm_saes/runners/analyze.py</code> <pre><code>@torch.no_grad()\ndef analyze_sae(settings: AnalyzeSAESettings) -&gt; None:\n    \"\"\"Analyze a SAE model.\n\n    Args:\n        settings: Configuration settings for SAE analysis\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(settings.model_parallel_size,),\n            mesh_dim_names=(\"model\",),\n        )\n        if settings.model_parallel_size &gt; 1\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    mongo_client = None\n    if settings.mongo is not None:\n        mongo_client = MongoClient(settings.mongo)\n        logger.info(\"MongoDB client initialized\")\n    else:\n        assert settings.output_dir is not None, \"Output directory must be provided if MongoDB client is not provided\"\n        logger.info(f\"Analysis results will be saved to {settings.output_dir}\")\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory = ActivationFactory(settings.activation_factory, device_mesh=device_mesh)\n\n    sae = AbstractSparseAutoEncoder.from_pretrained(\n        settings.sae.pretrained_name_or_path,\n        device_mesh=device_mesh,\n        device=settings.sae.device,\n        dtype=settings.sae.dtype,\n        fold_activation_scale=settings.sae.fold_activation_scale,\n        strict_loading=settings.sae.strict_loading,\n    )\n\n    logger.info(f\"SAE model loaded: {type(sae).__name__}\")\n\n    analyzer = FeatureAnalyzer(settings.analyzer)\n    logger.info(\"Feature analyzer initialized\")\n\n    logger.info(\"Processing activations for analysis\")\n\n    with torch.amp.autocast(device_type=settings.device_type, dtype=settings.amp_dtype):\n        result = analyzer.analyze_chunk(\n            activation_factory,\n            sae=sae,\n            device_mesh=device_mesh,\n            activation_factory_process_kwargs={\n                \"model\": model,\n                \"model_name\": settings.model_name,\n                \"datasets\": datasets,\n            },\n        )\n\n    logger.info(\"Analysis completed, saving results\")\n    start_idx = 0 if device_mesh is None else device_mesh.get_local_rank(\"model\") * len(result)\n    if mongo_client is not None:\n        logger.info(\"Saving results to MongoDB\")\n        mongo_client.add_feature_analysis(\n            name=settings.feature_analysis_name,\n            sae_name=settings.sae_name,\n            sae_series=settings.sae_series,\n            analysis=result,\n            start_idx=start_idx,\n        )\n        logger.info(\"Results saved to MongoDB\")\n    else:\n        assert settings.output_dir is not None, \"Output directory must be set when MongoDB is not used\"\n        logger.info(f\"Saving results to output directory: {settings.output_dir}\")\n        pickle_path = save_analysis_to_file(\n            output_dir=settings.output_dir,\n            analysis_name=settings.feature_analysis_name,\n            sae_name=settings.sae_name,\n            sae_series=settings.sae_series,\n            analysis=result,\n            start_idx=start_idx,\n            device_mesh=device_mesh,\n        )\n        logger.info(f\"Results saved to: {pickle_path}\")\n\n    logger.info(\"SAE analysis completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings","title":"AnalyzeCrossCoderSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for analyzing a CrossCoder model.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: PretrainedSAE\n</code></pre> <p>Configuration for the CrossCoder model architecture and parameters</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.activation_factories","title":"activation_factories  <code>instance-attribute</code>","text":"<pre><code>activation_factories: list[ActivationFactoryConfig]\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.analyzer","title":"analyzer  <code>instance-attribute</code>","text":"<pre><code>analyzer: FeatureAnalyzerConfig\n</code></pre> <p>Configuration for feature analysis</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.amp_dtype","title":"amp_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>amp_dtype: dtype = bfloat16\n</code></pre> <p>The dtype to use for outputting activations. If <code>None</code>, will not override the dtype.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.feature_analysis_name","title":"feature_analysis_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_analysis_name: str = 'default'\n</code></pre> <p>Name of the feature analysis.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for the MongoDB database.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.output_dir","title":"output_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_dir: Path | None = None\n</code></pre> <p>Directory to save analysis results. Only used if MongoDB client is not provided.</p>"},{"location":"reference/runners/#lm_saes.AnalyzeCrossCoderSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.analyze_crosscoder","title":"analyze_crosscoder","text":"<pre><code>analyze_crosscoder(\n    settings: AnalyzeCrossCoderSettings,\n) -&gt; None\n</code></pre> <p>Analyze a CrossCoder model. The key difference to analyze_sae is that the activation factories are a list of ActivationFactoryConfig, one for each head; and the analyzing contains a device mesh transformation from head parallelism to model (feature) parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AnalyzeCrossCoderSettings</code> <p>Configuration settings for CrossCoder analysis</p> required Source code in <code>src/lm_saes/runners/analyze.py</code> <pre><code>@torch.no_grad()\ndef analyze_crosscoder(settings: AnalyzeCrossCoderSettings) -&gt; None:\n    \"\"\"Analyze a CrossCoder model. The key difference to analyze_sae is that the activation factories are a list of ActivationFactoryConfig, one for each head; and the analyzing contains a device mesh transformation from head parallelism to model (feature) parallelism.\n\n    Args:\n        settings: Configuration settings for CrossCoder analysis\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    parallel_size = len(settings.activation_factories)\n\n    logger.info(f\"Analyzing CrossCoder with {parallel_size} parallel size\")\n\n    crosscoder_device_mesh = init_device_mesh(\n        device_type=settings.device_type,\n        mesh_shape=(parallel_size,),\n        mesh_dim_names=(\"head\",),\n    )\n\n    device_mesh = init_device_mesh(\n        device_type=settings.device_type,\n        mesh_shape=(parallel_size,),\n        mesh_dim_names=(\"model\",),\n    )\n\n    logger.info(\"Device meshes initialized for CrossCoder analysis\")\n\n    mongo_client = None\n    if settings.mongo is not None:\n        mongo_client = MongoClient(settings.mongo)\n        logger.info(\"MongoDB client initialized\")\n    else:\n        assert settings.output_dir is not None, \"Output directory must be provided if MongoDB client is not provided\"\n        logger.info(f\"Analysis results will be saved to: {settings.output_dir}\")\n\n    logger.info(\"Setting up activation factory for CrossCoder head\")\n    activation_factory = ActivationFactory(settings.activation_factories[crosscoder_device_mesh.get_local_rank(\"head\")])\n\n    logger.info(\"Loading CrossCoder model\")\n    sae = CrossCoder.from_pretrained(\n        settings.sae.pretrained_name_or_path,\n        device_mesh=crosscoder_device_mesh,\n        device=settings.sae.device,\n        dtype=settings.sae.dtype,\n        fold_activation_scale=settings.sae.fold_activation_scale,\n        strict_loading=settings.sae.strict_loading,\n    )\n\n    assert len(settings.activation_factories) * len(settings.activation_factories[0].hook_points) == sae.cfg.n_heads, (\n        \"Total number of hook points must match the number of heads in the CrossCoder\"\n    )\n\n    logger.info(\"Feature analyzer initialized\")\n    analyzer = FeatureAnalyzer(settings.analyzer)\n\n    logger.info(\"Processing activations for CrossCoder analysis\")\n\n    with torch.amp.autocast(device_type=settings.device_type, dtype=settings.amp_dtype):\n        result = analyzer.analyze_chunk(\n            activation_factory,\n            sae=sae,\n            device_mesh=device_mesh,\n        )\n\n    logger.info(\"CrossCoder analysis completed, saving results to MongoDB\")\n    start_idx = 0 if device_mesh is None else device_mesh.get_local_rank(\"model\") * len(result)\n    if mongo_client is not None:\n        mongo_client.add_feature_analysis(\n            name=settings.feature_analysis_name,\n            sae_name=settings.sae_name,\n            sae_series=settings.sae_series,\n            analysis=result,\n            start_idx=start_idx,\n        )\n    else:\n        assert settings.output_dir is not None, \"Output directory must be set when MongoDB is not used\"\n        logger.info(f\"Saving results to output directory: {settings.output_dir}\")\n        pickle_path = save_analysis_to_file(\n            output_dir=settings.output_dir,\n            analysis_name=settings.feature_analysis_name,\n            sae_name=settings.sae_name,\n            sae_series=settings.sae_series,\n            analysis=result,\n            start_idx=start_idx,\n            device_mesh=device_mesh,\n        )\n        logger.info(f\"Results saved to: {pickle_path}\")\n\n    logger.info(\"CrossCoder analysis completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings","title":"GenerateActivationsSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for activation generation.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for loading the language model. If <code>None</code>, will read from the database.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Name of the model to load. Use as identifier for the model in the database.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.dataset","title":"dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset: DatasetConfig | None = None\n</code></pre> <p>Configuration for loading the dataset. If <code>None</code>, will read from the database.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.dataset_name","title":"dataset_name  <code>instance-attribute</code>","text":"<pre><code>dataset_name: str\n</code></pre> <p>Name of the dataset. Use as identifier for the dataset in the database.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.hook_points","title":"hook_points  <code>instance-attribute</code>","text":"<pre><code>hook_points: list[str]\n</code></pre> <p>List of model hook points to capture activations from</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.output_dir","title":"output_dir  <code>instance-attribute</code>","text":"<pre><code>output_dir: str\n</code></pre> <p>Directory to save activation files</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.target","title":"target  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target: ActivationFactoryTarget = ACTIVATIONS_2D\n</code></pre> <p>Target type for activation generation</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.model_batch_size","title":"model_batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_batch_size: int = 1\n</code></pre> <p>Batch size for model forward</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size: int\n</code></pre> <p>Size of the batch for activation generation</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.buffer_size","title":"buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer_size: int | None = None\n</code></pre> <p>Size of the buffer for activation generation</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.buffer_shuffle","title":"buffer_shuffle  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer_shuffle: BufferShuffleConfig | None = None\n</code></pre> <p>\"Manual seed and device of generator for generating randomperm in buffer</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.total_tokens","title":"total_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_tokens: int | None = None\n</code></pre> <p>Optional total number of tokens to generate</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.context_size","title":"context_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>context_size: int = 128\n</code></pre> <p>Context window size for tokenization</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.n_samples_per_chunk","title":"n_samples_per_chunk  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_samples_per_chunk: int | None = None\n</code></pre> <p>Number of samples per saved chunk</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int | None = None\n</code></pre> <p>Number of workers for parallel writing</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format: Literal['pt', 'safetensors'] = 'safetensors'\n</code></pre> <p>Format to save activations in ('pt' or 'safetensors')</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.n_shards","title":"n_shards  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_shards: int | None = None\n</code></pre> <p>Number of shards to split the dataset into. If None, the dataset is split to the world size. Must be larger than the world size.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.start_shard","title":"start_shard  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_shard: int = 0\n</code></pre> <p>The shard to start writing from</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for the MongoDB database. If <code>None</code>, will not use the database.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.ignore_token_ids","title":"ignore_token_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ignore_token_ids: list[int] | None = None\n</code></pre> <p>Tokens to ignore in the activations.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.override_dtype","title":"override_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>override_dtype: dtype | None = None\n</code></pre> <p>Dtype to override the activations to. If <code>None</code>, will not override the dtype.</p>"},{"location":"reference/runners/#lm_saes.GenerateActivationsSettings.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context: dict) -&gt; None\n</code></pre> <p>Validate configuration after initialization.</p> Source code in <code>src/lm_saes/runners/generate.py</code> <pre><code>def model_post_init(self, __context: dict) -&gt; None:\n    \"\"\"Validate configuration after initialization.\"\"\"\n    if self.mongo is not None:\n        assert self.model is not None, \"Database not provided. Must manually provide model config.\"\n        assert self.dataset is not None, \"Database not provided. Must manually provide dataset config.\"\n</code></pre>"},{"location":"reference/runners/#lm_saes.generate_activations","title":"generate_activations","text":"<pre><code>generate_activations(\n    settings: GenerateActivationsSettings,\n) -&gt; None\n</code></pre> <p>Generate and save model activations from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>GenerateActivationsSettings</code> <p>Configuration settings for activation generation</p> required Source code in <code>src/lm_saes/runners/generate.py</code> <pre><code>def generate_activations(settings: GenerateActivationsSettings) -&gt; None:\n    \"\"\"Generate and save model activations from a dataset.\n\n    Args:\n        settings: Configuration settings for activation generation\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    # Initialize device mesh\n    device_mesh = (\n        init_device_mesh(\n            device_type=settings.device_type,\n            mesh_shape=(int(os.environ.get(\"WORLD_SIZE\", 1)), 1),\n            mesh_dim_names=(\"data\", \"model\"),\n        )\n        if os.environ.get(\"WORLD_SIZE\") is not None\n        else None\n    )\n\n    logger.info(f\"Device mesh initialized: {device_mesh}\")\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n    if mongo_client:\n        logger.info(\"MongoDB client initialized\")\n\n    # Load configurations\n    logger.info(\"Loading model and dataset configurations\")\n    model_cfg = load_config(\n        config=settings.model, name=settings.model_name, mongo_client=mongo_client, config_type=\"model\"\n    )\n\n    dataset_cfg = load_config(\n        config=settings.dataset, name=settings.dataset_name, mongo_client=mongo_client, config_type=\"dataset\"\n    )\n\n    # Load model and dataset\n    logger.info(\"Loading model and dataset\")\n    model = load_model(model_cfg)\n    dataset, metadata = load_dataset(\n        dataset_cfg,\n        device_mesh=device_mesh,\n        n_shards=settings.n_shards,\n        start_shard=settings.start_shard,\n    )\n\n    logger.info(f\"Model loaded: {settings.model_name}\")\n    logger.info(f\"Dataset loaded: {settings.dataset_name}\")\n\n    # Configure activation generation\n    logger.info(\"Configuring activation factory\")\n    factory_cfg = ActivationFactoryConfig(\n        sources=[ActivationFactoryDatasetSource(name=settings.dataset_name)],\n        target=settings.target,\n        hook_points=settings.hook_points,\n        context_size=settings.context_size,\n        model_batch_size=settings.model_batch_size,\n        batch_size=settings.batch_size,\n        buffer_size=settings.buffer_size,\n        buffer_shuffle=settings.buffer_shuffle,\n        ignore_token_ids=settings.ignore_token_ids,\n        override_dtype=settings.override_dtype,\n    )\n\n    # Configure activation writer\n    logger.info(\"Configuring activation writer\")\n    writer_cfg = ActivationWriterConfig(\n        hook_points=settings.hook_points,\n        total_generating_tokens=settings.total_tokens,\n        n_samples_per_chunk=settings.n_samples_per_chunk,\n        cache_dir=settings.output_dir,\n        format=settings.format,\n        num_workers=settings.num_workers,\n    )\n\n    # Create factory and writer\n    factory = ActivationFactory(factory_cfg)\n    writer = ActivationWriter(writer_cfg)\n\n    logger.info(\"Starting activation generation and writing\")\n    # Generate and write activations\n    activations = factory.process(\n        model=model, model_name=settings.model_name, datasets={settings.dataset_name: (dataset, metadata)}\n    )\n    writer.process(activations, device_mesh=device_mesh, start_shard=settings.start_shard)\n\n    logger.info(\"Activation generation completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings","title":"AutoInterpSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for automatic interpretation of SAE features.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model to interpret. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model to interpret. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig\n</code></pre> <p>Configuration for the language model used to generate activations.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Name of the model to load.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.auto_interp","title":"auto_interp  <code>instance-attribute</code>","text":"<pre><code>auto_interp: AutoInterpConfig\n</code></pre> <p>Configuration for the auto-interpretation process.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.mongo","title":"mongo  <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig\n</code></pre> <p>Configuration for the MongoDB database.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.features","title":"features  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>features: list[int] | None = None\n</code></pre> <p>List of specific feature indices to interpret. If None, will interpret all features.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.analysis_name","title":"analysis_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>analysis_name: str = 'default'\n</code></pre> <p>Name of the analysis to use for interpretation.</p>"},{"location":"reference/runners/#lm_saes.AutoInterpSettings.max_workers","title":"max_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_workers: int = 10\n</code></pre> <p>Maximum number of workers to use for interpretation.</p>"},{"location":"reference/runners/#lm_saes.auto_interp","title":"auto_interp","text":"<pre><code>auto_interp(settings: AutoInterpSettings)\n</code></pre> <p>Synchronous wrapper for async_auto_interp.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AutoInterpSettings</code> <p>Configuration for feature interpretation</p> required Source code in <code>src/lm_saes/runners/autointerp.py</code> <pre><code>def auto_interp(settings: AutoInterpSettings):\n    \"\"\"Synchronous wrapper for async_auto_interp.\n\n    Args:\n        settings: Configuration for feature interpretation\n    \"\"\"\n    asyncio.run(async_auto_interp(settings))\n</code></pre>"},{"location":"reference/runners/#lm_saes.SweepSAESettings","title":"SweepSAESettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for sweeping a Sparse Autoencoder (SAE).</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items: list[SweepingItem]\n</code></pre> <p>List of sweeping items</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.activation_factory","title":"activation_factory  <code>instance-attribute</code>","text":"<pre><code>activation_factory: ActivationFactoryConfig\n</code></pre> <p>Configuration for generating activations</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.eval","title":"eval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval: bool = False\n</code></pre> <p>Whether to run in evaluation mode</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre> <p>Size of data parallel mesh</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.model_parallel_size","title":"model_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_parallel_size: int = 1\n</code></pre> <p>Size of model parallel (tensor parallel) mesh</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for MongoDB</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str | None = None\n</code></pre> <p>Name of the tokenizer to load.</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.datasets","title":"datasets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasets: dict[str, DatasetConfig | None] | None = None\n</code></pre> <p>Name to dataset config mapping. Required if using dataset sources.</p>"},{"location":"reference/runners/#lm_saes.SweepSAESettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.SweepingItem","title":"SweepingItem  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single item in a sweeping configuration.</p> <p>Fields:</p> <ul> <li> <code>sae</code>                 (<code>BaseSAEConfig | PretrainedSAE</code>)             </li> <li> <code>sae_name</code>                 (<code>str</code>)             </li> <li> <code>sae_series</code>                 (<code>str</code>)             </li> <li> <code>initializer</code>                 (<code>InitializerConfig | None</code>)             </li> <li> <code>trainer</code>                 (<code>TrainerConfig</code>)             </li> <li> <code>wandb</code>                 (<code>WandbConfig | None</code>)             </li> </ul>"},{"location":"reference/runners/#lm_saes.SweepingItem.sae","title":"sae  <code>pydantic-field</code>","text":"<pre><code>sae: BaseSAEConfig | PretrainedSAE\n</code></pre> <p>Configuration for the SAE model architecture and parameters, or the path to a pretrained SAE.</p>"},{"location":"reference/runners/#lm_saes.SweepingItem.sae_name","title":"sae_name  <code>pydantic-field</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.SweepingItem.sae_series","title":"sae_series  <code>pydantic-field</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.SweepingItem.initializer","title":"initializer  <code>pydantic-field</code>","text":"<pre><code>initializer: InitializerConfig | None = None\n</code></pre> <p>Configuration for model initialization. Should be None for a pretrained SAE.</p>"},{"location":"reference/runners/#lm_saes.SweepingItem.trainer","title":"trainer  <code>pydantic-field</code>","text":"<pre><code>trainer: TrainerConfig\n</code></pre> <p>Configuration for training process</p>"},{"location":"reference/runners/#lm_saes.SweepingItem.wandb","title":"wandb  <code>pydantic-field</code>","text":"<pre><code>wandb: WandbConfig | None = None\n</code></pre> <p>Configuration for Weights &amp; Biases logging</p>"},{"location":"reference/runners/#lm_saes.sweep_sae","title":"sweep_sae","text":"<pre><code>sweep_sae(settings: SweepSAESettings) -&gt; None\n</code></pre> <p>Sweep experiments for training SAE models.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SweepSAESettings</code> <p>Configuration settings for SAE sweeping</p> required Source code in <code>src/lm_saes/runners/train.py</code> <pre><code>def sweep_sae(settings: SweepSAESettings) -&gt; None:\n    \"\"\"Sweep experiments for training SAE models.\n\n    Args:\n        settings: Configuration settings for SAE sweeping\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    n_sweeps = len(settings.items)\n\n    device_mesh = init_device_mesh(\n        device_type=settings.device_type,\n        mesh_shape=(n_sweeps, settings.data_parallel_size, settings.model_parallel_size),\n        mesh_dim_names=(\"sweep\", \"data\", \"model\"),\n    )\n\n    logger.info(f\"Device mesh initialized for sweep with {n_sweeps} configurations\")\n\n    mongo_client = MongoClient(settings.mongo) if settings.mongo is not None else None\n\n    logger.info(\"Loading configurations on rank 0\")\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=False,\n    )\n\n    dataset_cfgs = (\n        {\n            dataset_name: load_config(\n                config=dataset_cfg,\n                name=dataset_name,\n                mongo_client=mongo_client,\n                config_type=\"dataset\",\n            )\n            for dataset_name, dataset_cfg in settings.datasets.items()\n        }\n        if settings.datasets is not None\n        else None\n    )\n\n    # Load model and datasets\n    model = load_model(model_cfg) if model_cfg is not None else None\n    datasets = (\n        {\n            dataset_name: load_dataset(dataset_cfg, device_mesh=device_mesh)\n            for dataset_name, dataset_cfg in dataset_cfgs.items()\n        }\n        if dataset_cfgs is not None\n        else None\n    )\n\n    activation_factory = ActivationFactory(settings.activation_factory, device_mesh=device_mesh)\n\n    logger.info(\"Processing activations stream\")\n    activations_stream = activation_factory.process(\n        model=model,\n        model_name=settings.model_name,\n        datasets=datasets,\n    )\n\n    sae_device_mesh = device_mesh[\"data\", \"model\"]\n    logger.info(f\"Created 2D sub-mesh for SAE: {sae_device_mesh}\")\n\n    item = settings.items[device_mesh.get_local_rank(\"sweep\")]\n    logger.info(f\"Processing sweep item: {item.sae_name}/{item.sae_series}\")\n\n    def convert_activations_to_2d_mesh(stream_3d, mesh_2d):\n        from torch.distributed.tensor import DTensor\n\n        for batch in stream_3d:\n            converted_batch = {}\n            for key, value in batch.items():\n                if isinstance(value, torch.Tensor):\n                    assert isinstance(value, DTensor), \"value must be a DTensor\"\n                    local_tensor = value.to_local()\n                    from lm_saes.utils.distributed import DimMap\n\n                    converted_value = DTensor.from_local(\n                        local_tensor,\n                        device_mesh=mesh_2d,\n                        placements=DimMap({\"data\": 0}).placements(mesh_2d),\n                    )\n                    converted_batch[key] = converted_value\n                else:\n                    converted_batch[key] = value\n            yield converted_batch\n\n    activations_stream = convert_activations_to_2d_mesh(activations_stream, sae_device_mesh)\n\n    logger.info(\"Initializing SAE on 2D sub-mesh\")\n\n    assert item.initializer is None or not isinstance(item.initializer, str), (\n        \"Cannot use an initializer for a pretrained SAE\"\n    )\n    if isinstance(item.sae, PretrainedSAE):\n        sae = AbstractSparseAutoEncoder.from_pretrained(\n            item.sae.pretrained_name_or_path,\n            device_mesh=sae_device_mesh,\n            fold_activation_scale=item.sae.fold_activation_scale,\n            strict_loading=item.sae.strict_loading,\n            device=item.sae.device,\n            dtype=item.sae.dtype,\n        )\n    elif item.initializer is not None:\n        initializer = Initializer(item.initializer)\n        sae = initializer.initialize_sae_from_config(\n            item.sae,\n            activation_stream=activations_stream,\n            device_mesh=sae_device_mesh,\n            model=model,\n        )\n    else:\n        sae = AbstractSparseAutoEncoder.from_config(item.sae, device_mesh=sae_device_mesh)\n\n    wandb_logger = (\n        wandb.init(\n            project=item.wandb.wandb_project,\n            config=item.model_dump(),\n            name=item.wandb.exp_name,\n            entity=item.wandb.wandb_entity,\n            settings=wandb.Settings(x_disable_stats=True),\n            mode=os.getenv(\"WANDB_MODE\", \"online\"),  # type: ignore\n        )\n        if item.wandb is not None and is_primary_rank(device_mesh)\n        else None\n    )\n    # TODO: implement eval_fn\n    eval_fn = (lambda x: None) if settings.eval else None\n\n    logger.info(\"Starting training for sweep item\")\n    trainer = Trainer(item.trainer)\n    sae.cfg.save_hyperparameters(item.trainer.exp_result_path)\n    trainer.fit(sae=sae, activation_stream=activations_stream, eval_fn=eval_fn, wandb_logger=wandb_logger)\n\n    logger.info(\"Training completed, saving sweep item\")\n    sae.save_pretrained(\n        save_path=item.trainer.exp_result_path,\n    )\n    if is_primary_rank(device_mesh) and mongo_client is not None:\n        assert item.sae_name is not None and item.sae_series is not None, (\n            \"sae_name and sae_series must be provided when saving to MongoDB\"\n        )\n        mongo_client.create_sae(\n            name=item.sae_name,\n            series=item.sae_series,\n            path=str(Path(item.trainer.exp_result_path).absolute()),\n            cfg=sae.cfg,\n        )\n\n    if wandb_logger is not None:\n        wandb_logger.finish()\n        logger.info(\"WandB session closed for sweep item\")\n\n    logger.info(f\"Sweep item completed: {item.sae_name}/{item.sae_series}\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings","title":"DirectLogitAttributeSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for analyzing a CrossCoder model.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.sae","title":"sae  <code>instance-attribute</code>","text":"<pre><code>sae: PretrainedSAE\n</code></pre> <p>Configuration for the SAE model architecture and parameters</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.sae_name","title":"sae_name  <code>instance-attribute</code>","text":"<pre><code>sae_name: str\n</code></pre> <p>Name of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.layer_idx","title":"layer_idx  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>layer_idx: int | None | None = None\n</code></pre> <p>The index of layer to DLA.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.sae_series","title":"sae_series  <code>instance-attribute</code>","text":"<pre><code>sae_series: str\n</code></pre> <p>Series of the SAE model. Use as identifier for the SAE model in the database.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: LanguageModelConfig | None = None\n</code></pre> <p>Configuration for the language model.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Name of the language model.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.direct_logit_attributor","title":"direct_logit_attributor  <code>instance-attribute</code>","text":"<pre><code>direct_logit_attributor: DirectLogitAttributorConfig\n</code></pre> <p>Configuration for the direct logit attributor.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.mongo","title":"mongo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mongo: MongoDBConfig | None = None\n</code></pre> <p>Configuration for the MongoDB database.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.analysis_file","title":"analysis_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>analysis_file: Path | None = None\n</code></pre> <p>The analysis results file to be updated. Only used if MongoDB client is not provided.</p>"},{"location":"reference/runners/#lm_saes.DirectLogitAttributeSettings.device_type","title":"device_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_type: str = 'cuda'\n</code></pre> <p>Device type to use for distributed training ('cuda' or 'cpu')</p>"},{"location":"reference/runners/#lm_saes.direct_logit_attribute","title":"direct_logit_attribute","text":"<pre><code>direct_logit_attribute(\n    settings: DirectLogitAttributeSettings,\n) -&gt; None\n</code></pre> <p>Direct logit attribute a SAE model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>DirectLogitAttributeSettings</code> <p>Configuration settings for DirectLogitAttributor</p> required Source code in <code>src/lm_saes/runners/analyze.py</code> <pre><code>@torch.no_grad()\ndef direct_logit_attribute(settings: DirectLogitAttributeSettings) -&gt; None:\n    \"\"\"Direct logit attribute a SAE model.\n\n    Args:\n        settings: Configuration settings for DirectLogitAttributor\n    \"\"\"\n    # Set up logging\n    setup_logging(level=\"INFO\")\n\n    # device_mesh = (\n    #     init_device_mesh(\n    #         device_type=settings.device_type,\n    #         mesh_shape=(settings.head_parallel_size, settings.data_parallel_size, settings.model_parallel_size),\n    #         mesh_dim_names=(\"head\", \"data\", \"model\"),\n    #     )\n    #     if settings.head_parallel_size &gt; 1 or settings.data_parallel_size &gt; 1 or settings.model_parallel_size &gt; 1\n    #     else None\n    # )\n\n    mongo_client = None\n    if settings.mongo is not None:\n        mongo_client = MongoClient(settings.mongo)\n        logger.info(\"MongoDB client initialized\")\n    else:\n        assert settings.analysis_file is not None, (\n            \"Analysis directory must be provided if MongoDB client is not provided\"\n        )\n        # the analysis directory should contain the analysis results to be updated\n        logger.info(f\"Analysis results to be updated: {settings.analysis_file}\")\n\n    logger.info(\"Loading SAE model\")\n    sae = AbstractSparseAutoEncoder.from_pretrained(\n        settings.sae.pretrained_name_or_path,\n        device=settings.sae.device,\n        dtype=settings.sae.dtype,\n        fold_activation_scale=settings.sae.fold_activation_scale,\n        strict_loading=settings.sae.strict_loading,\n    )\n\n    # Load configurations\n    model_cfg = load_config(\n        config=settings.model,\n        name=settings.model_name,\n        mongo_client=mongo_client,\n        config_type=\"model\",\n        required=True,\n    )\n    model_cfg.device = settings.device_type\n    model_cfg.dtype = sae.cfg.dtype\n\n    model = load_model(model_cfg)\n    assert isinstance(model, TransformerLensLanguageModel), (\n        \"DirectLogitAttributor only supports TransformerLensLanguageModel as the model backend\"\n    )\n\n    logger.info(\"Direct logit attribution\")\n    direct_logit_attributor = DirectLogitAttributor(settings.direct_logit_attributor)\n    results = direct_logit_attributor.direct_logit_attribute(sae, model, settings.layer_idx)\n\n    # if is_master():\n    if mongo_client is not None:\n        logger.info(\"Direct logit attribution completed, saving results to MongoDB\")\n        mongo_client.update_features(\n            sae_name=settings.sae_name,\n            sae_series=settings.sae_series,\n            update_data=[{\"logits\": result} for result in results],\n            start_idx=0,\n        )\n    else:\n        assert settings.analysis_file is not None, \"analysis_file must be set when MongoDB is not used\"\n        logger.info(f\"Loading analysis results from: {settings.analysis_file}\")\n\n        # Load existing analysis results\n        with open(settings.analysis_file, \"rb\") as f:\n            analysis_data = pickle.load(f)\n\n        # Update each feature with logits\n        assert len(analysis_data) == len(results), (\n            f\"Number of features in analysis file ({len(analysis_data)}) does not match \"\n            f\"number of results from direct logit attribution ({len(results)})\"\n        )\n\n        for i, result in enumerate(results):\n            assert analysis_data[i][\"feature_idx\"] == i, \"Feature index mismatch\"\n            analysis_data[i][\"logits\"] = result\n\n        # Save updated analysis back to file\n        logger.info(f\"Saving updated analysis results to: {settings.analysis_file}\")\n        with open(settings.analysis_file, \"wb\") as f:\n            pickle.dump(analysis_data, f)\n\n        logger.info(f\"Updated {len(results)} features with logit attributions\")\n\n    logger.info(\"Direct logit attribution completed successfully\")\n</code></pre>"},{"location":"reference/runners/#lm_saes.CheckActivationConsistencySettings","title":"CheckActivationConsistencySettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for checking activation consistency. It will check if the activations are consistent across different hook points by comparing their token ids.</p>"},{"location":"reference/runners/#lm_saes.CheckActivationConsistencySettings.paths","title":"paths  <code>instance-attribute</code>","text":"<pre><code>paths: dict[str, Path]\n</code></pre> <p>Paths to the activations to check.</p>"},{"location":"reference/runners/#lm_saes.CheckActivationConsistencySettings.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: str = 'cuda'\n</code></pre> <p>Device to use for checking activation consistency</p>"},{"location":"reference/runners/#lm_saes.CheckActivationConsistencySettings.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre> <p>Number of workers to use for checking activation consistency</p>"},{"location":"reference/runners/#lm_saes.CheckActivationConsistencySettings.prefetch_factor","title":"prefetch_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prefetch_factor: int | None = None\n</code></pre> <p>Number of samples loaded in advance by each worker</p>"},{"location":"reference/runners/#lm_saes.check_activation_consistency","title":"check_activation_consistency","text":"<pre><code>check_activation_consistency(\n    settings: CheckActivationConsistencySettings,\n) -&gt; None\n</code></pre> <p>Check activation consistency.</p> Source code in <code>src/lm_saes/runners/generate.py</code> <pre><code>def check_activation_consistency(settings: CheckActivationConsistencySettings) -&gt; None:\n    \"\"\"Check activation consistency.\"\"\"\n\n    loader = CachedActivationLoader(\n        cache_dirs=settings.paths,\n        device=settings.device,\n        num_workers=settings.num_workers,\n        prefetch_factor=settings.prefetch_factor,\n    )\n\n    activations = loader.process()\n    for activation in activations:\n        pass\n</code></pre>"},{"location":"reference/training/","title":"Training","text":""},{"location":"reference/training/#training","title":"Training","text":"<p>Training infrastructure: trainer, optimizer configs, initialization, and logging.</p>"},{"location":"reference/training/#lm_saes.TrainerConfig","title":"TrainerConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>l1_coefficient</code>                 (<code>float | None</code>)             </li> <li> <code>l1_coefficient_warmup_steps</code>                 (<code>int | float</code>)             </li> <li> <code>lp_coefficient</code>                 (<code>float | None</code>)             </li> <li> <code>auxk_coefficient</code>                 (<code>float | None</code>)             </li> <li> <code>amp_dtype</code>                 (<code>dtype | None</code>)             </li> <li> <code>sparsity_loss_type</code>                 (<code>Literal['power', 'tanh', 'tanh-quad', None]</code>)             </li> <li> <code>tanh_stretch_coefficient</code>                 (<code>float</code>)             </li> <li> <code>frequency_scale</code>                 (<code>float</code>)             </li> <li> <code>p</code>                 (<code>int</code>)             </li> <li> <code>initial_k</code>                 (<code>int | float | None</code>)             </li> <li> <code>k_warmup_steps</code>                 (<code>int | float</code>)             </li> <li> <code>k_cold_booting_steps</code>                 (<code>int | float</code>)             </li> <li> <code>k_schedule_type</code>                 (<code>Literal['linear', 'exponential']</code>)             </li> <li> <code>k_exponential_factor</code>                 (<code>float</code>)             </li> <li> <code>k_aux</code>                 (<code>int</code>)             </li> <li> <code>dead_threshold</code>                 (<code>float</code>)             </li> <li> <code>skip_metrics_calculation</code>                 (<code>bool</code>)             </li> <li> <code>gradient_accumulation_steps</code>                 (<code>int</code>)             </li> <li> <code>lr</code>                 (<code>float | dict[str, float]</code>)             </li> <li> <code>betas</code>                 (<code>tuple[float, float]</code>)             </li> <li> <code>optimizer_class</code>                 (<code>Literal['adam', 'sparseadam']</code>)             </li> <li> <code>optimizer_foreach</code>                 (<code>bool</code>)             </li> <li> <code>lr_scheduler_name</code>                 (<code>Literal['constant', 'constantwithwarmup', 'linearwarmupdecay', 'cosineannealing', 'cosineannealingwarmup', 'exponentialwarmup']</code>)             </li> <li> <code>lr_end_ratio</code>                 (<code>float</code>)             </li> <li> <code>lr_warm_up_steps</code>                 (<code>int | float</code>)             </li> <li> <code>lr_cool_down_steps</code>                 (<code>int | float</code>)             </li> <li> <code>jumprelu_lr_factor</code>                 (<code>float</code>)             </li> <li> <code>clip_grad_norm</code>                 (<code>float</code>)             </li> <li> <code>feature_sampling_window</code>                 (<code>int</code>)             </li> <li> <code>total_training_tokens</code>                 (<code>int</code>)             </li> <li> <code>log_frequency</code>                 (<code>int</code>)             </li> <li> <code>eval_frequency</code>                 (<code>int</code>)             </li> <li> <code>n_checkpoints</code>                 (<code>int</code>)             </li> <li> <code>check_point_save_mode</code>                 (<code>Literal['log', 'linear']</code>)             </li> <li> <code>from_pretrained_path</code>                 (<code>str | None</code>)             </li> <li> <code>exp_result_path</code>                 (<code>str</code>)             </li> </ul>"},{"location":"reference/training/#lm_saes.TrainerConfig.l1_coefficient","title":"l1_coefficient  <code>pydantic-field</code>","text":"<pre><code>l1_coefficient: float | None = 8e-05\n</code></pre> <p>Coefficient for the L1 sparsity loss. This loss is used to penalize the sparsity of the feature activations.</p>"},{"location":"reference/training/#lm_saes.TrainerConfig.l1_coefficient_warmup_steps","title":"l1_coefficient_warmup_steps  <code>pydantic-field</code>","text":"<pre><code>l1_coefficient_warmup_steps: int | float = 0.1\n</code></pre> <p>Steps (int) or fraction of total steps (float) to warm up the sparsity coefficient from 0.</p>"},{"location":"reference/training/#lm_saes.TrainerConfig.lp_coefficient","title":"lp_coefficient  <code>pydantic-field</code>","text":"<pre><code>lp_coefficient: float | None = None\n</code></pre> <p>Coefficient for the Lp sparsity loss. This loss is used to . To use the JumpReLU \\(L^p\\) penalty, set lp_coefficient to a positive value.</p>"},{"location":"reference/training/#lm_saes.TrainerConfig.auxk_coefficient","title":"auxk_coefficient  <code>pydantic-field</code>","text":"<pre><code>auxk_coefficient: float | None = None\n</code></pre> <p>Coefficient for the Aux-K auxiliary loss. This loss is used to revive dead latents during training. To use the Aux-K loss, set auxk_coefficient to a positive value.</p>"},{"location":"reference/training/#lm_saes.Trainer","title":"Trainer","text":"<pre><code>Trainer(cfg: TrainerConfig)\n</code></pre> Source code in <code>src/lm_saes/trainer.py</code> <pre><code>def __init__(self, cfg: TrainerConfig):\n    self.cfg = cfg\n    self.checkpoint_thresholds: list[int] = []\n    self.total_training_steps: int = 0\n    self.lr_warm_up_steps: int = 0\n    self.lr_cool_down_steps: int = 0\n    self.k_warmup_steps: int = 0\n    self.k_cold_booting_steps: int = 0\n    self.l1_coefficient_warmup_steps: int = 0\n    self.cur_step: int = 0\n    self.cur_tokens: int = 0\n    self.optimizer: Optimizer | None = None\n    self.scheduler: lr_scheduler.LRScheduler | None = None\n    self.wandb_logger: Run | None = None\n    self.metrics: list[Metric] = []\n    # Dead statistics for auxk loss\n    self.tokens_since_last_activation: Tensor | None = None\n    self.is_dead: Tensor | None = None\n</code></pre>"},{"location":"reference/training/#lm_saes.Trainer.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    sae: AbstractSparseAutoEncoder,\n    checkpoint_path: Path | str,\n) -&gt; None\n</code></pre> <p>Save a complete checkpoint including model, optimizer, scheduler, and trainer state.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <code>AbstractSparseAutoEncoder</code> <p>The sparse autoencoder model to save</p> required <code>checkpoint_path</code> <code>Path | str</code> <p>Path where to save the checkpoint (without extension)</p> required Source code in <code>src/lm_saes/trainer.py</code> <pre><code>def save_checkpoint(self, sae: AbstractSparseAutoEncoder, checkpoint_path: Path | str) -&gt; None:\n    \"\"\"\n    Save a complete checkpoint including model, optimizer, scheduler, and\n    trainer state.\n\n    Args:\n        sae: The sparse autoencoder model to save\n        checkpoint_path: Path where to save the checkpoint (without extension)\n    \"\"\"\n\n    # Create checkpoint directory if it doesn't exist\n    checkpoint_dir = Path(checkpoint_path) / \"checkpoints\" / f\"step_{self.cur_step}\"\n\n    if checkpoint_dir and not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir, exist_ok=True)\n\n    sae.cfg.save_hyperparameters(checkpoint_dir)\n    # Save model state\n    if sae.device_mesh is None:\n        sae.save_checkpoint(checkpoint_dir / \"sae_weights.safetensors\")\n    else:\n        sae.save_checkpoint(checkpoint_dir / \"sae_weights.dcp\")\n\n    if is_primary_rank(sae.device_mesh):\n        # Prepare trainer state\n        trainer_state = {\n            \"cur_step\": self.cur_step,\n            \"cur_tokens\": self.cur_tokens,\n            \"total_training_steps\": self.total_training_steps,\n            \"lr_warm_up_steps\": self.lr_warm_up_steps,\n            \"lr_cool_down_steps\": self.lr_cool_down_steps,\n            \"k_warmup_steps\": self.k_warmup_steps,\n            \"k_cold_booting_steps\": self.k_cold_booting_steps,\n            \"l1_coefficient_warmup_steps\": self.l1_coefficient_warmup_steps,\n            \"checkpoint_thresholds\": self.checkpoint_thresholds,\n            \"cfg\": self.cfg,\n        }\n        # Save trainer state\n        trainer_path = checkpoint_dir / \"trainer.pt\"\n        torch.save(trainer_state, trainer_path)\n        if self.wandb_logger is not None:\n            with open(checkpoint_dir / \"wandb_run_id.json\", \"w\") as f:\n                json.dump({\"wandb_run_id\": self.wandb_logger.id}, f)\n    # Save optimizer state - handle distributed tensors\n    if self.optimizer is not None:\n        if sae.device_mesh is None:\n            if is_primary_rank(sae.device_mesh):\n                optimizer_path = checkpoint_dir / \"optimizer.pt\"\n                optimizer_state = self.optimizer.state_dict()\n                torch.save(optimizer_state, optimizer_path)\n        else:\n            optimizer_path = checkpoint_dir / \"optimizer.dcp\"\n            optimizer_state = self.optimizer.state_dict()\n            fs_writer = FileSystemWriter(optimizer_path)\n            dcp.save(optimizer_state, storage_writer=fs_writer)\n\n    # Save scheduler state - handle distributed tensors\n    if self.scheduler is not None:\n        if sae.device_mesh is None:\n            if is_primary_rank(sae.device_mesh):\n                scheduler_path = checkpoint_dir / \"scheduler.pt\"\n                scheduler_state = self.scheduler.state_dict()\n                torch.save(scheduler_state, scheduler_path)\n        else:\n            scheduler_path = checkpoint_dir / \"scheduler.dcp\"\n            scheduler_state = self.scheduler.state_dict()\n            fs_writer = FileSystemWriter(scheduler_path)\n            dcp.save(scheduler_state, storage_writer=fs_writer)\n\n    logger.info(f\"Checkpoint saved to {checkpoint_path}\")\n</code></pre>"},{"location":"reference/training/#lm_saes.Trainer.from_checkpoint","title":"from_checkpoint  <code>classmethod</code>","text":"<pre><code>from_checkpoint(\n    sae: AbstractSparseAutoEncoder, checkpoint_path: str\n) -&gt; Trainer\n</code></pre> <p>Load a complete checkpoint including model, optimizer, scheduler, and trainer state.</p> <p>Parameters:</p> Name Type Description Default <code>device_mesh</code> <p>The device mesh to load the model into</p> required <code>checkpoint_path</code> <code>str</code> <p>Path where the checkpoint was saved (without extension)</p> required <p>Returns:</p> Name Type Description <code>Trainer</code> <code>Trainer</code> <p>A new trainer instance with loaded state</p> Source code in <code>src/lm_saes/trainer.py</code> <pre><code>@classmethod\ndef from_checkpoint(\n    cls,\n    sae: AbstractSparseAutoEncoder,\n    checkpoint_path: str,\n) -&gt; \"Trainer\":\n    \"\"\"\n    Load a complete checkpoint including model, optimizer, scheduler, and\n    trainer state.\n\n    Args:\n        device_mesh: The device mesh to load the model into\n        checkpoint_path: Path where the checkpoint was saved (without extension)\n\n    Returns:\n        Trainer: A new trainer instance with loaded state\n    \"\"\"\n    # Load trainer state first to get the config\n    checkpoint_dir = Path(checkpoint_path)\n    trainer_path = checkpoint_dir / \"trainer.pt\"\n    if os.path.exists(trainer_path):\n        trainer_state = torch.load(trainer_path, map_location=\"cpu\", weights_only=False)\n        cfg = trainer_state.get(\"cfg\")\n        if cfg is None:\n            raise ValueError(\"Checkpoint does not contain trainer config\")\n\n        # Create trainer instance with loaded config\n        trainer = cls(cfg)\n        trainer.cfg.from_pretrained_path = checkpoint_path\n\n        # Restore trainer state variables\n        trainer.cur_step = trainer_state[\"cur_step\"]\n        trainer.cur_tokens = trainer_state[\"cur_tokens\"]\n        trainer.total_training_steps = trainer_state[\"total_training_steps\"]\n        trainer.lr_warm_up_steps = trainer_state[\"lr_warm_up_steps\"]\n        trainer.lr_cool_down_steps = trainer_state[\"lr_cool_down_steps\"]\n        trainer.k_warmup_steps = trainer_state[\"k_warmup_steps\"]\n        trainer.k_cold_booting_steps = trainer_state[\"k_cold_booting_steps\"]\n        trainer.l1_coefficient_warmup_steps = trainer_state[\"l1_coefficient_warmup_steps\"]\n        trainer.checkpoint_thresholds = trainer_state[\"checkpoint_thresholds\"]\n\n        logger.info(f\"Loaded trainer state from step {trainer.cur_step}\")\n    else:\n        raise ValueError(f\"Trainer checkpoint not found at {trainer_path}\")\n\n    trainer._initialize_optimizer(sae)\n    assert trainer.optimizer is not None and trainer.scheduler is not None, (\n        \"Optimizer and scheduler should be already initialized\"\n    )\n\n    # Load optimizer state\n    if sae.device_mesh is None:\n        optimizer_path = checkpoint_dir / \"optimizer.pt\"\n        optimizer_state = torch.load(optimizer_path, map_location=\"cpu\")\n        trainer.optimizer.load_state_dict(optimizer_state)\n        logger.info(\"Loaded optimizer state\")\n    else:\n        optimizer_path = checkpoint_dir / \"optimizer.dcp\"\n        fs_reader = FileSystemReader(str(optimizer_path))\n        optimizer_state = trainer.optimizer.state_dict()\n        dcp.load(optimizer_state, storage_reader=fs_reader)\n        trainer.optimizer.load_state_dict(optimizer_state)\n        logger.info(\"Loaded optimizer state\")\n        logger.info(f\"trainer.optimizer.state_dict(): {trainer.optimizer.state_dict()}\")\n\n    # Load scheduler state\n    if sae.device_mesh is None:\n        scheduler_path = checkpoint_dir / \"scheduler.pt\"\n        scheduler_state = torch.load(scheduler_path, map_location=\"cpu\")\n        trainer.scheduler.load_state_dict(scheduler_state)\n        logger.info(\"Loaded scheduler state\")\n    else:\n        scheduler_path = checkpoint_dir / \"scheduler.dcp\"\n        fs_reader = FileSystemReader(str(scheduler_path))\n        scheduler_state = trainer.scheduler.state_dict()\n        dcp.load(scheduler_state, storage_reader=fs_reader)\n        trainer.scheduler.load_state_dict(scheduler_state)\n        logger.info(\"Loaded scheduler state\")\n        logger.info(f\"trainer.scheduler.state_dict(): {trainer.scheduler.state_dict()}\")\n\n    logger.info(f\"Checkpoint loaded from {checkpoint_path}\")\n    return trainer\n</code></pre>"},{"location":"reference/training/#lm_saes.Trainer.update_dead_statistics","title":"update_dead_statistics","text":"<pre><code>update_dead_statistics(\n    feature_acts: Tensor,\n    mask: Tensor | None,\n    specs: tuple[str, ...],\n) -&gt; Tensor\n</code></pre> <p>Update the dead latents tracking based on current feature activations.</p> <p>Parameters:</p> Name Type Description Default <code>feature_acts</code> <code>Tensor</code> <p>Feature activations tensor of shape (batch, d_sae) or (batch, seq_len, d_sae)</p> required <p>Returns:</p> Name Type Description <code>is_dead</code> <code>Tensor</code> <p>Boolean tensor indicating which features are dead.</p> Source code in <code>src/lm_saes/trainer.py</code> <pre><code>@torch.no_grad()\ndef update_dead_statistics(self, feature_acts: Tensor, mask: Tensor | None, specs: tuple[str, ...]) -&gt; Tensor:\n    \"\"\"Update the dead latents tracking based on current feature activations.\n\n    Args:\n        feature_acts: Feature activations tensor of shape (batch, d_sae) or (batch, seq_len, d_sae)\n\n    Returns:\n        is_dead: Boolean tensor indicating which features are dead.\n    \"\"\"\n    assert self.tokens_since_last_activation is not None, (\n        \"tokens_since_last_activation must be initialized before calling update_dead_statistics\"\n    )\n    assert self.is_dead is not None, \"is_dead must be initialized before calling update_dead_statistics\"\n\n    valid_tokens = mask.sum() if mask is not None else feature_acts[..., 0].numel()\n\n    feature_acts_sum, _ = apply_token_mask(feature_acts, specs, mask, \"sum\")\n    activated = feature_acts_sum.gt(0)\n\n    self.tokens_since_last_activation = torch.where(\n        activated,\n        torch.zeros_like(self.tokens_since_last_activation),\n        self.tokens_since_last_activation + valid_tokens,\n    )\n    self.is_dead = self.tokens_since_last_activation &gt;= self.cfg.dead_threshold\n    return self.is_dead\n</code></pre>"},{"location":"reference/training/#lm_saes.WandbConfig","title":"WandbConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>wandb_project</code>                 (<code>str</code>)             </li> <li> <code>exp_name</code>                 (<code>str | None</code>)             </li> <li> <code>wandb_entity</code>                 (<code>str | None</code>)             </li> <li> <code>wandb_run_id</code>                 (<code>str | None</code>)             </li> <li> <code>wandb_resume</code>                 (<code>Literal['allow', 'must', 'never', 'auto']</code>)             </li> </ul>"},{"location":"reference/training/#lm_saes.InitializerConfig","title":"InitializerConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Fields:</p> <ul> <li> <code>bias_init_method</code>                 (<code>Literal['all_zero', 'geometric_median']</code>)             </li> <li> <code>decoder_uniform_bound</code>                 (<code>float</code>)             </li> <li> <code>encoder_uniform_bound</code>                 (<code>float</code>)             </li> <li> <code>init_encoder_with_decoder_transpose</code>                 (<code>bool</code>)             </li> <li> <code>init_encoder_with_decoder_transpose_factor</code>                 (<code>float</code>)             </li> <li> <code>init_log_jumprelu_threshold_value</code>                 (<code>float | None</code>)             </li> <li> <code>grid_search_init_norm</code>                 (<code>bool</code>)             </li> <li> <code>initialize_W_D_with_active_subspace</code>                 (<code>bool</code>)             </li> <li> <code>d_active_subspace</code>                 (<code>int | None</code>)             </li> <li> <code>initialize_lorsa_with_mhsa</code>                 (<code>bool | None</code>)             </li> <li> <code>initialize_tc_with_mlp</code>                 (<code>bool | None</code>)             </li> <li> <code>model_layer</code>                 (<code>int | None</code>)             </li> <li> <code>init_encoder_bias_with_mean_hidden_pre</code>                 (<code>bool</code>)             </li> </ul>"},{"location":"reference/training/#lm_saes.InitializerConfig.bias_init_method","title":"bias_init_method  <code>pydantic-field</code>","text":"<pre><code>bias_init_method: Literal[\n    \"all_zero\", \"geometric_median\"\n] = \"all_zero\"\n</code></pre> <p>Method for initializing the decoder bias. <code>\"geometric_median\"</code> sets the bias to the geometric median of the activation distribution, which is more robust than <code>\"all_zero\"</code> for skewed activations.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.decoder_uniform_bound","title":"decoder_uniform_bound  <code>pydantic-field</code>","text":"<pre><code>decoder_uniform_bound: float = 1.0\n</code></pre> <p>Half-range of the uniform distribution used to initialize decoder weights, weights are sampled from U(-decoder_uniform_bound, decoder_uniform_bound).</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.encoder_uniform_bound","title":"encoder_uniform_bound  <code>pydantic-field</code>","text":"<pre><code>encoder_uniform_bound: float = 1.0\n</code></pre> <p>Half-range of the uniform distribution used to initialize encoder weights, weights are sampled from U(-encoder_uniform_bound, encoder_uniform_bound).</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.init_encoder_with_decoder_transpose","title":"init_encoder_with_decoder_transpose  <code>pydantic-field</code>","text":"<pre><code>init_encoder_with_decoder_transpose: bool = True\n</code></pre> <p>If <code>True</code>, the encoder weight matrix is initialized as the transpose of the decoder weight matrix (scaled by <code>init_encoder_with_decoder_transpose_factor</code>), providing a better starting point for SAE training.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.init_encoder_with_decoder_transpose_factor","title":"init_encoder_with_decoder_transpose_factor  <code>pydantic-field</code>","text":"<pre><code>init_encoder_with_decoder_transpose_factor: float = 1.0\n</code></pre> <p>Scaling factor applied to the transposed decoder weights when initializing the encoder.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.init_log_jumprelu_threshold_value","title":"init_log_jumprelu_threshold_value  <code>pydantic-field</code>","text":"<pre><code>init_log_jumprelu_threshold_value: float | None = None\n</code></pre> <p>Initial value for the log-threshold parameter of JumpReLU activations. Only used when the SAE uses a JumpReLU activation function.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.grid_search_init_norm","title":"grid_search_init_norm  <code>pydantic-field</code>","text":"<pre><code>grid_search_init_norm: bool = False\n</code></pre> <p>Performs a coarse-then-fine grid search over decoder norms to find the value that minimizes the initial reconstruction loss, then sets the decoder to that norm.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.initialize_W_D_with_active_subspace","title":"initialize_W_D_with_active_subspace  <code>pydantic-field</code>","text":"<pre><code>initialize_W_D_with_active_subspace: bool = False\n</code></pre> <p>Initializes the decoder weight matrix within the active (high-variance) subspace of the input activations via SVD. Recommended for low-rank activations such as attention outputs to reduce dead features.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.d_active_subspace","title":"d_active_subspace  <code>pydantic-field</code>","text":"<pre><code>d_active_subspace: int | None = None\n</code></pre> <p>Dimension of the active subspace used when <code>initialize_W_D_with_active_subspace=True</code></p>"},{"location":"reference/training/#lm_saes.InitializerConfig.initialize_lorsa_with_mhsa","title":"initialize_lorsa_with_mhsa  <code>pydantic-field</code>","text":"<pre><code>initialize_lorsa_with_mhsa: bool | None = None\n</code></pre> <p>Initializes the Lorsa QK weights from the target model's attention (MHSA) weights at <code>model_layer</code>.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.initialize_tc_with_mlp","title":"initialize_tc_with_mlp  <code>pydantic-field</code>","text":"<pre><code>initialize_tc_with_mlp: bool | None = None\n</code></pre> <p>Initializes the transcoder decoder weights from the target model's MLP weights at <code>model_layer</code>.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.model_layer","title":"model_layer  <code>pydantic-field</code>","text":"<pre><code>model_layer: int | None = None\n</code></pre> <p>Layer index of the target model from which to extract weights for <code>initialize_lorsa_with_mhsa</code> or <code>initialize_tc_with_mlp</code>.</p>"},{"location":"reference/training/#lm_saes.InitializerConfig.init_encoder_bias_with_mean_hidden_pre","title":"init_encoder_bias_with_mean_hidden_pre  <code>pydantic-field</code>","text":"<pre><code>init_encoder_bias_with_mean_hidden_pre: bool = False\n</code></pre> <p>Initializes the encoder bias to the negative mean of the pre-activation distribution.</p>"},{"location":"reference/training/#lm_saes.Initializer","title":"Initializer","text":"<pre><code>Initializer(cfg: InitializerConfig)\n</code></pre> Source code in <code>src/lm_saes/initializer.py</code> <pre><code>def __init__(self, cfg: InitializerConfig):\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/training/#lm_saes.Initializer.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(sae: AbstractSparseAutoEncoder)\n</code></pre> <p>Initialize the parameters of the SAE. Only used when the state is \"training\" to initialize sae.</p> Source code in <code>src/lm_saes/initializer.py</code> <pre><code>@torch.no_grad()\ndef initialize_parameters(self, sae: AbstractSparseAutoEncoder):\n    \"\"\"Initialize the parameters of the SAE.\n    Only used when the state is \"training\" to initialize sae.\n    \"\"\"\n\n    sae.init_parameters(\n        encoder_uniform_bound=self.cfg.encoder_uniform_bound,\n        decoder_uniform_bound=self.cfg.decoder_uniform_bound,\n        init_log_jumprelu_threshold_value=self.cfg.init_log_jumprelu_threshold_value,\n    )\n\n    if self.cfg.init_encoder_with_decoder_transpose:\n        sae.init_encoder_with_decoder_transpose(self.cfg.init_encoder_with_decoder_transpose_factor)\n\n    return sae\n</code></pre>"},{"location":"reference/training/#lm_saes.Initializer.initialization_search","title":"initialization_search","text":"<pre><code>initialization_search(\n    sae: AbstractSparseAutoEncoder,\n    activation_batch: dict[str, Tensor],\n    wandb_logger: Run | None = None,\n)\n</code></pre> <p>This function is used to search for the best initialization norm for the SAE decoder.</p> Source code in <code>src/lm_saes/initializer.py</code> <pre><code>@torch.no_grad()\ndef initialization_search(\n    self,\n    sae: AbstractSparseAutoEncoder,\n    activation_batch: Dict[str, Tensor],\n    wandb_logger: Run | None = None,\n):\n    \"\"\"\n    This function is used to search for the best initialization norm for the SAE decoder.\n    \"\"\"\n    batch = sae.normalize_activations(activation_batch)\n\n    if self.cfg.bias_init_method == \"geometric_median\":\n        assert sae.b_D is not None, \"Decoder bias should exist if use_decoder_bias is True\"\n        if isinstance(sae, CrossLayerTranscoder):\n            for i in range(sae.cfg.n_layers):\n                hook_point_out = sae.cfg.hook_points_out[i]\n                normalized_mean_activation = batch[hook_point_out].mean(0)\n                sae.b_D[i].copy_(normalized_mean_activation)\n        elif (\n            isinstance(sae, MixtureOfLinearTransform)\n            or isinstance(sae, LowRankSparseAttention)\n            or isinstance(sae, SparseAutoEncoder)\n        ):\n            label = sae.prepare_label(batch)\n            normalized_mean_activation = label.mean(dim=list(range((batch[sae.cfg.hook_point_out].ndim - 1))))\n            sae.b_D.copy_(normalized_mean_activation)\n        else:\n            raise ValueError(\n                f\"Bias initialization method {self.cfg.bias_init_method} is not supported for {sae.cfg.sae_type}\"\n            )\n\n    if self.cfg.init_encoder_bias_with_mean_hidden_pre:\n        sae.init_encoder_bias_with_mean_hidden_pre(batch)\n\n    @torch.autocast(device_type=sae.cfg.device, dtype=sae.cfg.dtype)\n    def grid_search_best_init_norm(search_range: List[float]) -&gt; float:\n        losses: Dict[float, float] = {}\n\n        for norm in search_range:\n            sae.set_decoder_to_fixed_norm(norm, force_exact=True)\n            if self.cfg.init_encoder_with_decoder_transpose:\n                sae.init_encoder_with_decoder_transpose(self.cfg.init_encoder_with_decoder_transpose_factor)\n            if self.cfg.init_encoder_bias_with_mean_hidden_pre:\n                sae.init_encoder_bias_with_mean_hidden_pre(batch)\n            mse = item(sae.compute_loss(batch)[\"l_rec\"].mean())\n            losses[norm] = mse\n        best_norm = min(losses, key=losses.get)  # type: ignore\n        return best_norm\n\n    if self.cfg.grid_search_init_norm:\n        best_norm_coarse = grid_search_best_init_norm(torch.linspace(0.1, 5.0, 50).numpy().tolist())\n        best_norm_fine_grained = grid_search_best_init_norm(\n            torch.linspace(best_norm_coarse - 0.09, best_norm_coarse + 0.1, 20).numpy().tolist()\n        )\n\n        logger.info(f\"The best (i.e. lowest MSE) initialized norm is {best_norm_fine_grained}\")\n        if wandb_logger is not None:\n            wandb_logger.log({\"best_norm_fine_grained\": best_norm_fine_grained})\n\n        sae.set_decoder_to_fixed_norm(best_norm_fine_grained, force_exact=True)\n\n    if self.cfg.init_encoder_with_decoder_transpose:\n        sae.init_encoder_with_decoder_transpose(self.cfg.init_encoder_with_decoder_transpose_factor)\n    if self.cfg.init_encoder_bias_with_mean_hidden_pre:\n        sae.init_encoder_bias_with_mean_hidden_pre(batch)\n\n    return sae\n</code></pre>"},{"location":"reference/training/#lm_saes.Initializer.initialize_sae_from_config","title":"initialize_sae_from_config","text":"<pre><code>initialize_sae_from_config(\n    cfg: BaseSAEConfig,\n    activation_stream: Iterable[dict[str, Tensor]]\n    | None = None,\n    activation_norm: dict[str, float] | None = None,\n    device_mesh: DeviceMesh | None = None,\n    wandb_logger: Run | None = None,\n    model: LanguageModel | None = None,\n)\n</code></pre> <p>Initialize the SAE from the SAE config. Args:     cfg (SAEConfig): The SAE config.     activation_iter (Iterable[dict[str, Tensor]] | None): The activation iterator.     activation_norm (dict[str, float] | None): The activation normalization. Used for dataset-wise normalization when self.cfg.norm_activation is \"dataset-wise\".     device_mesh (DeviceMesh | None): The device mesh.</p> Source code in <code>src/lm_saes/initializer.py</code> <pre><code>def initialize_sae_from_config(\n    self,\n    cfg: BaseSAEConfig,\n    activation_stream: Iterable[dict[str, Tensor]] | None = None,\n    activation_norm: dict[str, float] | None = None,\n    device_mesh: DeviceMesh | None = None,\n    wandb_logger: Run | None = None,\n    model: LanguageModel | None = None,\n):\n    \"\"\"\n    Initialize the SAE from the SAE config.\n    Args:\n        cfg (SAEConfig): The SAE config.\n        activation_iter (Iterable[dict[str, Tensor]] | None): The activation iterator.\n        activation_norm (dict[str, float] | None): The activation normalization. Used for dataset-wise normalization when self.cfg.norm_activation is \"dataset-wise\".\n        device_mesh (DeviceMesh | None): The device mesh.\n    \"\"\"\n    sae: AbstractSparseAutoEncoder = AbstractSparseAutoEncoder.from_config(\n        cfg,\n        device_mesh=device_mesh,\n    )\n\n    sae = self.initialize_parameters(sae)\n    if sae.cfg.norm_activation == \"dataset-wise\":\n        if activation_norm is None:\n            assert activation_stream is not None, (\n                \"Activation iterator must be provided for dataset-wise normalization\"\n            )\n\n            activation_norm = calculate_activation_norm(\n                activation_stream, cfg.associated_hook_points, device_mesh=device_mesh\n            )\n        sae.set_dataset_average_activation_norm(activation_norm)\n\n    if isinstance(sae, LowRankSparseAttention) and self.cfg.initialize_lorsa_with_mhsa:\n        assert sae.cfg.norm_activation == \"dataset-wise\", (\n            \"Norm activation must be dataset-wise for Lorsa if use initialize_lorsa_with_mhsa\"\n        )\n        assert isinstance(model, TransformerLensLanguageModel) and model.model is not None, (\n            \"Only support TransformerLens backend for initializing Lorsa with Original Multi Head Sparse Attention\"\n        )\n        assert self.cfg.model_layer is not None, (\n            \"Model layer must be provided for initializing Lorsa with Original Multi Head Sparse Attention\"\n        )\n        assert isinstance(model.model, HookedTransformer), \"Model must be a TransformerLens model\"\n        assert isinstance(model.model.blocks[self.cfg.model_layer], TransformerBlock), (\n            \"Block must be a TransformerBlock\"\n        )\n        assert isinstance(model.model.blocks[self.cfg.model_layer].attn, Attention | GroupedQueryAttention), (\n            \"Attention must be an Attention or GroupedQueryAttention\"\n        )\n        sae.init_lorsa_with_mhsa(\n            cast(\n                Attention | GroupedQueryAttention,\n                model.model.blocks[self.cfg.model_layer].attn,\n            )\n        )\n\n    assert activation_stream is not None, \"Activation iterator must be provided for initialization search\"\n    activation_batch = next(iter(activation_stream))  # type: ignore\n\n    if (\n        isinstance(sae, SparseAutoEncoder)\n        and sae.cfg.hook_point_in != sae.cfg.hook_point_out\n        and self.cfg.initialize_tc_with_mlp\n    ):\n        batch = sae.normalize_activations(activation_batch)\n        assert sae.cfg.norm_activation == \"dataset-wise\"\n        assert isinstance(model, TransformerLensLanguageModel) and model.model is not None\n        assert self.cfg.model_layer is not None\n        assert isinstance(model.model, HookedTransformer), \"Model must be a TransformerLens model\"\n        assert isinstance(model.model.blocks[self.cfg.model_layer], TransformerBlock), (\n            \"Block must be a TransformerBlock\"\n        )\n        assert isinstance(model.model.blocks[self.cfg.model_layer].mlp, CanBeUsedAsMLP)\n        sae.init_tc_with_mlp(\n            batch=batch,\n            mlp=cast(CanBeUsedAsMLP, model.model.blocks[self.cfg.model_layer].mlp),\n        )\n\n    if self.cfg.initialize_W_D_with_active_subspace:\n        batch = sae.normalize_activations(activation_batch)\n        if isinstance(sae, LowRankSparseAttention):\n            assert sae.cfg.norm_activation == \"dataset-wise\", (\n                \"Norm activation must be dataset-wise for Lorsa if use initialize_W_D_with_active_subspace\"\n            )\n            assert isinstance(model, TransformerLensLanguageModel) and model.model is not None, (\n                \"Only support TransformerLens backend for initializing Lorsa decoder weight with active subspace\"\n            )\n            assert self.cfg.model_layer is not None, (\n                \"Model layer must be provided for initializing Lorsa decoder weight with active subspace\"\n            )\n            sae.init_W_V_with_active_subspace_per_head(\n                batch=batch,\n                mhsa=cast(\n                    Attention | GroupedQueryAttention,\n                    model.model.blocks[self.cfg.model_layer].attn,\n                ),\n            )\n        else:\n            assert self.cfg.d_active_subspace is not None, (\n                \"d_active_subspace must be provided for initializing other SAEs with active subspace\"\n            )\n            sae.init_W_D_with_active_subspace(batch=batch, d_active_subspace=self.cfg.d_active_subspace)\n\n    sae = self.initialization_search(sae, activation_batch, wandb_logger=wandb_logger)\n\n    return sae\n</code></pre>"}]}